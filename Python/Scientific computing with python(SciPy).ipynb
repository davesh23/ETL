{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21.333333333333336, 2.368475785867001e-13)\n",
      "21.333333333333332\n"
     ]
    }
   ],
   "source": [
    "x2 = lambda x: x**2\n",
    "print(integrate.quad(x2, 0, 4))\n",
    "print(4**3/3.)  #analytical result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package scipy.integrate in scipy:\n",
      "\n",
      "NAME\n",
      "    scipy.integrate\n",
      "\n",
      "DESCRIPTION\n",
      "    =============================================\n",
      "    Integration and ODEs (:mod:`scipy.integrate`)\n",
      "    =============================================\n",
      "    \n",
      "    .. currentmodule:: scipy.integrate\n",
      "    \n",
      "    Integrating functions, given function object\n",
      "    ============================================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       quad          -- General purpose integration\n",
      "       dblquad       -- General purpose double integration\n",
      "       tplquad       -- General purpose triple integration\n",
      "       nquad         -- General purpose n-dimensional integration\n",
      "       fixed_quad    -- Integrate func(x) using Gaussian quadrature of order n\n",
      "       quadrature    -- Integrate with given tolerance using Gaussian quadrature\n",
      "       romberg       -- Integrate func using Romberg integration\n",
      "       quad_explain  -- Print information for use of quad\n",
      "       newton_cotes  -- Weights and error coefficient for Newton-Cotes integration\n",
      "       IntegrationWarning -- Warning on issues during integration\n",
      "    \n",
      "    Integrating functions, given fixed samples\n",
      "    ==========================================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       trapz         -- Use trapezoidal rule to compute integral.\n",
      "       cumtrapz      -- Use trapezoidal rule to cumulatively compute integral.\n",
      "       simps         -- Use Simpson's rule to compute integral from samples.\n",
      "       romb          -- Use Romberg Integration to compute integral from\n",
      "                     -- (2**k + 1) evenly-spaced samples.\n",
      "    \n",
      "    .. seealso::\n",
      "    \n",
      "       :mod:`scipy.special` for orthogonal polynomials (special) for Gaussian\n",
      "       quadrature roots and weights for other weighting factors and regions.\n",
      "    \n",
      "    Integrators of ODE systems\n",
      "    ==========================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       odeint        -- General integration of ordinary differential equations.\n",
      "       ode           -- Integrate ODE using VODE and ZVODE routines.\n",
      "       complex_ode   -- Convert a complex-valued ODE to real-valued and integrate.\n",
      "       solve_bvp     -- Solve a boundary value problem for a system of ODEs.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _bvp\n",
      "    _dop\n",
      "    _ode\n",
      "    _odepack\n",
      "    _quadpack\n",
      "    _test_multivariate\n",
      "    _test_odeint_banded\n",
      "    lsoda\n",
      "    odepack\n",
      "    quadpack\n",
      "    quadrature\n",
      "    setup\n",
      "    vode\n",
      "\n",
      "CLASSES\n",
      "    builtins.UserWarning(builtins.Warning)\n",
      "        scipy.integrate.quadpack.IntegrationWarning\n",
      "    builtins.object\n",
      "        scipy.integrate._ode.ode\n",
      "            scipy.integrate._ode.complex_ode\n",
      "    \n",
      "    class IntegrationWarning(builtins.UserWarning)\n",
      "     |  Warning on issues during integration.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IntegrationWarning\n",
      "     |      builtins.UserWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      helper for pickle\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class complex_ode(ode)\n",
      "     |  A wrapper of ode for complex systems.\n",
      "     |  \n",
      "     |  This functions similarly as `ode`, but re-maps a complex-valued\n",
      "     |  equation system to a real-valued one before using the integrators.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  f : callable ``f(t, y, *f_args)``\n",
      "     |      Rhs of the equation. t is a scalar, ``y.shape == (n,)``.\n",
      "     |      ``f_args`` is set by calling ``set_f_params(*args)``.\n",
      "     |  jac : callable ``jac(t, y, *jac_args)``\n",
      "     |      Jacobian of the rhs, ``jac[i,j] = d f[i] / d y[j]``.\n",
      "     |      ``jac_args`` is set by calling ``set_f_params(*args)``.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  t : float\n",
      "     |      Current time.\n",
      "     |  y : ndarray\n",
      "     |      Current variable values.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  For usage examples, see `ode`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      complex_ode\n",
      "     |      ode\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, f, jac=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  integrate(self, t, step=0, relax=0)\n",
      "     |      Find y=y(t), set y as an initial condition, and return y.\n",
      "     |  \n",
      "     |  set_initial_value(self, y, t=0.0)\n",
      "     |      Set initial conditions y(t) = y.\n",
      "     |  \n",
      "     |  set_integrator(self, name, **integrator_params)\n",
      "     |      Set integrator by name.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Name of the integrator\n",
      "     |      integrator_params\n",
      "     |          Additional parameters for the integrator.\n",
      "     |  \n",
      "     |  set_solout(self, solout)\n",
      "     |      Set callable to be called at every successful integration step.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      solout : callable\n",
      "     |          ``solout(t, y)`` is called at each internal integrator step,\n",
      "     |          t is a scalar providing the current independent position\n",
      "     |          y is the current soloution ``y.shape == (n,)``\n",
      "     |          solout should return -1 to stop integration\n",
      "     |          otherwise it should return None or 0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  y\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ode:\n",
      "     |  \n",
      "     |  set_f_params(self, *args)\n",
      "     |      Set extra parameters for user-supplied function f.\n",
      "     |  \n",
      "     |  set_jac_params(self, *args)\n",
      "     |      Set extra parameters for user-supplied function jac.\n",
      "     |  \n",
      "     |  successful(self)\n",
      "     |      Check if integration was successful.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ode:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ode(builtins.object)\n",
      "     |  A generic interface class to numeric integrators.\n",
      "     |  \n",
      "     |  Solve an equation system :math:`y'(t) = f(t,y)` with (optional) ``jac = df/dy``.\n",
      "     |  \n",
      "     |  *Note*: The first two arguments of ``f(t, y, ...)`` are in the\n",
      "     |  opposite order of the arguments in the system definition function used\n",
      "     |  by `scipy.integrate.odeint`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  f : callable ``f(t, y, *f_args)``\n",
      "     |      Right-hand side of the differential equation. t is a scalar,\n",
      "     |      ``y.shape == (n,)``.\n",
      "     |      ``f_args`` is set by calling ``set_f_params(*args)``.\n",
      "     |      `f` should return a scalar, array or list (not a tuple).\n",
      "     |  jac : callable ``jac(t, y, *jac_args)``, optional\n",
      "     |      Jacobian of the right-hand side, ``jac[i,j] = d f[i] / d y[j]``.\n",
      "     |      ``jac_args`` is set by calling ``set_jac_params(*args)``.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  t : float\n",
      "     |      Current time.\n",
      "     |  y : ndarray\n",
      "     |      Current variable values.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  odeint : an integrator with a simpler interface based on lsoda from ODEPACK\n",
      "     |  quad : for finding the area under a curve\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Available integrators are listed below. They can be selected using\n",
      "     |  the `set_integrator` method.\n",
      "     |  \n",
      "     |  \"vode\"\n",
      "     |  \n",
      "     |      Real-valued Variable-coefficient Ordinary Differential Equation\n",
      "     |      solver, with fixed-leading-coefficient implementation. It provides\n",
      "     |      implicit Adams method (for non-stiff problems) and a method based on\n",
      "     |      backward differentiation formulas (BDF) (for stiff problems).\n",
      "     |  \n",
      "     |      Source: http://www.netlib.org/ode/vode.f\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |  \n",
      "     |         This integrator is not re-entrant. You cannot have two `ode`\n",
      "     |         instances using the \"vode\" integrator at the same time.\n",
      "     |  \n",
      "     |      This integrator accepts the following parameters in `set_integrator`\n",
      "     |      method of the `ode` class:\n",
      "     |  \n",
      "     |      - atol : float or sequence\n",
      "     |        absolute tolerance for solution\n",
      "     |      - rtol : float or sequence\n",
      "     |        relative tolerance for solution\n",
      "     |      - lband : None or int\n",
      "     |      - uband : None or int\n",
      "     |        Jacobian band width, jac[i,j] != 0 for i-lband <= j <= i+uband.\n",
      "     |        Setting these requires your jac routine to return the jacobian\n",
      "     |        in packed format, jac_packed[i-j+uband, j] = jac[i,j]. The\n",
      "     |        dimension of the matrix must be (lband+uband+1, len(y)).\n",
      "     |      - method: 'adams' or 'bdf'\n",
      "     |        Which solver to use, Adams (non-stiff) or BDF (stiff)\n",
      "     |      - with_jacobian : bool\n",
      "     |        This option is only considered when the user has not supplied a\n",
      "     |        Jacobian function and has not indicated (by setting either band)\n",
      "     |        that the Jacobian is banded.  In this case, `with_jacobian` specifies\n",
      "     |        whether the iteration method of the ODE solver's correction step is\n",
      "     |        chord iteration with an internally generated full Jacobian or\n",
      "     |        functional iteration with no Jacobian.\n",
      "     |      - nsteps : int\n",
      "     |        Maximum number of (internally defined) steps allowed during one\n",
      "     |        call to the solver.\n",
      "     |      - first_step : float\n",
      "     |      - min_step : float\n",
      "     |      - max_step : float\n",
      "     |        Limits for the step sizes used by the integrator.\n",
      "     |      - order : int\n",
      "     |        Maximum order used by the integrator,\n",
      "     |        order <= 12 for Adams, <= 5 for BDF.\n",
      "     |  \n",
      "     |  \"zvode\"\n",
      "     |  \n",
      "     |      Complex-valued Variable-coefficient Ordinary Differential Equation\n",
      "     |      solver, with fixed-leading-coefficient implementation.  It provides\n",
      "     |      implicit Adams method (for non-stiff problems) and a method based on\n",
      "     |      backward differentiation formulas (BDF) (for stiff problems).\n",
      "     |  \n",
      "     |      Source: http://www.netlib.org/ode/zvode.f\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |  \n",
      "     |         This integrator is not re-entrant. You cannot have two `ode`\n",
      "     |         instances using the \"zvode\" integrator at the same time.\n",
      "     |  \n",
      "     |      This integrator accepts the same parameters in `set_integrator`\n",
      "     |      as the \"vode\" solver.\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |  \n",
      "     |          When using ZVODE for a stiff system, it should only be used for\n",
      "     |          the case in which the function f is analytic, that is, when each f(i)\n",
      "     |          is an analytic function of each y(j).  Analyticity means that the\n",
      "     |          partial derivative df(i)/dy(j) is a unique complex number, and this\n",
      "     |          fact is critical in the way ZVODE solves the dense or banded linear\n",
      "     |          systems that arise in the stiff case.  For a complex stiff ODE system\n",
      "     |          in which f is not analytic, ZVODE is likely to have convergence\n",
      "     |          failures, and for this problem one should instead use DVODE on the\n",
      "     |          equivalent real system (in the real and imaginary parts of y).\n",
      "     |  \n",
      "     |  \"lsoda\"\n",
      "     |  \n",
      "     |      Real-valued Variable-coefficient Ordinary Differential Equation\n",
      "     |      solver, with fixed-leading-coefficient implementation. It provides\n",
      "     |      automatic method switching between implicit Adams method (for non-stiff\n",
      "     |      problems) and a method based on backward differentiation formulas (BDF)\n",
      "     |      (for stiff problems).\n",
      "     |  \n",
      "     |      Source: http://www.netlib.org/odepack\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |  \n",
      "     |         This integrator is not re-entrant. You cannot have two `ode`\n",
      "     |         instances using the \"lsoda\" integrator at the same time.\n",
      "     |  \n",
      "     |      This integrator accepts the following parameters in `set_integrator`\n",
      "     |      method of the `ode` class:\n",
      "     |  \n",
      "     |      - atol : float or sequence\n",
      "     |        absolute tolerance for solution\n",
      "     |      - rtol : float or sequence\n",
      "     |        relative tolerance for solution\n",
      "     |      - lband : None or int\n",
      "     |      - uband : None or int\n",
      "     |        Jacobian band width, jac[i,j] != 0 for i-lband <= j <= i+uband.\n",
      "     |        Setting these requires your jac routine to return the jacobian\n",
      "     |        in packed format, jac_packed[i-j+uband, j] = jac[i,j].\n",
      "     |      - with_jacobian : bool\n",
      "     |        *Not used.*\n",
      "     |      - nsteps : int\n",
      "     |        Maximum number of (internally defined) steps allowed during one\n",
      "     |        call to the solver.\n",
      "     |      - first_step : float\n",
      "     |      - min_step : float\n",
      "     |      - max_step : float\n",
      "     |        Limits for the step sizes used by the integrator.\n",
      "     |      - max_order_ns : int\n",
      "     |        Maximum order used in the nonstiff case (default 12).\n",
      "     |      - max_order_s : int\n",
      "     |        Maximum order used in the stiff case (default 5).\n",
      "     |      - max_hnil : int\n",
      "     |        Maximum number of messages reporting too small step size (t + h = t)\n",
      "     |        (default 0)\n",
      "     |      - ixpr : int\n",
      "     |        Whether to generate extra printing at method switches (default False).\n",
      "     |  \n",
      "     |  \"dopri5\"\n",
      "     |  \n",
      "     |      This is an explicit runge-kutta method of order (4)5 due to Dormand &\n",
      "     |      Prince (with stepsize control and dense output).\n",
      "     |  \n",
      "     |      Authors:\n",
      "     |  \n",
      "     |          E. Hairer and G. Wanner\n",
      "     |          Universite de Geneve, Dept. de Mathematiques\n",
      "     |          CH-1211 Geneve 24, Switzerland\n",
      "     |          e-mail:  ernst.hairer@math.unige.ch, gerhard.wanner@math.unige.ch\n",
      "     |  \n",
      "     |      This code is described in [HNW93]_.\n",
      "     |  \n",
      "     |      This integrator accepts the following parameters in set_integrator()\n",
      "     |      method of the ode class:\n",
      "     |  \n",
      "     |      - atol : float or sequence\n",
      "     |        absolute tolerance for solution\n",
      "     |      - rtol : float or sequence\n",
      "     |        relative tolerance for solution\n",
      "     |      - nsteps : int\n",
      "     |        Maximum number of (internally defined) steps allowed during one\n",
      "     |        call to the solver.\n",
      "     |      - first_step : float\n",
      "     |      - max_step : float\n",
      "     |      - safety : float\n",
      "     |        Safety factor on new step selection (default 0.9)\n",
      "     |      - ifactor : float\n",
      "     |      - dfactor : float\n",
      "     |        Maximum factor to increase/decrease step size by in one step\n",
      "     |      - beta : float\n",
      "     |        Beta parameter for stabilised step size control.\n",
      "     |      - verbosity : int\n",
      "     |        Switch for printing messages (< 0 for no messages).\n",
      "     |  \n",
      "     |  \"dop853\"\n",
      "     |  \n",
      "     |      This is an explicit runge-kutta method of order 8(5,3) due to Dormand\n",
      "     |      & Prince (with stepsize control and dense output).\n",
      "     |  \n",
      "     |      Options and references the same as \"dopri5\".\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  \n",
      "     |  A problem to integrate and the corresponding jacobian:\n",
      "     |  \n",
      "     |  >>> from scipy.integrate import ode\n",
      "     |  >>>\n",
      "     |  >>> y0, t0 = [1.0j, 2.0], 0\n",
      "     |  >>>\n",
      "     |  >>> def f(t, y, arg1):\n",
      "     |  ...     return [1j*arg1*y[0] + y[1], -arg1*y[1]**2]\n",
      "     |  >>> def jac(t, y, arg1):\n",
      "     |  ...     return [[1j*arg1, 1], [0, -arg1*2*y[1]]]\n",
      "     |  \n",
      "     |  The integration:\n",
      "     |  \n",
      "     |  >>> r = ode(f, jac).set_integrator('zvode', method='bdf')\n",
      "     |  >>> r.set_initial_value(y0, t0).set_f_params(2.0).set_jac_params(2.0)\n",
      "     |  >>> t1 = 10\n",
      "     |  >>> dt = 1\n",
      "     |  >>> while r.successful() and r.t < t1:\n",
      "     |  ...     print(r.t+dt, r.integrate(r.t+dt))\n",
      "     |  (1, array([-0.71038232+0.23749653j,  0.40000271+0.j        ]))\n",
      "     |  (2.0, array([ 0.19098503-0.52359246j,  0.22222356+0.j        ]))\n",
      "     |  (3.0, array([ 0.47153208+0.52701229j,  0.15384681+0.j        ]))\n",
      "     |  (4.0, array([-0.61905937+0.30726255j,  0.11764744+0.j        ]))\n",
      "     |  (5.0, array([ 0.02340997-0.61418799j,  0.09523835+0.j        ]))\n",
      "     |  (6.0, array([ 0.58643071+0.339819j,  0.08000018+0.j      ]))\n",
      "     |  (7.0, array([-0.52070105+0.44525141j,  0.06896565+0.j        ]))\n",
      "     |  (8.0, array([-0.15986733-0.61234476j,  0.06060616+0.j        ]))\n",
      "     |  (9.0, array([ 0.64850462+0.15048982j,  0.05405414+0.j        ]))\n",
      "     |  (10.0, array([-0.38404699+0.56382299j,  0.04878055+0.j        ]))\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [HNW93] E. Hairer, S.P. Norsett and G. Wanner, Solving Ordinary\n",
      "     |      Differential Equations i. Nonstiff Problems. 2nd edition.\n",
      "     |      Springer Series in Computational Mathematics,\n",
      "     |      Springer-Verlag (1993)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, f, jac=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  integrate(self, t, step=0, relax=0)\n",
      "     |      Find y=y(t), set y as an initial condition, and return y.\n",
      "     |  \n",
      "     |  set_f_params(self, *args)\n",
      "     |      Set extra parameters for user-supplied function f.\n",
      "     |  \n",
      "     |  set_initial_value(self, y, t=0.0)\n",
      "     |      Set initial conditions y(t) = y.\n",
      "     |  \n",
      "     |  set_integrator(self, name, **integrator_params)\n",
      "     |      Set integrator by name.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          Name of the integrator.\n",
      "     |      integrator_params\n",
      "     |          Additional parameters for the integrator.\n",
      "     |  \n",
      "     |  set_jac_params(self, *args)\n",
      "     |      Set extra parameters for user-supplied function jac.\n",
      "     |  \n",
      "     |  set_solout(self, solout)\n",
      "     |      Set callable to be called at every successful integration step.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      solout : callable\n",
      "     |          ``solout(t, y)`` is called at each internal integrator step,\n",
      "     |          t is a scalar providing the current independent position\n",
      "     |          y is the current soloution ``y.shape == (n,)``\n",
      "     |          solout should return -1 to stop integration\n",
      "     |          otherwise it should return None or 0\n",
      "     |  \n",
      "     |  successful(self)\n",
      "     |      Check if integration was successful.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  y\n",
      "\n",
      "FUNCTIONS\n",
      "    cumtrapz(y, x=None, dx=1.0, axis=-1, initial=None)\n",
      "        Cumulatively integrate y(x) using the composite trapezoidal rule.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            Values to integrate.\n",
      "        x : array_like, optional\n",
      "            The coordinate to integrate along.  If None (default), use spacing `dx`\n",
      "            between consecutive elements in `y`.\n",
      "        dx : float, optional\n",
      "            Spacing between elements of `y`.  Only used if `x` is None.\n",
      "        axis : int, optional\n",
      "            Specifies the axis to cumulate.  Default is -1 (last axis).\n",
      "        initial : scalar, optional\n",
      "            If given, uses this value as the first value in the returned result.\n",
      "            Typically this value should be 0.  Default is None, which means no\n",
      "            value at ``x[0]`` is returned and `res` has one element less than `y`\n",
      "            along the axis of integration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : ndarray\n",
      "            The result of cumulative integration of `y` along `axis`.\n",
      "            If `initial` is None, the shape is such that the axis of integration\n",
      "            has one less value than `y`.  If `initial` is given, the shape is equal\n",
      "            to that of `y`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.cumsum, numpy.cumprod\n",
      "        quad: adaptive quadrature using QUADPACK\n",
      "        romberg: adaptive Romberg quadrature\n",
      "        quadrature: adaptive Gaussian quadrature\n",
      "        fixed_quad: fixed-order Gaussian quadrature\n",
      "        dblquad: double integrals\n",
      "        tplquad: triple integrals\n",
      "        romb: integrators for sampled data\n",
      "        ode: ODE integrators\n",
      "        odeint: ODE integrators\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import integrate\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        >>> x = np.linspace(-2, 2, num=20)\n",
      "        >>> y = x\n",
      "        >>> y_int = integrate.cumtrapz(y, x, initial=0)\n",
      "        >>> plt.plot(x, y_int, 'ro', x, y[0] + 0.5 * x**2, 'b-')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    dblquad(func, a, b, gfun, hfun, args=(), epsabs=1.49e-08, epsrel=1.49e-08)\n",
      "        Compute a double integral.\n",
      "        \n",
      "        Return the double (definite) integral of ``func(y, x)`` from ``x = a..b``\n",
      "        and ``y = gfun(x)..hfun(x)``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            A Python function or method of at least two variables: y must be the\n",
      "            first argument and x the second argument.\n",
      "        a, b : float\n",
      "            The limits of integration in x: `a` < `b`\n",
      "        gfun : callable\n",
      "            The lower boundary curve in y which is a function taking a single\n",
      "            floating point argument (x) and returning a floating point result: a\n",
      "            lambda function can be useful here.\n",
      "        hfun : callable\n",
      "            The upper boundary curve in y (same requirements as `gfun`).\n",
      "        args : sequence, optional\n",
      "            Extra arguments to pass to `func`.\n",
      "        epsabs : float, optional\n",
      "            Absolute tolerance passed directly to the inner 1-D quadrature\n",
      "            integration. Default is 1.49e-8.\n",
      "        epsrel : float, optional\n",
      "            Relative tolerance of the inner 1-D integrals. Default is 1.49e-8.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y : float\n",
      "            The resultant integral.\n",
      "        abserr : float\n",
      "            An estimate of the error.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        quad : single integral\n",
      "        tplquad : triple integral\n",
      "        nquad : N-dimensional integrals\n",
      "        fixed_quad : fixed-order Gaussian quadrature\n",
      "        quadrature : adaptive Gaussian quadrature\n",
      "        odeint : ODE integrator\n",
      "        ode : ODE integrator\n",
      "        simps : integrator for sampled data\n",
      "        romb : integrator for sampled data\n",
      "        scipy.special : for coefficients and roots of orthogonal polynomials\n",
      "    \n",
      "    fixed_quad(func, a, b, args=(), n=5)\n",
      "        Compute a definite integral using fixed-order Gaussian quadrature.\n",
      "        \n",
      "        Integrate `func` from `a` to `b` using Gaussian quadrature of\n",
      "        order `n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            A Python function or method to integrate (must accept vector inputs).\n",
      "            If integrating a vector-valued function, the returned array must have\n",
      "            shape ``(..., len(x))``.\n",
      "        a : float\n",
      "            Lower limit of integration.\n",
      "        b : float\n",
      "            Upper limit of integration.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to pass to function, if any.\n",
      "        n : int, optional\n",
      "            Order of quadrature integration. Default is 5.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        val : float\n",
      "            Gaussian quadrature approximation to the integral\n",
      "        none : None\n",
      "            Statically returned value of None\n",
      "        \n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        quad : adaptive quadrature using QUADPACK\n",
      "        dblquad : double integrals\n",
      "        tplquad : triple integrals\n",
      "        romberg : adaptive Romberg quadrature\n",
      "        quadrature : adaptive Gaussian quadrature\n",
      "        romb : integrators for sampled data\n",
      "        simps : integrators for sampled data\n",
      "        cumtrapz : cumulative integration for sampled data\n",
      "        ode : ODE integrator\n",
      "        odeint : ODE integrator\n",
      "    \n",
      "    newton_cotes(rn, equal=0)\n",
      "        Return weights and error coefficient for Newton-Cotes integration.\n",
      "        \n",
      "        Suppose we have (N+1) samples of f at the positions\n",
      "        x_0, x_1, ..., x_N.  Then an N-point Newton-Cotes formula for the\n",
      "        integral between x_0 and x_N is:\n",
      "        \n",
      "        :math:`\\int_{x_0}^{x_N} f(x)dx = \\Delta x \\sum_{i=0}^{N} a_i f(x_i)\n",
      "        + B_N (\\Delta x)^{N+2} f^{N+1} (\\xi)`\n",
      "        \n",
      "        where :math:`\\xi \\in [x_0,x_N]`\n",
      "        and :math:`\\Delta x = \\frac{x_N-x_0}{N}` is the average samples spacing.\n",
      "        \n",
      "        If the samples are equally-spaced and N is even, then the error\n",
      "        term is :math:`B_N (\\Delta x)^{N+3} f^{N+2}(\\xi)`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        rn : int\n",
      "            The integer order for equally-spaced data or the relative positions of\n",
      "            the samples with the first sample at 0 and the last at N, where N+1 is\n",
      "            the length of `rn`.  N is the order of the Newton-Cotes integration.\n",
      "        equal : int, optional\n",
      "            Set to 1 to enforce equally spaced data.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        an : ndarray\n",
      "            1-D array of weights to apply to the function at the provided sample\n",
      "            positions.\n",
      "        B : float\n",
      "            Error coefficient.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Normally, the Newton-Cotes rules are used on smaller integration\n",
      "        regions and a composite rule is used to return the total integral.\n",
      "    \n",
      "    nquad(func, ranges, args=None, opts=None, full_output=False)\n",
      "        Integration over multiple variables.\n",
      "        \n",
      "        Wraps `quad` to enable integration over multiple variables.\n",
      "        Various options allow improved integration of discontinuous functions, as\n",
      "        well as the use of weighted integration, and generally finer control of the\n",
      "        integration process.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : {callable, scipy.LowLevelCallable}\n",
      "            The function to be integrated. Has arguments of ``x0, ... xn``,\n",
      "            ``t0, tm``, where integration is carried out over ``x0, ... xn``, which\n",
      "            must be floats.  Function signature should be\n",
      "            ``func(x0, x1, ..., xn, t0, t1, ..., tm)``.  Integration is carried out\n",
      "            in order.  That is, integration over ``x0`` is the innermost integral,\n",
      "            and ``xn`` is the outermost.\n",
      "        \n",
      "            If the user desires improved integration performance, then `f` may\n",
      "            be a `scipy.LowLevelCallable` with one of the signatures::\n",
      "        \n",
      "                double func(int n, double *xx)\n",
      "                double func(int n, double *xx, void *user_data)\n",
      "        \n",
      "            where ``n`` is the number of extra parameters and args is an array\n",
      "            of doubles of the additional parameters, the ``xx`` array contains the \n",
      "            coordinates. The ``user_data`` is the data contained in the\n",
      "            `scipy.LowLevelCallable`.\n",
      "        ranges : iterable object\n",
      "            Each element of ranges may be either a sequence  of 2 numbers, or else\n",
      "            a callable that returns such a sequence.  ``ranges[0]`` corresponds to\n",
      "            integration over x0, and so on.  If an element of ranges is a callable,\n",
      "            then it will be called with all of the integration arguments available,\n",
      "            as well as any parametric arguments. e.g. if \n",
      "            ``func = f(x0, x1, x2, t0, t1)``, then ``ranges[0]`` may be defined as\n",
      "            either ``(a, b)`` or else as ``(a, b) = range0(x1, x2, t0, t1)``.\n",
      "        args : iterable object, optional\n",
      "            Additional arguments ``t0, ..., tn``, required by `func`, `ranges`, and\n",
      "            ``opts``.\n",
      "        opts : iterable object or dict, optional\n",
      "            Options to be passed to `quad`.  May be empty, a dict, or\n",
      "            a sequence of dicts or functions that return a dict.  If empty, the\n",
      "            default options from scipy.integrate.quad are used.  If a dict, the same\n",
      "            options are used for all levels of integraion.  If a sequence, then each\n",
      "            element of the sequence corresponds to a particular integration. e.g.\n",
      "            opts[0] corresponds to integration over x0, and so on. If a callable, \n",
      "            the signature must be the same as for ``ranges``. The available\n",
      "            options together with their default values are:\n",
      "        \n",
      "              - epsabs = 1.49e-08\n",
      "              - epsrel = 1.49e-08\n",
      "              - limit  = 50\n",
      "              - points = None\n",
      "              - weight = None\n",
      "              - wvar   = None\n",
      "              - wopts  = None\n",
      "        \n",
      "            For more information on these options, see `quad` and `quad_explain`.\n",
      "        \n",
      "        full_output : bool, optional\n",
      "            Partial implementation of ``full_output`` from scipy.integrate.quad. \n",
      "            The number of integrand function evaluations ``neval`` can be obtained \n",
      "            by setting ``full_output=True`` when calling nquad.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        result : float\n",
      "            The result of the integration.\n",
      "        abserr : float\n",
      "            The maximum of the estimates of the absolute error in the various\n",
      "            integration results.\n",
      "        out_dict : dict, optional\n",
      "            A dict containing additional information on the integration. \n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        quad : 1-dimensional numerical integration\n",
      "        dblquad, tplquad : double and triple integrals\n",
      "        fixed_quad : fixed-order Gaussian quadrature\n",
      "        quadrature : adaptive Gaussian quadrature\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import integrate\n",
      "        >>> func = lambda x0,x1,x2,x3 : x0**2 + x1*x2 - x3**3 + np.sin(x0) + (\n",
      "        ...                                 1 if (x0-.2*x3-.5-.25*x1>0) else 0)\n",
      "        >>> points = [[lambda x1,x2,x3 : 0.2*x3 + 0.5 + 0.25*x1], [], [], []]\n",
      "        >>> def opts0(*args, **kwargs):\n",
      "        ...     return {'points':[0.2*args[2] + 0.5 + 0.25*args[0]]}\n",
      "        >>> integrate.nquad(func, [[0,1], [-1,1], [.13,.8], [-.15,1]],\n",
      "        ...                 opts=[opts0,{},{},{}], full_output=True)\n",
      "        (1.5267454070738633, 2.9437360001402324e-14, {'neval': 388962})\n",
      "        \n",
      "        >>> scale = .1\n",
      "        >>> def func2(x0, x1, x2, x3, t0, t1):\n",
      "        ...     return x0*x1*x3**2 + np.sin(x2) + 1 + (1 if x0+t1*x1-t0>0 else 0)\n",
      "        >>> def lim0(x1, x2, x3, t0, t1):\n",
      "        ...     return [scale * (x1**2 + x2 + np.cos(x3)*t0*t1 + 1) - 1,\n",
      "        ...             scale * (x1**2 + x2 + np.cos(x3)*t0*t1 + 1) + 1]\n",
      "        >>> def lim1(x2, x3, t0, t1):\n",
      "        ...     return [scale * (t0*x2 + t1*x3) - 1,\n",
      "        ...             scale * (t0*x2 + t1*x3) + 1]\n",
      "        >>> def lim2(x3, t0, t1):\n",
      "        ...     return [scale * (x3 + t0**2*t1**3) - 1,\n",
      "        ...             scale * (x3 + t0**2*t1**3) + 1]\n",
      "        >>> def lim3(t0, t1):\n",
      "        ...     return [scale * (t0+t1) - 1, scale * (t0+t1) + 1]\n",
      "        >>> def opts0(x1, x2, x3, t0, t1):\n",
      "        ...     return {'points' : [t0 - t1*x1]}\n",
      "        >>> def opts1(x2, x3, t0, t1):\n",
      "        ...     return {}\n",
      "        >>> def opts2(x3, t0, t1):\n",
      "        ...     return {}\n",
      "        >>> def opts3(t0, t1):\n",
      "        ...     return {}\n",
      "        >>> integrate.nquad(func2, [lim0, lim1, lim2, lim3], args=(0,0),\n",
      "        ...                 opts=[opts0, opts1, opts2, opts3])\n",
      "        (25.066666666666666, 2.7829590483937256e-13)\n",
      "    \n",
      "    odeint(func, y0, t, args=(), Dfun=None, col_deriv=0, full_output=0, ml=None, mu=None, rtol=None, atol=None, tcrit=None, h0=0.0, hmax=0.0, hmin=0.0, ixpr=0, mxstep=0, mxhnil=0, mxordn=12, mxords=5, printmessg=0)\n",
      "        Integrate a system of ordinary differential equations.\n",
      "        \n",
      "        Solve a system of ordinary differential equations using lsoda from the\n",
      "        FORTRAN library odepack.\n",
      "        \n",
      "        Solves the initial value problem for stiff or non-stiff systems\n",
      "        of first order ode-s::\n",
      "        \n",
      "            dy/dt = func(y, t0, ...)\n",
      "        \n",
      "        where y can be a vector.\n",
      "        \n",
      "        *Note*: The first two arguments of ``func(y, t0, ...)`` are in the\n",
      "        opposite order of the arguments in the system definition function used\n",
      "        by the `scipy.integrate.ode` class.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable(y, t0, ...)\n",
      "            Computes the derivative of y at t0.\n",
      "        y0 : array\n",
      "            Initial condition on y (can be a vector).\n",
      "        t : array\n",
      "            A sequence of time points for which to solve for y.  The initial\n",
      "            value point should be the first element of this sequence.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to pass to function.\n",
      "        Dfun : callable(y, t0, ...)\n",
      "            Gradient (Jacobian) of `func`.\n",
      "        col_deriv : bool, optional\n",
      "            True if `Dfun` defines derivatives down columns (faster),\n",
      "            otherwise `Dfun` should define derivatives across rows.\n",
      "        full_output : bool, optional\n",
      "            True if to return a dictionary of optional outputs as the second output\n",
      "        printmessg : bool, optional\n",
      "            Whether to print the convergence message\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y : array, shape (len(t), len(y0))\n",
      "            Array containing the value of y for each desired time in t,\n",
      "            with the initial value `y0` in the first row.\n",
      "        infodict : dict, only returned if full_output == True\n",
      "            Dictionary containing additional output information\n",
      "        \n",
      "            =======  ============================================================\n",
      "            key      meaning\n",
      "            =======  ============================================================\n",
      "            'hu'     vector of step sizes successfully used for each time step.\n",
      "            'tcur'   vector with the value of t reached for each time step.\n",
      "                     (will always be at least as large as the input times).\n",
      "            'tolsf'  vector of tolerance scale factors, greater than 1.0,\n",
      "                     computed when a request for too much accuracy was detected.\n",
      "            'tsw'    value of t at the time of the last method switch\n",
      "                     (given for each time step)\n",
      "            'nst'    cumulative number of time steps\n",
      "            'nfe'    cumulative number of function evaluations for each time step\n",
      "            'nje'    cumulative number of jacobian evaluations for each time step\n",
      "            'nqu'    a vector of method orders for each successful step.\n",
      "            'imxer'  index of the component of largest magnitude in the\n",
      "                     weighted local error vector (e / ewt) on an error return, -1\n",
      "                     otherwise.\n",
      "            'lenrw'  the length of the double work array required.\n",
      "            'leniw'  the length of integer work array required.\n",
      "            'mused'  a vector of method indicators for each successful time step:\n",
      "                     1: adams (nonstiff), 2: bdf (stiff)\n",
      "            =======  ============================================================\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        ml, mu : int, optional\n",
      "            If either of these are not None or non-negative, then the\n",
      "            Jacobian is assumed to be banded.  These give the number of\n",
      "            lower and upper non-zero diagonals in this banded matrix.\n",
      "            For the banded case, `Dfun` should return a matrix whose\n",
      "            rows contain the non-zero bands (starting with the lowest diagonal).\n",
      "            Thus, the return matrix `jac` from `Dfun` should have shape\n",
      "            ``(ml + mu + 1, len(y0))`` when ``ml >=0`` or ``mu >=0``.\n",
      "            The data in `jac` must be stored such that ``jac[i - j + mu, j]``\n",
      "            holds the derivative of the `i`th equation with respect to the `j`th\n",
      "            state variable.  If `col_deriv` is True, the transpose of this\n",
      "            `jac` must be returned.\n",
      "        rtol, atol : float, optional\n",
      "            The input parameters `rtol` and `atol` determine the error\n",
      "            control performed by the solver.  The solver will control the\n",
      "            vector, e, of estimated local errors in y, according to an\n",
      "            inequality of the form ``max-norm of (e / ewt) <= 1``,\n",
      "            where ewt is a vector of positive error weights computed as\n",
      "            ``ewt = rtol * abs(y) + atol``.\n",
      "            rtol and atol can be either vectors the same length as y or scalars.\n",
      "            Defaults to 1.49012e-8.\n",
      "        tcrit : ndarray, optional\n",
      "            Vector of critical points (e.g. singularities) where integration\n",
      "            care should be taken.\n",
      "        h0 : float, (0: solver-determined), optional\n",
      "            The step size to be attempted on the first step.\n",
      "        hmax : float, (0: solver-determined), optional\n",
      "            The maximum absolute step size allowed.\n",
      "        hmin : float, (0: solver-determined), optional\n",
      "            The minimum absolute step size allowed.\n",
      "        ixpr : bool, optional\n",
      "            Whether to generate extra printing at method switches.\n",
      "        mxstep : int, (0: solver-determined), optional\n",
      "            Maximum number of (internally defined) steps allowed for each\n",
      "            integration point in t.\n",
      "        mxhnil : int, (0: solver-determined), optional\n",
      "            Maximum number of messages printed.\n",
      "        mxordn : int, (0: solver-determined), optional\n",
      "            Maximum order to be allowed for the non-stiff (Adams) method.\n",
      "        mxords : int, (0: solver-determined), optional\n",
      "            Maximum order to be allowed for the stiff (BDF) method.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ode : a more object-oriented integrator based on VODE.\n",
      "        quad : for finding the area under a curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The second order differential equation for the angle `theta` of a\n",
      "        pendulum acted on by gravity with friction can be written::\n",
      "        \n",
      "            theta''(t) + b*theta'(t) + c*sin(theta(t)) = 0\n",
      "        \n",
      "        where `b` and `c` are positive constants, and a prime (') denotes a\n",
      "        derivative.  To solve this equation with `odeint`, we must first convert\n",
      "        it to a system of first order equations.  By defining the angular\n",
      "        velocity ``omega(t) = theta'(t)``, we obtain the system::\n",
      "        \n",
      "            theta'(t) = omega(t)\n",
      "            omega'(t) = -b*omega(t) - c*sin(theta(t))\n",
      "        \n",
      "        Let `y` be the vector [`theta`, `omega`].  We implement this system\n",
      "        in python as:\n",
      "        \n",
      "        >>> def pend(y, t, b, c):\n",
      "        ...     theta, omega = y\n",
      "        ...     dydt = [omega, -b*omega - c*np.sin(theta)]\n",
      "        ...     return dydt\n",
      "        ...\n",
      "        \n",
      "        We assume the constants are `b` = 0.25 and `c` = 5.0:\n",
      "        \n",
      "        >>> b = 0.25\n",
      "        >>> c = 5.0\n",
      "        \n",
      "        For initial conditions, we assume the pendulum is nearly vertical\n",
      "        with `theta(0)` = `pi` - 0.1, and it initially at rest, so\n",
      "        `omega(0)` = 0.  Then the vector of initial conditions is\n",
      "        \n",
      "        >>> y0 = [np.pi - 0.1, 0.0]\n",
      "        \n",
      "        We generate a solution 101 evenly spaced samples in the interval\n",
      "        0 <= `t` <= 10.  So our array of times is:\n",
      "        \n",
      "        >>> t = np.linspace(0, 10, 101)\n",
      "        \n",
      "        Call `odeint` to generate the solution.  To pass the parameters\n",
      "        `b` and `c` to `pend`, we give them to `odeint` using the `args`\n",
      "        argument.\n",
      "        \n",
      "        >>> from scipy.integrate import odeint\n",
      "        >>> sol = odeint(pend, y0, t, args=(b, c))\n",
      "        \n",
      "        The solution is an array with shape (101, 2).  The first column\n",
      "        is `theta(t)`, and the second is `omega(t)`.  The following code\n",
      "        plots both components.\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> plt.plot(t, sol[:, 0], 'b', label='theta(t)')\n",
      "        >>> plt.plot(t, sol[:, 1], 'g', label='omega(t)')\n",
      "        >>> plt.legend(loc='best')\n",
      "        >>> plt.xlabel('t')\n",
      "        >>> plt.grid()\n",
      "        >>> plt.show()\n",
      "    \n",
      "    quad(func, a, b, args=(), full_output=0, epsabs=1.49e-08, epsrel=1.49e-08, limit=50, points=None, weight=None, wvar=None, wopts=None, maxp1=50, limlst=50)\n",
      "        Compute a definite integral.\n",
      "        \n",
      "        Integrate func from `a` to `b` (possibly infinite interval) using a\n",
      "        technique from the Fortran library QUADPACK.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : {function, scipy.LowLevelCallable}\n",
      "            A Python function or method to integrate.  If `func` takes many\n",
      "            arguments, it is integrated along the axis corresponding to the\n",
      "            first argument.\n",
      "        \n",
      "            If the user desires improved integration performance, then `f` may\n",
      "            be a `scipy.LowLevelCallable` with one of the signatures::\n",
      "        \n",
      "                double func(double x)\n",
      "                double func(double x, void *user_data)\n",
      "                double func(int n, double *xx)\n",
      "                double func(int n, double *xx, void *user_data)\n",
      "        \n",
      "            The ``user_data`` is the data contained in the `scipy.LowLevelCallable`.\n",
      "            In the call forms with ``xx``,  ``n`` is the length of the ``xx`` \n",
      "            array which contains ``xx[0] == x`` and the rest of the items are\n",
      "            numbers contained in the ``args`` argument of quad.\n",
      "        \n",
      "            In addition, certain ctypes call signatures are supported for \n",
      "            backward compatibility, but those should not be used in new code.\n",
      "        a : float\n",
      "            Lower limit of integration (use -numpy.inf for -infinity).\n",
      "        b : float\n",
      "            Upper limit of integration (use numpy.inf for +infinity).\n",
      "        args : tuple, optional\n",
      "            Extra arguments to pass to `func`.\n",
      "        full_output : int, optional\n",
      "            Non-zero to return a dictionary of integration information.\n",
      "            If non-zero, warning messages are also suppressed and the\n",
      "            message is appended to the output tuple.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y : float\n",
      "            The integral of func from `a` to `b`.\n",
      "        abserr : float\n",
      "            An estimate of the absolute error in the result.\n",
      "        infodict : dict\n",
      "            A dictionary containing additional information.\n",
      "            Run scipy.integrate.quad_explain() for more information.\n",
      "        message\n",
      "            A convergence message.\n",
      "        explain\n",
      "            Appended only with 'cos' or 'sin' weighting and infinite\n",
      "            integration limits, it contains an explanation of the codes in\n",
      "            infodict['ierlst']\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        epsabs : float or int, optional\n",
      "            Absolute error tolerance.\n",
      "        epsrel : float or int, optional\n",
      "            Relative error tolerance.\n",
      "        limit : float or int, optional\n",
      "            An upper bound on the number of subintervals used in the adaptive\n",
      "            algorithm.\n",
      "        points : (sequence of floats,ints), optional\n",
      "            A sequence of break points in the bounded integration interval\n",
      "            where local difficulties of the integrand may occur (e.g.,\n",
      "            singularities, discontinuities). The sequence does not have\n",
      "            to be sorted.\n",
      "        weight : float or int, optional\n",
      "            String indicating weighting function. Full explanation for this\n",
      "            and the remaining arguments can be found below.\n",
      "        wvar : optional\n",
      "            Variables for use with weighting functions.\n",
      "        wopts : optional\n",
      "            Optional input for reusing Chebyshev moments.\n",
      "        maxp1 : float or int, optional\n",
      "            An upper bound on the number of Chebyshev moments.\n",
      "        limlst : int, optional\n",
      "            Upper bound on the number of cycles (>=3) for use with a sinusoidal\n",
      "            weighting and an infinite end-point.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        dblquad : double integral\n",
      "        tplquad : triple integral\n",
      "        nquad : n-dimensional integrals (uses `quad` recursively)\n",
      "        fixed_quad : fixed-order Gaussian quadrature\n",
      "        quadrature : adaptive Gaussian quadrature\n",
      "        odeint : ODE integrator\n",
      "        ode : ODE integrator\n",
      "        simps : integrator for sampled data\n",
      "        romb : integrator for sampled data\n",
      "        scipy.special : for coefficients and roots of orthogonal polynomials\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        **Extra information for quad() inputs and outputs**\n",
      "        \n",
      "        If full_output is non-zero, then the third output argument\n",
      "        (infodict) is a dictionary with entries as tabulated below.  For\n",
      "        infinite limits, the range is transformed to (0,1) and the\n",
      "        optional outputs are given with respect to this transformed range.\n",
      "        Let M be the input argument limit and let K be infodict['last'].\n",
      "        The entries are:\n",
      "        \n",
      "        'neval'\n",
      "            The number of function evaluations.\n",
      "        'last'\n",
      "            The number, K, of subintervals produced in the subdivision process.\n",
      "        'alist'\n",
      "            A rank-1 array of length M, the first K elements of which are the\n",
      "            left end points of the subintervals in the partition of the\n",
      "            integration range.\n",
      "        'blist'\n",
      "            A rank-1 array of length M, the first K elements of which are the\n",
      "            right end points of the subintervals.\n",
      "        'rlist'\n",
      "            A rank-1 array of length M, the first K elements of which are the\n",
      "            integral approximations on the subintervals.\n",
      "        'elist'\n",
      "            A rank-1 array of length M, the first K elements of which are the\n",
      "            moduli of the absolute error estimates on the subintervals.\n",
      "        'iord'\n",
      "            A rank-1 integer array of length M, the first L elements of\n",
      "            which are pointers to the error estimates over the subintervals\n",
      "            with ``L=K`` if ``K<=M/2+2`` or ``L=M+1-K`` otherwise. Let I be the\n",
      "            sequence ``infodict['iord']`` and let E be the sequence\n",
      "            ``infodict['elist']``.  Then ``E[I[1]], ..., E[I[L]]`` forms a\n",
      "            decreasing sequence.\n",
      "        \n",
      "        If the input argument points is provided (i.e. it is not None),\n",
      "        the following additional outputs are placed in the output\n",
      "        dictionary.  Assume the points sequence is of length P.\n",
      "        \n",
      "        'pts'\n",
      "            A rank-1 array of length P+2 containing the integration limits\n",
      "            and the break points of the intervals in ascending order.\n",
      "            This is an array giving the subintervals over which integration\n",
      "            will occur.\n",
      "        'level'\n",
      "            A rank-1 integer array of length M (=limit), containing the\n",
      "            subdivision levels of the subintervals, i.e., if (aa,bb) is a\n",
      "            subinterval of ``(pts[1], pts[2])`` where ``pts[0]`` and ``pts[2]``\n",
      "            are adjacent elements of ``infodict['pts']``, then (aa,bb) has level l\n",
      "            if ``|bb-aa| = |pts[2]-pts[1]| * 2**(-l)``.\n",
      "        'ndin'\n",
      "            A rank-1 integer array of length P+2.  After the first integration\n",
      "            over the intervals (pts[1], pts[2]), the error estimates over some\n",
      "            of the intervals may have been increased artificially in order to\n",
      "            put their subdivision forward.  This array has ones in slots\n",
      "            corresponding to the subintervals for which this happens.\n",
      "        \n",
      "        **Weighting the integrand**\n",
      "        \n",
      "        The input variables, *weight* and *wvar*, are used to weight the\n",
      "        integrand by a select list of functions.  Different integration\n",
      "        methods are used to compute the integral with these weighting\n",
      "        functions.  The possible values of weight and the corresponding\n",
      "        weighting functions are.\n",
      "        \n",
      "        ==========  ===================================   =====================\n",
      "        ``weight``  Weight function used                  ``wvar``\n",
      "        ==========  ===================================   =====================\n",
      "        'cos'       cos(w*x)                              wvar = w\n",
      "        'sin'       sin(w*x)                              wvar = w\n",
      "        'alg'       g(x) = ((x-a)**alpha)*((b-x)**beta)   wvar = (alpha, beta)\n",
      "        'alg-loga'  g(x)*log(x-a)                         wvar = (alpha, beta)\n",
      "        'alg-logb'  g(x)*log(b-x)                         wvar = (alpha, beta)\n",
      "        'alg-log'   g(x)*log(x-a)*log(b-x)                wvar = (alpha, beta)\n",
      "        'cauchy'    1/(x-c)                               wvar = c\n",
      "        ==========  ===================================   =====================\n",
      "        \n",
      "        wvar holds the parameter w, (alpha, beta), or c depending on the weight\n",
      "        selected.  In these expressions, a and b are the integration limits.\n",
      "        \n",
      "        For the 'cos' and 'sin' weighting, additional inputs and outputs are\n",
      "        available.\n",
      "        \n",
      "        For finite integration limits, the integration is performed using a\n",
      "        Clenshaw-Curtis method which uses Chebyshev moments.  For repeated\n",
      "        calculations, these moments are saved in the output dictionary:\n",
      "        \n",
      "        'momcom'\n",
      "            The maximum level of Chebyshev moments that have been computed,\n",
      "            i.e., if ``M_c`` is ``infodict['momcom']`` then the moments have been\n",
      "            computed for intervals of length ``|b-a| * 2**(-l)``,\n",
      "            ``l=0,1,...,M_c``.\n",
      "        'nnlog'\n",
      "            A rank-1 integer array of length M(=limit), containing the\n",
      "            subdivision levels of the subintervals, i.e., an element of this\n",
      "            array is equal to l if the corresponding subinterval is\n",
      "            ``|b-a|* 2**(-l)``.\n",
      "        'chebmo'\n",
      "            A rank-2 array of shape (25, maxp1) containing the computed\n",
      "            Chebyshev moments.  These can be passed on to an integration\n",
      "            over the same interval by passing this array as the second\n",
      "            element of the sequence wopts and passing infodict['momcom'] as\n",
      "            the first element.\n",
      "        \n",
      "        If one of the integration limits is infinite, then a Fourier integral is\n",
      "        computed (assuming w neq 0).  If full_output is 1 and a numerical error\n",
      "        is encountered, besides the error message attached to the output tuple,\n",
      "        a dictionary is also appended to the output tuple which translates the\n",
      "        error codes in the array ``info['ierlst']`` to English messages.  The\n",
      "        output information dictionary contains the following entries instead of\n",
      "        'last', 'alist', 'blist', 'rlist', and 'elist':\n",
      "        \n",
      "        'lst'\n",
      "            The number of subintervals needed for the integration (call it ``K_f``).\n",
      "        'rslst'\n",
      "            A rank-1 array of length M_f=limlst, whose first ``K_f`` elements\n",
      "            contain the integral contribution over the interval\n",
      "            ``(a+(k-1)c, a+kc)`` where ``c = (2*floor(|w|) + 1) * pi / |w|``\n",
      "            and ``k=1,2,...,K_f``.\n",
      "        'erlst'\n",
      "            A rank-1 array of length ``M_f`` containing the error estimate\n",
      "            corresponding to the interval in the same position in\n",
      "            ``infodict['rslist']``.\n",
      "        'ierlst'\n",
      "            A rank-1 integer array of length ``M_f`` containing an error flag\n",
      "            corresponding to the interval in the same position in\n",
      "            ``infodict['rslist']``.  See the explanation dictionary (last entry\n",
      "            in the output tuple) for the meaning of the codes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Calculate :math:`\\int^4_0 x^2 dx` and compare with an analytic result\n",
      "        \n",
      "        >>> from scipy import integrate\n",
      "        >>> x2 = lambda x: x**2\n",
      "        >>> integrate.quad(x2, 0, 4)\n",
      "        (21.333333333333332, 2.3684757858670003e-13)\n",
      "        >>> print(4**3 / 3.)  # analytical result\n",
      "        21.3333333333\n",
      "        \n",
      "        Calculate :math:`\\int^\\infty_0 e^{-x} dx`\n",
      "        \n",
      "        >>> invexp = lambda x: np.exp(-x)\n",
      "        >>> integrate.quad(invexp, 0, np.inf)\n",
      "        (1.0, 5.842605999138044e-11)\n",
      "        \n",
      "        >>> f = lambda x,a : a*x\n",
      "        >>> y, err = integrate.quad(f, 0, 1, args=(1,))\n",
      "        >>> y\n",
      "        0.5\n",
      "        >>> y, err = integrate.quad(f, 0, 1, args=(3,))\n",
      "        >>> y\n",
      "        1.5\n",
      "        \n",
      "        Calculate :math:`\\int^1_0 x^2 + y^2 dx` with ctypes, holding\n",
      "        y parameter as 1::\n",
      "        \n",
      "            testlib.c =>\n",
      "                double func(int n, double args[n]){\n",
      "                    return args[0]*args[0] + args[1]*args[1];}\n",
      "            compile to library testlib.*\n",
      "        \n",
      "        ::\n",
      "        \n",
      "           from scipy import integrate\n",
      "           import ctypes\n",
      "           lib = ctypes.CDLL('/home/.../testlib.*') #use absolute path\n",
      "           lib.func.restype = ctypes.c_double\n",
      "           lib.func.argtypes = (ctypes.c_int,ctypes.c_double)\n",
      "           integrate.quad(lib.func,0,1,(1))\n",
      "           #(1.3333333333333333, 1.4802973661668752e-14)\n",
      "           print((1.0**3/3.0 + 1.0) - (0.0**3/3.0 + 0.0)) #Analytic result\n",
      "           # 1.3333333333333333\n",
      "    \n",
      "    quad_explain(output=<ipykernel.iostream.OutStream object at 0x0000000006756EB8>)\n",
      "        Print extra information about integrate.quad() parameters and returns.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        output : instance with \"write\" method, optional\n",
      "            Information about `quad` is passed to ``output.write()``.\n",
      "            Default is ``sys.stdout``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        None\n",
      "    \n",
      "    quadrature(func, a, b, args=(), tol=1.49e-08, rtol=1.49e-08, maxiter=50, vec_func=True, miniter=1)\n",
      "        Compute a definite integral using fixed-tolerance Gaussian quadrature.\n",
      "        \n",
      "        Integrate `func` from `a` to `b` using Gaussian quadrature\n",
      "        with absolute tolerance `tol`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            A Python function or method to integrate.\n",
      "        a : float\n",
      "            Lower limit of integration.\n",
      "        b : float\n",
      "            Upper limit of integration.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to pass to function.\n",
      "        tol, rtol : float, optional\n",
      "            Iteration stops when error between last two iterates is less than\n",
      "            `tol` OR the relative change is less than `rtol`.\n",
      "        maxiter : int, optional\n",
      "            Maximum order of Gaussian quadrature.\n",
      "        vec_func : bool, optional\n",
      "            True or False if func handles arrays as arguments (is\n",
      "            a \"vector\" function). Default is True.\n",
      "        miniter : int, optional\n",
      "            Minimum order of Gaussian quadrature.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        val : float\n",
      "            Gaussian quadrature approximation (within tolerance) to integral.\n",
      "        err : float\n",
      "            Difference between last two estimates of the integral.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        romberg: adaptive Romberg quadrature\n",
      "        fixed_quad: fixed-order Gaussian quadrature\n",
      "        quad: adaptive quadrature using QUADPACK\n",
      "        dblquad: double integrals\n",
      "        tplquad: triple integrals\n",
      "        romb: integrator for sampled data\n",
      "        simps: integrator for sampled data\n",
      "        cumtrapz: cumulative integration for sampled data\n",
      "        ode: ODE integrator\n",
      "        odeint: ODE integrator\n",
      "    \n",
      "    romb(y, dx=1.0, axis=-1, show=False)\n",
      "        Romberg integration using samples of a function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            A vector of ``2**k + 1`` equally-spaced samples of a function.\n",
      "        dx : float, optional\n",
      "            The sample spacing. Default is 1.\n",
      "        axis : int, optional\n",
      "            The axis along which to integrate. Default is -1 (last axis).\n",
      "        show : bool, optional\n",
      "            When `y` is a single 1-D array, then if this argument is True\n",
      "            print the table showing Richardson extrapolation from the\n",
      "            samples. Default is False.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        romb : ndarray\n",
      "            The integrated result for `axis`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        quad : adaptive quadrature using QUADPACK\n",
      "        romberg : adaptive Romberg quadrature\n",
      "        quadrature : adaptive Gaussian quadrature\n",
      "        fixed_quad : fixed-order Gaussian quadrature\n",
      "        dblquad : double integrals\n",
      "        tplquad : triple integrals\n",
      "        simps : integrators for sampled data\n",
      "        cumtrapz : cumulative integration for sampled data\n",
      "        ode : ODE integrators\n",
      "        odeint : ODE integrators\n",
      "    \n",
      "    romberg(function, a, b, args=(), tol=1.48e-08, rtol=1.48e-08, show=False, divmax=10, vec_func=False)\n",
      "        Romberg integration of a callable function or method.\n",
      "        \n",
      "        Returns the integral of `function` (a function of one variable)\n",
      "        over the interval (`a`, `b`).\n",
      "        \n",
      "        If `show` is 1, the triangular array of the intermediate results\n",
      "        will be printed.  If `vec_func` is True (default is False), then\n",
      "        `function` is assumed to support vector arguments.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        function : callable\n",
      "            Function to be integrated.\n",
      "        a : float\n",
      "            Lower limit of integration.\n",
      "        b : float\n",
      "            Upper limit of integration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        results  : float\n",
      "            Result of the integration.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        args : tuple, optional\n",
      "            Extra arguments to pass to function. Each element of `args` will\n",
      "            be passed as a single argument to `func`. Default is to pass no\n",
      "            extra arguments.\n",
      "        tol, rtol : float, optional\n",
      "            The desired absolute and relative tolerances. Defaults are 1.48e-8.\n",
      "        show : bool, optional\n",
      "            Whether to print the results. Default is False.\n",
      "        divmax : int, optional\n",
      "            Maximum order of extrapolation. Default is 10.\n",
      "        vec_func : bool, optional\n",
      "            Whether `func` handles arrays as arguments (i.e whether it is a\n",
      "            \"vector\" function). Default is False.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fixed_quad : Fixed-order Gaussian quadrature.\n",
      "        quad : Adaptive quadrature using QUADPACK.\n",
      "        dblquad : Double integrals.\n",
      "        tplquad : Triple integrals.\n",
      "        romb : Integrators for sampled data.\n",
      "        simps : Integrators for sampled data.\n",
      "        cumtrapz : Cumulative integration for sampled data.\n",
      "        ode : ODE integrator.\n",
      "        odeint : ODE integrator.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] 'Romberg's method' http://en.wikipedia.org/wiki/Romberg%27s_method\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Integrate a gaussian from 0 to 1 and compare to the error function.\n",
      "        \n",
      "        >>> from scipy import integrate\n",
      "        >>> from scipy.special import erf\n",
      "        >>> gaussian = lambda x: 1/np.sqrt(np.pi) * np.exp(-x**2)\n",
      "        >>> result = integrate.romberg(gaussian, 0, 1, show=True)\n",
      "        Romberg integration of <function vfunc at ...> from [0, 1]\n",
      "        \n",
      "        ::\n",
      "        \n",
      "           Steps  StepSize  Results\n",
      "               1  1.000000  0.385872\n",
      "               2  0.500000  0.412631  0.421551\n",
      "               4  0.250000  0.419184  0.421368  0.421356\n",
      "               8  0.125000  0.420810  0.421352  0.421350  0.421350\n",
      "              16  0.062500  0.421215  0.421350  0.421350  0.421350  0.421350\n",
      "              32  0.031250  0.421317  0.421350  0.421350  0.421350  0.421350  0.421350\n",
      "        \n",
      "        The final result is 0.421350396475 after 33 function evaluations.\n",
      "        \n",
      "        >>> print(\"%g %g\" % (2*result, erf(1)))\n",
      "        0.842701 0.842701\n",
      "    \n",
      "    simps(y, x=None, dx=1, axis=-1, even='avg')\n",
      "        Integrate y(x) using samples along the given axis and the composite\n",
      "        Simpson's rule.  If x is None, spacing of dx is assumed.\n",
      "        \n",
      "        If there are an even number of samples, N, then there are an odd\n",
      "        number of intervals (N-1), but Simpson's rule requires an even number\n",
      "        of intervals.  The parameter 'even' controls how this is handled.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            Array to be integrated.\n",
      "        x : array_like, optional\n",
      "            If given, the points at which `y` is sampled.\n",
      "        dx : int, optional\n",
      "            Spacing of integration points along axis of `y`. Only used when\n",
      "            `x` is None. Default is 1.\n",
      "        axis : int, optional\n",
      "            Axis along which to integrate. Default is the last axis.\n",
      "        even : str {'avg', 'first', 'last'}, optional\n",
      "            'avg' : Average two results:1) use the first N-2 intervals with\n",
      "                      a trapezoidal rule on the last interval and 2) use the last\n",
      "                      N-2 intervals with a trapezoidal rule on the first interval.\n",
      "        \n",
      "            'first' : Use Simpson's rule for the first N-2 intervals with\n",
      "                    a trapezoidal rule on the last interval.\n",
      "        \n",
      "            'last' : Use Simpson's rule for the last N-2 intervals with a\n",
      "                   trapezoidal rule on the first interval.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        quad: adaptive quadrature using QUADPACK\n",
      "        romberg: adaptive Romberg quadrature\n",
      "        quadrature: adaptive Gaussian quadrature\n",
      "        fixed_quad: fixed-order Gaussian quadrature\n",
      "        dblquad: double integrals\n",
      "        tplquad: triple integrals\n",
      "        romb: integrators for sampled data\n",
      "        cumtrapz: cumulative integration for sampled data\n",
      "        ode: ODE integrators\n",
      "        odeint: ODE integrators\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an odd number of samples that are equally spaced the result is\n",
      "        exact if the function is a polynomial of order 3 or less.  If\n",
      "        the samples are not equally spaced, then the result is exact only\n",
      "        if the function is a polynomial of order 2 or less.\n",
      "    \n",
      "    solve_bvp(fun, bc, x, y, p=None, S=None, fun_jac=None, bc_jac=None, tol=0.001, max_nodes=1000, verbose=0)\n",
      "        Solve a boundary-value problem for a system of ODEs.\n",
      "        \n",
      "        This function numerically solves a first order system of ODEs subject to\n",
      "        two-point boundary conditions::\n",
      "        \n",
      "            dy / dx = f(x, y, p) + S * y / (x - a), a <= x <= b\n",
      "            bc(y(a), y(b), p) = 0\n",
      "        \n",
      "        Here x is a 1-dimensional independent variable, y(x) is a n-dimensional\n",
      "        vector-valued function and p is a k-dimensional vector of unknown\n",
      "        parameters which is to be found along with y(x). For the problem to be\n",
      "        determined there must be n + k boundary conditions, i.e. bc must be\n",
      "        (n + k)-dimensional function.\n",
      "        \n",
      "        The last singular term in the right-hand side of the system is optional.\n",
      "        It is defined by an n-by-n matrix S, such that the solution must satisfy\n",
      "        S y(a) = 0. This condition will be forced during iterations, so it must not\n",
      "        contradict boundary conditions. See [2]_ for the explanation how this term\n",
      "        is handled when solving BVPs numerically.\n",
      "        \n",
      "        Problems in a complex domain can be solved as well. In this case y and p\n",
      "        are considered to be complex, and f and bc are assumed to be complex-valued\n",
      "        functions, but x stays real. Note that f and bc must be complex\n",
      "        differentiable (satisfy Cauchy-Riemann equations [4]_), otherwise you\n",
      "        should rewrite your problem for real and imaginary parts separately. To\n",
      "        solve a problem in a complex domain, pass an initial guess for y with a\n",
      "        complex data type (see below).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Right-hand side of the system. The calling signature is ``fun(x, y)``,\n",
      "            or ``fun(x, y, p)`` if parameters are present. All arguments are\n",
      "            ndarray: ``x`` with shape (m,), ``y`` with shape (n, m), meaning that\n",
      "            ``y[:, i]`` corresponds to ``x[i]``, and ``p`` with shape (k,). The\n",
      "            return value must be an array with shape (n, m) and with the same\n",
      "            layout as ``y``.\n",
      "        bc : callable\n",
      "            Function evaluating residuals of the boundary conditions. The calling\n",
      "            signature is ``bc(ya, yb)``, or ``bc(ya, yb, p)`` if parameters are\n",
      "            present. All arguments are ndarray: ``ya`` and ``yb`` with shape (n,),\n",
      "            and ``p`` with shape (k,). The return value must be an array with\n",
      "            shape (n + k,).\n",
      "        x : array_like, shape (m,)\n",
      "            Initial mesh. Must be a strictly increasing sequence of real numbers\n",
      "            with ``x[0]=a`` and ``x[-1]=b``.\n",
      "        y : array_like, shape (n, m)\n",
      "            Initial guess for the function values at the mesh nodes, i-th column\n",
      "            corresponds to ``x[i]``. For problems in a complex domain pass `y`\n",
      "            with a complex data type (even if the initial guess is purely real).\n",
      "        p : array_like with shape (k,) or None, optional\n",
      "            Initial guess for the unknown parameters. If None (default), it is\n",
      "            assumed that the problem doesn't depend on any parameters.\n",
      "        S : array_like with shape (n, n) or None\n",
      "            Matrix defining the singular term. If None (default), the problem is\n",
      "            solved without the singular term.\n",
      "        fun_jac : callable or None, optional\n",
      "            Function computing derivatives of f with respect to y and p. The\n",
      "            calling signature is ``fun_jac(x, y)``, or ``fun_jac(x, y, p)`` if\n",
      "            parameters are present. The return must contain 1 or 2 elements in the\n",
      "            following order:\n",
      "        \n",
      "                * df_dy : array_like with shape (n, n, m) where an element\n",
      "                  (i, j, q) equals to d f_i(x_q, y_q, p) / d (y_q)_j.\n",
      "                * df_dp : array_like with shape (n, k, m) where an element\n",
      "                  (i, j, q) equals to d f_i(x_q, y_q, p) / d p_j.\n",
      "        \n",
      "            Here q numbers nodes at which x and y are defined, whereas i and j\n",
      "            number vector components. If the problem is solved without unknown\n",
      "            parameters df_dp should not be returned.\n",
      "        \n",
      "            If `fun_jac` is None (default), the derivatives will be estimated\n",
      "            by the forward finite differences.\n",
      "        bc_jac : callable or None, optional\n",
      "            Function computing derivatives of bc with respect to ya, yb and p.\n",
      "            The calling signature is ``bc_jac(ya, yb)``, or ``bc_jac(ya, yb, p)``\n",
      "            if parameters are present. The return must contain 2 or 3 elements in\n",
      "            the following order:\n",
      "        \n",
      "                * dbc_dya : array_like with shape (n, n) where an element (i, j)\n",
      "                  equals to d bc_i(ya, yb, p) / d ya_j.\n",
      "                * dbc_dyb : array_like with shape (n, n) where an element (i, j)\n",
      "                  equals to d bc_i(ya, yb, p) / d yb_j.\n",
      "                * dbc_dp : array_like with shape (n, k) where an element (i, j)\n",
      "                  equals to d bc_i(ya, yb, p) / d p_j.\n",
      "        \n",
      "            If the problem is solved without unknown parameters dbc_dp should not\n",
      "            be returned.\n",
      "        \n",
      "            If `bc_jac` is None (default), the derivatives will be estimated by\n",
      "            the forward finite differences.\n",
      "        tol : float, optional\n",
      "            Desired tolerance of the solution. If we define ``r = y' - f(x, y)``\n",
      "            where y is the found solution, then the solver tries to achieve on each\n",
      "            mesh interval ``norm(r / (1 + abs(f)) < tol``, where ``norm`` is\n",
      "            estimated in a root mean squared sense (using a numerical quadrature\n",
      "            formula). Default is 1e-3.\n",
      "        max_nodes : int, optional\n",
      "            Maximum allowed number of the mesh nodes. If exceeded, the algorithm\n",
      "            terminates. Default is 1000.\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "        \n",
      "                * 0 (default) : work silently.\n",
      "                * 1 : display a termination report.\n",
      "                * 2 : display progress during iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Bunch object with the following fields defined:\n",
      "        sol : PPoly\n",
      "            Found solution for y as `scipy.interpolate.PPoly` instance, a C1\n",
      "            continuous cubic spline.\n",
      "        p : ndarray or None, shape (k,)\n",
      "            Found parameters. None, if the parameters were not present in the\n",
      "            problem.\n",
      "        x : ndarray, shape (m,)\n",
      "            Nodes of the final mesh.\n",
      "        y : ndarray, shape (n, m)\n",
      "            Solution values at the mesh nodes.\n",
      "        yp : ndarray, shape (n, m)\n",
      "            Solution derivatives at the mesh nodes.\n",
      "        rms_residuals : ndarray, shape (m - 1,)\n",
      "            RMS values of the relative residuals over each mesh interval (see the\n",
      "            description of `tol` parameter).\n",
      "        niter : int\n",
      "            Number of completed iterations.\n",
      "        status : int\n",
      "            Reason for algorithm termination:\n",
      "        \n",
      "                * 0: The algorithm converged to the desired accuracy.\n",
      "                * 1: The maximum number of mesh nodes is exceeded.\n",
      "                * 2: A singular Jacobian encountered when solving the collocation\n",
      "                  system.\n",
      "        \n",
      "        message : string\n",
      "            Verbal description of the termination reason.\n",
      "        success : bool\n",
      "            True if the algorithm converged to the desired accuracy (``status=0``).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements a 4-th order collocation algorithm with the\n",
      "        control of residuals similar to [1]_. A collocation system is solved\n",
      "        by a damped Newton method with an affine-invariant criterion function as\n",
      "        described in [3]_.\n",
      "        \n",
      "        Note that in [1]_  integral residuals are defined without normalization\n",
      "        by interval lengths. So their definition is different by a multiplier of\n",
      "        h**0.5 (h is an interval length) from the definition used here.\n",
      "        \n",
      "        .. versionadded:: 0.18.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Kierzenka, L. F. Shampine, \"A BVP Solver Based on Residual\n",
      "               Control and the Maltab PSE\", ACM Trans. Math. Softw., Vol. 27,\n",
      "               Number 3, pp. 299-316, 2001.\n",
      "        .. [2] L.F. Shampine, P. H. Muir and H. Xu, \"A User-Friendly Fortran BVP\n",
      "               Solver\".\n",
      "        .. [3] U. Ascher, R. Mattheij and R. Russell \"Numerical Solution of\n",
      "               Boundary Value Problems for Ordinary Differential Equations\".\n",
      "        .. [4] `Cauchy-Riemann equations\n",
      "                <https://en.wikipedia.org/wiki/Cauchy-Riemann_equations>`_ on\n",
      "                Wikipedia.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In the first example we solve Bratu's problem::\n",
      "        \n",
      "            y'' + k * exp(y) = 0\n",
      "            y(0) = y(1) = 0\n",
      "        \n",
      "        for k = 1.\n",
      "        \n",
      "        We rewrite the equation as a first order system and implement its\n",
      "        right-hand side evaluation::\n",
      "        \n",
      "            y1' = y2\n",
      "            y2' = -exp(y1)\n",
      "        \n",
      "        >>> def fun(x, y):\n",
      "        ...     return np.vstack((y[1], -np.exp(y[0])))\n",
      "        \n",
      "        Implement evaluation of the boundary condition residuals:\n",
      "        \n",
      "        >>> def bc(ya, yb):\n",
      "        ...     return np.array([ya[0], yb[0]])\n",
      "        \n",
      "        Define the initial mesh with 5 nodes:\n",
      "        \n",
      "        >>> x = np.linspace(0, 1, 5)\n",
      "        \n",
      "        This problem is known to have two solutions. To obtain both of them we\n",
      "        use two different initial guesses for y. We denote them by subscripts\n",
      "        a and b.\n",
      "        \n",
      "        >>> y_a = np.zeros((2, x.size))\n",
      "        >>> y_b = np.zeros((2, x.size))\n",
      "        >>> y_b[0] = 3\n",
      "        \n",
      "        Now we are ready to run the solver.\n",
      "        \n",
      "        >>> from scipy.integrate import solve_bvp\n",
      "        >>> res_a = solve_bvp(fun, bc, x, y_a)\n",
      "        >>> res_b = solve_bvp(fun, bc, x, y_b)\n",
      "        \n",
      "        Let's plot the two found solutions. We take an advantage of having the\n",
      "        solution in a spline form to produce a smooth plot.\n",
      "        \n",
      "        >>> x_plot = np.linspace(0, 1, 100)\n",
      "        >>> y_plot_a = res_a.sol(x_plot)[0]\n",
      "        >>> y_plot_b = res_b.sol(x_plot)[0]\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> plt.plot(x_plot, y_plot_a, label='y_a')\n",
      "        >>> plt.plot(x_plot, y_plot_b, label='y_b')\n",
      "        >>> plt.legend()\n",
      "        >>> plt.xlabel(\"x\")\n",
      "        >>> plt.ylabel(\"y\")\n",
      "        >>> plt.show()\n",
      "        \n",
      "        We see that the two solutions have similar shape, but differ in scale\n",
      "        significantly.\n",
      "        \n",
      "        In the second example we solve a simple Sturm-Liouville problem::\n",
      "        \n",
      "            y'' + k**2 * y = 0\n",
      "            y(0) = y(1) = 0\n",
      "        \n",
      "        It is known that a non-trivial solution y = A * sin(k * x) is possible for\n",
      "        k = pi * n, where n is an integer. To establish the normalization constant\n",
      "        A = 1 we add a boundary condition::\n",
      "        \n",
      "            y'(0) = k\n",
      "        \n",
      "        Again we rewrite our equation as a first order system and implement its\n",
      "        right-hand side evaluation::\n",
      "        \n",
      "            y1' = y2\n",
      "            y2' = -k**2 * y1\n",
      "        \n",
      "        >>> def fun(x, y, p):\n",
      "        ...     k = p[0]\n",
      "        ...     return np.vstack((y[1], -k**2 * y[0]))\n",
      "        \n",
      "        Note that parameters p are passed as a vector (with one element in our\n",
      "        case).\n",
      "        \n",
      "        Implement the boundary conditions:\n",
      "        \n",
      "        >>> def bc(ya, yb, p):\n",
      "        ...     k = p[0]\n",
      "        ...     return np.array([ya[0], yb[0], ya[1] - k])\n",
      "        \n",
      "        Setup the initial mesh and guess for y. We aim to find the solution for\n",
      "        k = 2 * pi, to achieve that we set values of y to approximately follow\n",
      "        sin(2 * pi * x):\n",
      "        \n",
      "        >>> x = np.linspace(0, 1, 5)\n",
      "        >>> y = np.zeros((2, x.size))\n",
      "        >>> y[0, 1] = 1\n",
      "        >>> y[0, 3] = -1\n",
      "        \n",
      "        Run the solver with 6 as an initial guess for k.\n",
      "        \n",
      "        >>> sol = solve_bvp(fun, bc, x, y, p=[6])\n",
      "        \n",
      "        We see that the found k is approximately correct:\n",
      "        \n",
      "        >>> sol.p[0]\n",
      "        6.28329460046\n",
      "        \n",
      "        And finally plot the solution to see the anticipated sinusoid:\n",
      "        \n",
      "        >>> x_plot = np.linspace(0, 1, 100)\n",
      "        >>> y_plot = sol.sol(x_plot)[0]\n",
      "        >>> plt.plot(x_plot, y_plot)\n",
      "        >>> plt.xlabel(\"x\")\n",
      "        >>> plt.ylabel(\"y\")\n",
      "        >>> plt.show()\n",
      "    \n",
      "    tplquad(func, a, b, gfun, hfun, qfun, rfun, args=(), epsabs=1.49e-08, epsrel=1.49e-08)\n",
      "        Compute a triple (definite) integral.\n",
      "        \n",
      "        Return the triple integral of ``func(z, y, x)`` from ``x = a..b``,\n",
      "        ``y = gfun(x)..hfun(x)``, and ``z = qfun(x,y)..rfun(x,y)``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            A Python function or method of at least three variables in the\n",
      "            order (z, y, x).\n",
      "        a, b : float\n",
      "            The limits of integration in x: `a` < `b`\n",
      "        gfun : function\n",
      "            The lower boundary curve in y which is a function taking a single\n",
      "            floating point argument (x) and returning a floating point result:\n",
      "            a lambda function can be useful here.\n",
      "        hfun : function\n",
      "            The upper boundary curve in y (same requirements as `gfun`).\n",
      "        qfun : function\n",
      "            The lower boundary surface in z.  It must be a function that takes\n",
      "            two floats in the order (x, y) and returns a float.\n",
      "        rfun : function\n",
      "            The upper boundary surface in z. (Same requirements as `qfun`.)\n",
      "        args : tuple, optional\n",
      "            Extra arguments to pass to `func`.\n",
      "        epsabs : float, optional\n",
      "            Absolute tolerance passed directly to the innermost 1-D quadrature\n",
      "            integration. Default is 1.49e-8.\n",
      "        epsrel : float, optional\n",
      "            Relative tolerance of the innermost 1-D integrals. Default is 1.49e-8.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y : float\n",
      "            The resultant integral.\n",
      "        abserr : float\n",
      "            An estimate of the error.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        quad: Adaptive quadrature using QUADPACK\n",
      "        quadrature: Adaptive Gaussian quadrature\n",
      "        fixed_quad: Fixed-order Gaussian quadrature\n",
      "        dblquad: Double integrals\n",
      "        nquad : N-dimensional integrals\n",
      "        romb: Integrators for sampled data\n",
      "        simps: Integrators for sampled data\n",
      "        ode: ODE integrators\n",
      "        odeint: ODE integrators\n",
      "        scipy.special: For coefficients and roots of orthogonal polynomials\n",
      "    \n",
      "    trapz(y, x=None, dx=1.0, axis=-1)\n",
      "        Integrate along the given axis using the composite trapezoidal rule.\n",
      "        \n",
      "        Integrate `y` (`x`) along given axis.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            Input array to integrate.\n",
      "        x : array_like, optional\n",
      "            The sample points corresponding to the `y` values. If `x` is None,\n",
      "            the sample points are assumed to be evenly spaced `dx` apart. The\n",
      "            default is None.\n",
      "        dx : scalar, optional\n",
      "            The spacing between sample points when `x` is None. The default is 1.\n",
      "        axis : int, optional\n",
      "            The axis along which to integrate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        trapz : float\n",
      "            Definite integral as approximated by trapezoidal rule.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sum, cumsum\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Image [2]_ illustrates trapezoidal rule -- y-axis locations of points\n",
      "        will be taken from `y` array, by default x-axis distances between\n",
      "        points will be 1.0, alternatively they can be provided with `x` array\n",
      "        or with `dx` scalar.  Return value will be equal to combined area under\n",
      "        the red lines.\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wikipedia page: http://en.wikipedia.org/wiki/Trapezoidal_rule\n",
      "        \n",
      "        .. [2] Illustration image:\n",
      "               http://en.wikipedia.org/wiki/File:Composite_trapezoidal_rule_illustration.png\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> np.trapz([1,2,3])\n",
      "        4.0\n",
      "        >>> np.trapz([1,2,3], x=[4,6,8])\n",
      "        8.0\n",
      "        >>> np.trapz([1,2,3], dx=2)\n",
      "        8.0\n",
      "        >>> a = np.arange(6).reshape(2, 3)\n",
      "        >>> a\n",
      "        array([[0, 1, 2],\n",
      "               [3, 4, 5]])\n",
      "        >>> np.trapz(a, axis=0)\n",
      "        array([ 1.5,  2.5,  3.5])\n",
      "        >>> np.trapz(a, axis=1)\n",
      "        array([ 2.,  8.])\n",
      "\n",
      "DATA\n",
      "    __all__ = ['IntegrationWarning', 'absolute_import', 'complex_ode', 'cu...\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "\n",
      "FILE\n",
      "    c:\\programdata\\anaconda3\\lib\\site-packages\\scipy\\integrate\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(integrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0000000000000002, 5.842606996763696e-11)\n"
     ]
    }
   ],
   "source": [
    "invexp = lambda x: np.exp(-x)\n",
    "print(integrate.quad(invexp, 0,np.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0, 2.220446049250313e-14)\n"
     ]
    }
   ],
   "source": [
    "##[sinx] from 0 to np.pi\n",
    "\n",
    "invsin = lambda x: np.sin(x)\n",
    "print(integrate.quad(invsin, 0, np.pi))\n",
    "\n",
    "#[-cos x] from 0 to pi\n",
    "#[-cos pi + cos 0]\n",
    "#[1+1]\n",
    "#[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nadErpZSf+/8T35abToeyigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x97d2c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--------------Finding the minimum of a scalar funcion----------\n",
    "def f(x):\n",
    "    return x**2 + 10*np.sin(x)\n",
    "\n",
    "x = np.arange(-10,10,0.1)\n",
    "plt.plot(x, f(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.83746709]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -7.945823\n",
      "         Iterations: 5\n",
      "         Function evaluations: 18\n",
      "         Gradient evaluations: 6\n",
      "[-1.30644012]\n"
     ]
    }
   ],
   "source": [
    "#--------------BFGS algorithms-------------\n",
    "print(optimize.fmin_bfgs(f, 3, disp=False))\n",
    "\n",
    "print(optimize.fmin_bfgs(f, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        fun: -7.945823375615284\n",
      " lowest_optimization_result:       fun: -7.945823375615284\n",
      " hess_inv: array([[ 0.08581188]])\n",
      "      jac: array([ 0.])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 15\n",
      "      nit: 3\n",
      "     njev: 5\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-1.30644001])\n",
      "                    message: ['requested number of basinhopping iterations completed successfully']\n",
      "      minimization_failures: 0\n",
      "                       nfev: 1524\n",
      "                        nit: 100\n",
      "                       njev: 508\n",
      "                          x: array([-1.30644001])\n"
     ]
    }
   ],
   "source": [
    "#-----------basinhopping((f,0))-----------\n",
    "print(optimize.basinhopping(f , 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.]\n"
     ]
    }
   ],
   "source": [
    "#---------------roots of a scalar function--------------\n",
    "root = optimize.fsolve(f,1)  #our initial guess is 1\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.47948183]\n"
     ]
    }
   ],
   "source": [
    "root2 = optimize.fsolve(f,-2.5)  \n",
    "print(root2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.57478115e-12  -2.47948183e+00]\n"
     ]
    }
   ],
   "source": [
    "root3 = optimize.fsolve(f,[1,-2.5])  \n",
    "print(root3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package scipy.optimize in scipy:\n",
      "\n",
      "NAME\n",
      "    scipy.optimize\n",
      "\n",
      "DESCRIPTION\n",
      "    =====================================================\n",
      "    Optimization and root finding (:mod:`scipy.optimize`)\n",
      "    =====================================================\n",
      "    \n",
      "    .. currentmodule:: scipy.optimize\n",
      "    \n",
      "    Optimization\n",
      "    ============\n",
      "    \n",
      "    Local Optimization\n",
      "    ------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       minimize - Unified interface for minimizers of multivariate functions\n",
      "       minimize_scalar - Unified interface for minimizers of univariate functions\n",
      "       OptimizeResult - The optimization result returned by some optimizers\n",
      "       OptimizeWarning - The optimization encountered problems\n",
      "    \n",
      "    The `minimize` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.minimize-neldermead\n",
      "       optimize.minimize-powell\n",
      "       optimize.minimize-cg\n",
      "       optimize.minimize-bfgs\n",
      "       optimize.minimize-newtoncg\n",
      "       optimize.minimize-lbfgsb\n",
      "       optimize.minimize-tnc\n",
      "       optimize.minimize-cobyla\n",
      "       optimize.minimize-slsqp\n",
      "       optimize.minimize-dogleg\n",
      "       optimize.minimize-trustncg\n",
      "    \n",
      "    The `minimize_scalar` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.minimize_scalar-brent\n",
      "       optimize.minimize_scalar-bounded\n",
      "       optimize.minimize_scalar-golden\n",
      "    \n",
      "    The specific optimization method interfaces below in this subsection are\n",
      "    not recommended for use in new scripts; all of these methods are accessible\n",
      "    via a newer, more consistent interface provided by the functions above.\n",
      "    \n",
      "    General-purpose multivariate methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fmin - Nelder-Mead Simplex algorithm\n",
      "       fmin_powell - Powell's (modified) level set method\n",
      "       fmin_cg - Non-linear (Polak-Ribiere) conjugate gradient algorithm\n",
      "       fmin_bfgs - Quasi-Newton method (Broydon-Fletcher-Goldfarb-Shanno)\n",
      "       fmin_ncg - Line-search Newton Conjugate Gradient\n",
      "    \n",
      "    Constrained multivariate methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fmin_l_bfgs_b - Zhu, Byrd, and Nocedal's constrained optimizer\n",
      "       fmin_tnc - Truncated Newton code\n",
      "       fmin_cobyla - Constrained optimization by linear approximation\n",
      "       fmin_slsqp - Minimization using sequential least-squares programming\n",
      "       differential_evolution - stochastic minimization using differential evolution\n",
      "    \n",
      "    Univariate (scalar) minimization methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fminbound - Bounded minimization of a scalar function\n",
      "       brent - 1-D function minimization using Brent method\n",
      "       golden - 1-D function minimization using Golden Section method\n",
      "    \n",
      "    Equation (Local) Minimizers\n",
      "    ---------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       leastsq - Minimize the sum of squares of M equations in N unknowns\n",
      "       least_squares - Feature-rich least-squares minimization.\n",
      "       nnls - Linear least-squares problem with non-negativity constraint\n",
      "       lsq_linear - Linear least-squares problem with bound constraints\n",
      "    \n",
      "    Global Optimization\n",
      "    -------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       basinhopping - Basinhopping stochastic optimizer\n",
      "       brute - Brute force searching optimizer\n",
      "       differential_evolution - stochastic minimization using differential evolution\n",
      "    \n",
      "    Rosenbrock function\n",
      "    -------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       rosen - The Rosenbrock function.\n",
      "       rosen_der - The derivative of the Rosenbrock function.\n",
      "       rosen_hess - The Hessian matrix of the Rosenbrock function.\n",
      "       rosen_hess_prod - Product of the Rosenbrock Hessian with a vector.\n",
      "    \n",
      "    Fitting\n",
      "    =======\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       curve_fit -- Fit curve to a set of points\n",
      "    \n",
      "    Root finding\n",
      "    ============\n",
      "    \n",
      "    Scalar functions\n",
      "    ----------------\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       brentq - quadratic interpolation Brent method\n",
      "       brenth - Brent method, modified by Harris with hyperbolic extrapolation\n",
      "       ridder - Ridder's method\n",
      "       bisect - Bisection method\n",
      "       newton - Secant method or Newton's method\n",
      "    \n",
      "    Fixed point finding:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fixed_point - Single-variable fixed-point solver\n",
      "    \n",
      "    Multidimensional\n",
      "    ----------------\n",
      "    \n",
      "    General nonlinear solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       root - Unified interface for nonlinear solvers of multivariate functions\n",
      "       fsolve - Non-linear multi-variable equation solver\n",
      "       broyden1 - Broyden's first method\n",
      "       broyden2 - Broyden's second method\n",
      "    \n",
      "    The `root` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.root-hybr\n",
      "       optimize.root-lm\n",
      "       optimize.root-broyden1\n",
      "       optimize.root-broyden2\n",
      "       optimize.root-anderson\n",
      "       optimize.root-linearmixing\n",
      "       optimize.root-diagbroyden\n",
      "       optimize.root-excitingmixing\n",
      "       optimize.root-krylov\n",
      "       optimize.root-dfsane\n",
      "    \n",
      "    Large-scale nonlinear solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       newton_krylov\n",
      "       anderson\n",
      "    \n",
      "    Simple iterations:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       excitingmixing\n",
      "       linearmixing\n",
      "       diagbroyden\n",
      "    \n",
      "    :mod:`Additional information on the nonlinear solvers <scipy.optimize.nonlin>`\n",
      "    \n",
      "    Linear Programming\n",
      "    ==================\n",
      "    \n",
      "    Simplex Algorithm:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linprog -- Linear programming using the simplex algorithm\n",
      "       linprog_verbose_callback -- Sample callback function for linprog\n",
      "    \n",
      "    The `linprog` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.linprog-simplex\n",
      "    \n",
      "    Assignment problems:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linear_sum_assignment -- Solves the linear-sum assignment problem\n",
      "    \n",
      "    \n",
      "    Utilities\n",
      "    =========\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       approx_fprime - Approximate the gradient of a scalar function\n",
      "       bracket - Bracket a minimum, given two starting points\n",
      "       check_grad - Check the supplied derivative using finite differences\n",
      "       line_search - Return a step that satisfies the strong Wolfe conditions\n",
      "    \n",
      "       show_options - Show specific options optimization solvers\n",
      "       LbfgsInvHessProduct - Linear operator for L-BFGS approximate inverse Hessian\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _basinhopping\n",
      "    _cobyla\n",
      "    _differentialevolution\n",
      "    _group_columns\n",
      "    _hungarian\n",
      "    _lbfgsb\n",
      "    _linprog\n",
      "    _lsq (package)\n",
      "    _minimize\n",
      "    _minpack\n",
      "    _nnls\n",
      "    _numdiff\n",
      "    _root\n",
      "    _slsqp\n",
      "    _spectral\n",
      "    _trustregion\n",
      "    _trustregion_dogleg\n",
      "    _trustregion_ncg\n",
      "    _tstutils\n",
      "    _zeros\n",
      "    cobyla\n",
      "    lbfgsb\n",
      "    linesearch\n",
      "    minpack\n",
      "    minpack2\n",
      "    moduleTNC\n",
      "    nnls\n",
      "    nonlin\n",
      "    optimize\n",
      "    setup\n",
      "    slsqp\n",
      "    tnc\n",
      "    zeros\n",
      "\n",
      "CLASSES\n",
      "    builtins.UserWarning(builtins.Warning)\n",
      "        scipy.optimize.optimize.OptimizeWarning\n",
      "    builtins.dict(builtins.object)\n",
      "        scipy.optimize.optimize.OptimizeResult\n",
      "    scipy.sparse.linalg.interface.LinearOperator(builtins.object)\n",
      "        scipy.optimize.lbfgsb.LbfgsInvHessProduct\n",
      "    \n",
      "    class LbfgsInvHessProduct(scipy.sparse.linalg.interface.LinearOperator)\n",
      "     |  Linear operator for the L-BFGS approximate inverse Hessian.\n",
      "     |  \n",
      "     |  This operator computes the product of a vector with the approximate inverse\n",
      "     |  of the Hessian of the objective function, using the L-BFGS limited\n",
      "     |  memory approximation to the inverse Hessian, accumulated during the\n",
      "     |  optimization.\n",
      "     |  \n",
      "     |  Objects of this class implement the ``scipy.sparse.linalg.LinearOperator``\n",
      "     |  interface.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  sk : array_like, shape=(n_corr, n)\n",
      "     |      Array of `n_corr` most recent updates to the solution vector.\n",
      "     |      (See [1]).\n",
      "     |  yk : array_like, shape=(n_corr, n)\n",
      "     |      Array of `n_corr` most recent updates to the gradient. (See [1]).\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge. \"Updating quasi-Newton matrices with limited\n",
      "     |     storage.\" Mathematics of computation 35.151 (1980): 773-782.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LbfgsInvHessProduct\n",
      "     |      scipy.sparse.linalg.interface.LinearOperator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sk, yk)\n",
      "     |      Construct the operator.\n",
      "     |  \n",
      "     |  todense(self)\n",
      "     |      Return a dense array representation of this operator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      arr : ndarray, shape=(n, n)\n",
      "     |          An array with the same shape and containing\n",
      "     |          the same data represented by this `LinearOperator`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __add__(self, x)\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __matmul__(self, other)\n",
      "     |  \n",
      "     |  __mul__(self, x)\n",
      "     |  \n",
      "     |  __neg__(self)\n",
      "     |  \n",
      "     |  __pow__(self, p)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __rmatmul__(self, other)\n",
      "     |  \n",
      "     |  __rmul__(self, x)\n",
      "     |  \n",
      "     |  __sub__(self, x)\n",
      "     |  \n",
      "     |  adjoint(self)\n",
      "     |      Hermitian adjoint.\n",
      "     |      \n",
      "     |      Returns the Hermitian adjoint of self, aka the Hermitian\n",
      "     |      conjugate or Hermitian transpose. For a complex matrix, the\n",
      "     |      Hermitian adjoint is equal to the conjugate transpose.\n",
      "     |      \n",
      "     |      Can be abbreviated self.H instead of self.adjoint().\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A_H : LinearOperator\n",
      "     |          Hermitian adjoint of self.\n",
      "     |  \n",
      "     |  dot(self, x)\n",
      "     |      Matrix-matrix or matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          1-d or 2-d array, representing a vector or matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Ax : array\n",
      "     |          1-d or 2-d array (depending on the shape of x) that represents\n",
      "     |          the result of applying this linear operator on x.\n",
      "     |  \n",
      "     |  matmat(self, X)\n",
      "     |      Matrix-matrix multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y=A*X where A is an MxN linear\n",
      "     |      operator and X dense N*K matrix or ndarray.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {matrix, ndarray}\n",
      "     |          An array with shape (N,K).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (M,K) depending on\n",
      "     |          the type of the X argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This matmat wraps any user-specified matmat routine or overridden\n",
      "     |      _matmat method to ensure that y has the correct type.\n",
      "     |  \n",
      "     |  matvec(self, x)\n",
      "     |      Matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y=A*x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : {matrix, ndarray}\n",
      "     |          An array with shape (N,) or (N,1).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (M,) or (M,1) depending\n",
      "     |          on the type and shape of the x argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This matvec wraps the user-specified matvec routine or overridden\n",
      "     |      _matvec method to ensure that y has the correct shape and type.\n",
      "     |  \n",
      "     |  rmatvec(self, x)\n",
      "     |      Adjoint matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y = A^H * x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : {matrix, ndarray}\n",
      "     |          An array with shape (M,) or (M,1).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (N,) or (N,1) depending\n",
      "     |          on the type and shape of the x argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This rmatvec wraps the user-specified rmatvec routine or overridden\n",
      "     |      _rmatvec method to ensure that y has the correct shape and type.\n",
      "     |  \n",
      "     |  transpose(self)\n",
      "     |      Transpose this linear operator.\n",
      "     |      \n",
      "     |      Returns a LinearOperator that represents the transpose of this one.\n",
      "     |      Can be abbreviated self.T instead of self.transpose().\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwargs)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  H\n",
      "     |      Hermitian adjoint.\n",
      "     |      \n",
      "     |      Returns the Hermitian adjoint of self, aka the Hermitian\n",
      "     |      conjugate or Hermitian transpose. For a complex matrix, the\n",
      "     |      Hermitian adjoint is equal to the conjugate transpose.\n",
      "     |      \n",
      "     |      Can be abbreviated self.H instead of self.adjoint().\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A_H : LinearOperator\n",
      "     |          Hermitian adjoint of self.\n",
      "     |  \n",
      "     |  T\n",
      "     |      Transpose this linear operator.\n",
      "     |      \n",
      "     |      Returns a LinearOperator that represents the transpose of this one.\n",
      "     |      Can be abbreviated self.T instead of self.transpose().\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class OptimizeResult(builtins.dict)\n",
      "     |  Represents the optimization result.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  x : ndarray\n",
      "     |      The solution of the optimization.\n",
      "     |  success : bool\n",
      "     |      Whether or not the optimizer exited successfully.\n",
      "     |  status : int\n",
      "     |      Termination status of the optimizer. Its value depends on the\n",
      "     |      underlying solver. Refer to `message` for details.\n",
      "     |  message : str\n",
      "     |      Description of the cause of the termination.\n",
      "     |  fun, jac, hess: ndarray\n",
      "     |      Values of objective function, its Jacobian and its Hessian (if\n",
      "     |      available). The Hessians may be approximations, see the documentation\n",
      "     |      of the function in question.\n",
      "     |  hess_inv : object\n",
      "     |      Inverse of the objective function's Hessian; may be an approximation.\n",
      "     |      Not available for all solvers. The type of this attribute may be\n",
      "     |      either np.ndarray or scipy.sparse.linalg.LinearOperator.\n",
      "     |  nfev, njev, nhev : int\n",
      "     |      Number of evaluations of the objective functions and of its\n",
      "     |      Jacobian and Hessian.\n",
      "     |  nit : int\n",
      "     |      Number of iterations performed by the optimizer.\n",
      "     |  maxcv : float\n",
      "     |      The maximum constraint violation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  There may be additional attributes not listed above depending of the\n",
      "     |  specific solver. Since this class is essentially a subclass of dict\n",
      "     |  with attribute accessors, one can see which attributes are available\n",
      "     |  using the `keys()` method.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizeResult\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __delattr__ = __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__ = __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if D has a key k, else False.\n",
      "     |  \n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(...)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      D.clear() -> None.  Remove all items from D.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      D.copy() -> a shallow copy of D\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None, /) from builtins.type\n",
      "     |      Returns a new dict with keys from iterable and values equal to value.\n",
      "     |  \n",
      "     |  get(...)\n",
      "     |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  pop(...)\n",
      "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      "     |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      "     |  \n",
      "     |  popitem(...)\n",
      "     |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      "     |      2-tuple; but raise KeyError if D is empty.\n",
      "     |  \n",
      "     |  setdefault(...)\n",
      "     |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      "     |  \n",
      "     |  update(...)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class OptimizeWarning(builtins.UserWarning)\n",
      "     |  Base class for warnings generated by user code.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizeWarning\n",
      "     |      builtins.UserWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      helper for pickle\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "\n",
      "FUNCTIONS\n",
      "    anderson(F, xin, iter=None, alpha=None, w0=0.01, M=5, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using (extended) Anderson mixing.\n",
      "        \n",
      "        The Jacobian is formed by for a 'best' solution in the space\n",
      "        spanned by last `M` vectors. As a result, only a MxM matrix\n",
      "        inversions and MxN multiplications are required. [Ey]_\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        x0 : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        M : float, optional\n",
      "            Number of previous vectors to retain. Defaults to 5.\n",
      "        w0 : float, optional\n",
      "            Regularization parameter for numerical stability.\n",
      "            Compared to unity, good values of the order of 0.01.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Ey] V. Eyert, J. Comp. Phys., 124, 271 (1996).\n",
      "    \n",
      "    approx_fprime(xk, f, epsilon, *args)\n",
      "        Finite-difference approximation of the gradient of a scalar function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        xk : array_like\n",
      "            The coordinate vector at which to determine the gradient of `f`.\n",
      "        f : callable\n",
      "            The function of which to determine the gradient (partial derivatives).\n",
      "            Should take `xk` as first argument, other arguments to `f` can be\n",
      "            supplied in ``*args``.  Should return a scalar, the value of the\n",
      "            function at `xk`.\n",
      "        epsilon : array_like\n",
      "            Increment to `xk` to use for determining the function gradient.\n",
      "            If a scalar, uses the same finite difference delta for all partial\n",
      "            derivatives.  If an array, should contain one value per element of\n",
      "            `xk`.\n",
      "        \\*args : args, optional\n",
      "            Any other arguments that are to be passed to `f`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        grad : ndarray\n",
      "            The partial derivatives of `f` to `xk`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        check_grad : Check correctness of gradient function against approx_fprime.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function gradient is determined by the forward finite difference\n",
      "        formula::\n",
      "        \n",
      "                     f(xk[i] + epsilon[i]) - f(xk[i])\n",
      "            f'[i] = ---------------------------------\n",
      "                                epsilon[i]\n",
      "        \n",
      "        The main use of `approx_fprime` is in scalar function optimizers like\n",
      "        `fmin_bfgs`, to determine numerically the Jacobian of a function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c0, c1):\n",
      "        ...     \"Coordinate vector `x` should be an array of size two.\"\n",
      "        ...     return c0 * x[0]**2 + c1*x[1]**2\n",
      "        \n",
      "        >>> x = np.ones(2)\n",
      "        >>> c0, c1 = (1, 200)\n",
      "        >>> eps = np.sqrt(np.finfo(float).eps)\n",
      "        >>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)\n",
      "        array([   2.        ,  400.00004198])\n",
      "    \n",
      "    basinhopping(func, x0, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None, seed=None)\n",
      "        Find the global minimum of a function using the basin-hopping algorithm\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``f(x, *args)``\n",
      "            Function to be optimized.  ``args`` can be passed as an optional item\n",
      "            in the dict ``minimizer_kwargs``\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        niter : integer, optional\n",
      "            The number of basin hopping iterations\n",
      "        T : float, optional\n",
      "            The \"temperature\" parameter for the accept or reject criterion.  Higher\n",
      "            \"temperatures\" mean that larger jumps in function value will be\n",
      "            accepted.  For best results ``T`` should be comparable to the\n",
      "            separation\n",
      "            (in function value) between local minima.\n",
      "        stepsize : float, optional\n",
      "            initial step size for use in the random displacement.\n",
      "        minimizer_kwargs : dict, optional\n",
      "            Extra keyword arguments to be passed to the minimizer\n",
      "            ``scipy.optimize.minimize()`` Some important options could be:\n",
      "        \n",
      "                method : str\n",
      "                    The minimization method (e.g. ``\"L-BFGS-B\"``)\n",
      "                args : tuple\n",
      "                    Extra arguments passed to the objective function (``func``) and\n",
      "                    its derivatives (Jacobian, Hessian).\n",
      "        \n",
      "        take_step : callable ``take_step(x)``, optional\n",
      "            Replace the default step taking routine with this routine.  The default\n",
      "            step taking routine is a random displacement of the coordinates, but\n",
      "            other step taking algorithms may be better for some systems.\n",
      "            ``take_step`` can optionally have the attribute ``take_step.stepsize``.\n",
      "            If this attribute exists, then ``basinhopping`` will adjust\n",
      "            ``take_step.stepsize`` in order to try to optimize the global minimum\n",
      "            search.\n",
      "        accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional\n",
      "            Define a test which will be used to judge whether or not to accept the\n",
      "            step.  This will be used in addition to the Metropolis test based on\n",
      "            \"temperature\" ``T``.  The acceptable return values are True,\n",
      "            False, or ``\"force accept\"``. If any of the tests return False\n",
      "            then the step is rejected. If the latter, then this will override any\n",
      "            other tests in order to accept the step. This can be used, for example,\n",
      "            to forcefully escape from a local minimum that ``basinhopping`` is\n",
      "            trapped in.\n",
      "        callback : callable, ``callback(x, f, accept)``, optional\n",
      "            A callback function which will be called for all minima found.  ``x``\n",
      "            and ``f`` are the coordinates and function value of the trial minimum,\n",
      "            and ``accept`` is whether or not that minimum was accepted.  This can be\n",
      "            used, for example, to save the lowest N minima found.  Also,\n",
      "            ``callback`` can be used to specify a user defined stop criterion by\n",
      "            optionally returning True to stop the ``basinhopping`` routine.\n",
      "        interval : integer, optional\n",
      "            interval for how often to update the ``stepsize``\n",
      "        disp : bool, optional\n",
      "            Set to True to print status messages\n",
      "        niter_success : integer, optional\n",
      "            Stop the run if the global minimum candidate remains the same for this\n",
      "            number of iterations.\n",
      "        seed : int or `np.random.RandomState`, optional\n",
      "            If `seed` is not specified the `np.RandomState` singleton is used.\n",
      "            If `seed` is an int, a new `np.random.RandomState` instance is used,\n",
      "            seeded with seed.\n",
      "            If `seed` is already a `np.random.RandomState instance`, then that\n",
      "            `np.random.RandomState` instance is used.\n",
      "            Specify `seed` for repeatable minimizations. The random numbers\n",
      "            generated with this seed only affect the default Metropolis\n",
      "            `accept_test` and the default `take_step`. If you supply your own\n",
      "            `take_step` and `accept_test`, and these functions use random\n",
      "            number generation, then those functions are responsible for the state\n",
      "            of their random number generator.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.  Important\n",
      "            attributes are: ``x`` the solution array, ``fun`` the value of the\n",
      "            function at the solution, and ``message`` which describes the cause of\n",
      "            the termination. The ``OptimzeResult`` object returned by the selected\n",
      "            minimizer at the lowest minimum is also contained within this object\n",
      "            and can be accessed through the ``lowest_optimization_result`` attribute.\n",
      "            See `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        minimize :\n",
      "            The local minimization function called once for each basinhopping step.\n",
      "            ``minimizer_kwargs`` is passed to this routine.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Basin-hopping is a stochastic algorithm which attempts to find the global\n",
      "        minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_\n",
      "        [4]_.  The algorithm in its current form was described by David Wales and\n",
      "        Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.\n",
      "        \n",
      "        The algorithm is iterative with each cycle composed of the following\n",
      "        features\n",
      "        \n",
      "        1) random perturbation of the coordinates\n",
      "        \n",
      "        2) local minimization\n",
      "        \n",
      "        3) accept or reject the new coordinates based on the minimized function\n",
      "           value\n",
      "        \n",
      "        The acceptance test used here is the Metropolis criterion of standard Monte\n",
      "        Carlo algorithms, although there are many other possibilities [3]_.\n",
      "        \n",
      "        This global minimization method has been shown to be extremely efficient\n",
      "        for a wide variety of problems in physics and chemistry.  It is\n",
      "        particularly useful when the function has many minima separated by large\n",
      "        barriers. See the Cambridge Cluster Database\n",
      "        http://www-wales.ch.cam.ac.uk/CCD.html for databases of molecular systems\n",
      "        that have been optimized primarily using basin-hopping.  This database\n",
      "        includes minimization problems exceeding 300 degrees of freedom.\n",
      "        \n",
      "        See the free software program GMIN (http://www-wales.ch.cam.ac.uk/GMIN) for\n",
      "        a Fortran implementation of basin-hopping.  This implementation has many\n",
      "        different variations of the procedure described above, including more\n",
      "        advanced step taking algorithms and alternate acceptance criterion.\n",
      "        \n",
      "        For stochastic global optimization there is no way to determine if the true\n",
      "        global minimum has actually been found. Instead, as a consistency check,\n",
      "        the algorithm can be run from a number of different random starting points\n",
      "        to ensure the lowest minimum found in each example has converged to the\n",
      "        global minimum.  For this reason ``basinhopping`` will by default simply\n",
      "        run for the number of iterations ``niter`` and return the lowest minimum\n",
      "        found.  It is left to the user to ensure that this is in fact the global\n",
      "        minimum.\n",
      "        \n",
      "        Choosing ``stepsize``:  This is a crucial parameter in ``basinhopping`` and\n",
      "        depends on the problem being solved.  Ideally it should be comparable to\n",
      "        the typical separation between local minima of the function being\n",
      "        optimized.  ``basinhopping`` will, by default, adjust ``stepsize`` to find\n",
      "        an optimal value, but this may take many iterations.  You will get quicker\n",
      "        results if you set a sensible value for ``stepsize``.\n",
      "        \n",
      "        Choosing ``T``: The parameter ``T`` is the temperature used in the\n",
      "        metropolis criterion.  Basinhopping steps are accepted with probability\n",
      "        ``1`` if ``func(xnew) < func(xold)``, or otherwise with probability::\n",
      "        \n",
      "            exp( -(func(xnew) - func(xold)) / T )\n",
      "        \n",
      "        So, for best results, ``T`` should to be comparable to the typical\n",
      "        difference in function values between local minima.\n",
      "        \n",
      "        .. versionadded:: 0.12.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,\n",
      "            Cambridge, UK.\n",
      "        .. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and\n",
      "            the Lowest Energy Structures of Lennard-Jones Clusters Containing up to\n",
      "            110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.\n",
      "        .. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the\n",
      "            multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,\n",
      "            1987, 84, 6611.\n",
      "        .. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,\n",
      "            crystals, and biomolecules, Science, 1999, 285, 1368.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following example is a one-dimensional minimization problem,  with many\n",
      "        local minima superimposed on a parabola.\n",
      "        \n",
      "        >>> from scipy.optimize import basinhopping\n",
      "        >>> func = lambda x: np.cos(14.5 * x - 0.3) + (x + 0.2) * x\n",
      "        >>> x0=[1.]\n",
      "        \n",
      "        Basinhopping, internally, uses a local minimization algorithm.  We will use\n",
      "        the parameter ``minimizer_kwargs`` to tell basinhopping which algorithm to\n",
      "        use and how to set up that minimizer.  This parameter will be passed to\n",
      "        ``scipy.optimize.minimize()``.\n",
      "        \n",
      "        >>> minimizer_kwargs = {\"method\": \"BFGS\"}\n",
      "        >>> ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200)\n",
      "        >>> print(\"global minimum: x = %.4f, f(x0) = %.4f\" % (ret.x, ret.fun))\n",
      "        global minimum: x = -0.1951, f(x0) = -1.0009\n",
      "        \n",
      "        Next consider a two-dimensional minimization problem. Also, this time we\n",
      "        will use gradient information to significantly speed up the search.\n",
      "        \n",
      "        >>> def func2d(x):\n",
      "        ...     f = np.cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +\n",
      "        ...                                                            0.2) * x[0]\n",
      "        ...     df = np.zeros(2)\n",
      "        ...     df[0] = -14.5 * np.sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2\n",
      "        ...     df[1] = 2. * x[1] + 0.2\n",
      "        ...     return f, df\n",
      "        \n",
      "        We'll also use a different local minimization algorithm.  Also we must tell\n",
      "        the minimizer that our function returns both energy and gradient (jacobian)\n",
      "        \n",
      "        >>> minimizer_kwargs = {\"method\":\"L-BFGS-B\", \"jac\":True}\n",
      "        >>> x0 = [1.0, 1.0]\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200)\n",
      "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
      "        ...                                                           ret.x[1],\n",
      "        ...                                                           ret.fun))\n",
      "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
      "        \n",
      "        \n",
      "        Here is an example using a custom step taking routine.  Imagine you want\n",
      "        the first coordinate to take larger steps then the rest of the coordinates.\n",
      "        This can be implemented like so:\n",
      "        \n",
      "        >>> class MyTakeStep(object):\n",
      "        ...    def __init__(self, stepsize=0.5):\n",
      "        ...        self.stepsize = stepsize\n",
      "        ...    def __call__(self, x):\n",
      "        ...        s = self.stepsize\n",
      "        ...        x[0] += np.random.uniform(-2.*s, 2.*s)\n",
      "        ...        x[1:] += np.random.uniform(-s, s, x[1:].shape)\n",
      "        ...        return x\n",
      "        \n",
      "        Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude\n",
      "        of ``stepsize`` to optimize the search.  We'll use the same 2-D function as\n",
      "        before\n",
      "        \n",
      "        >>> mytakestep = MyTakeStep()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200, take_step=mytakestep)\n",
      "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
      "        ...                                                           ret.x[1],\n",
      "        ...                                                           ret.fun))\n",
      "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
      "        \n",
      "        \n",
      "        Now let's do an example using a custom callback function which prints the\n",
      "        value of every minimum found\n",
      "        \n",
      "        >>> def print_fun(x, f, accepted):\n",
      "        ...         print(\"at minimum %.4f accepted %d\" % (f, int(accepted)))\n",
      "        \n",
      "        We'll run it for only 10 basinhopping steps this time.\n",
      "        \n",
      "        >>> np.random.seed(1)\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=10, callback=print_fun)\n",
      "        at minimum 0.4159 accepted 1\n",
      "        at minimum -0.9073 accepted 1\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum 0.9102 accepted 1\n",
      "        at minimum 0.9102 accepted 1\n",
      "        at minimum 2.2945 accepted 0\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum -1.0109 accepted 1\n",
      "        at minimum -1.0109 accepted 1\n",
      "        \n",
      "        \n",
      "        The minimum at -1.0109 is actually the global minimum, found already on the\n",
      "        8th iteration.\n",
      "        \n",
      "        Now let's implement bounds on the problem using a custom ``accept_test``:\n",
      "        \n",
      "        >>> class MyBounds(object):\n",
      "        ...     def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1] ):\n",
      "        ...         self.xmax = np.array(xmax)\n",
      "        ...         self.xmin = np.array(xmin)\n",
      "        ...     def __call__(self, **kwargs):\n",
      "        ...         x = kwargs[\"x_new\"]\n",
      "        ...         tmax = bool(np.all(x <= self.xmax))\n",
      "        ...         tmin = bool(np.all(x >= self.xmin))\n",
      "        ...         return tmax and tmin\n",
      "        \n",
      "        >>> mybounds = MyBounds()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=10, accept_test=mybounds)\n",
      "    \n",
      "    bisect(f, a, b, args=(), xtol=2e-12, rtol=8.8817841970012523e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find root of a function within an interval.\n",
      "        \n",
      "        Basic bisection routine to find a zero of the function `f` between the\n",
      "        arguments `a` and `b`. `f(a)` and `f(b)` cannot have the same signs.\n",
      "        Slow but sure.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number.  `f` must be continuous, and\n",
      "            f(a) and f(b) must have opposite signs.\n",
      "        a : number\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : number\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``.\n",
      "        maxiter : number, optional\n",
      "            if convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised.  Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned.  If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where x is the root, and r is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : RootResults (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.  In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, bisect, newton\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        fsolve : n-dimensional root-finding\n",
      "    \n",
      "    bracket(func, xa=0.0, xb=1.0, args=(), grow_limit=110.0, maxiter=1000)\n",
      "        Bracket the minimum of the function.\n",
      "        \n",
      "        Given a function and distinct initial points, search in the\n",
      "        downhill direction (as defined by the initital points) and return\n",
      "        new points xa, xb, xc that bracket the minimum of the function\n",
      "        f(xa) > f(xb) < f(xc). It doesn't always mean that obtained\n",
      "        solution will satisfy xa<=x<=xb\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to minimize.\n",
      "        xa, xb : float, optional\n",
      "            Bracketing interval. Defaults `xa` to 0.0, and `xb` to 1.0.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to `func`.\n",
      "        grow_limit : float, optional\n",
      "            Maximum grow limit.  Defaults to 110.0\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Defaults to 1000.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xa, xb, xc : float\n",
      "            Bracket.\n",
      "        fa, fb, fc : float\n",
      "            Objective function values in bracket.\n",
      "        funcalls : int\n",
      "            Number of function evaluations made.\n",
      "    \n",
      "    brent(func, args=(), brack=None, tol=1.48e-08, full_output=0, maxiter=500)\n",
      "        Given a function of one-variable and a possible bracketing interval,\n",
      "        return the minimum of the function isolated to a fractional precision of\n",
      "        tol.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present).\n",
      "        brack : tuple, optional\n",
      "            Either a triple (xa,xb,xc) where xa<xb<xc and func(xb) <\n",
      "            func(xa), func(xc) or a pair (xa,xb) which are used as a\n",
      "            starting interval for a downhill bracket search (see\n",
      "            `bracket`). Providing the pair (xa,xb) does not always mean\n",
      "            the obtained solution will satisfy xa<=x<=xb.\n",
      "        tol : float, optional\n",
      "            Stop if between iteration change is less than `tol`.\n",
      "        full_output : bool, optional\n",
      "            If True, return all output args (xmin, fval, iter,\n",
      "            funcalls).\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations in solution.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xmin : ndarray\n",
      "            Optimum point.\n",
      "        fval : float\n",
      "            Optimum value.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of objective function evaluations made.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Brent' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses inverse parabolic interpolation when possible to speed up\n",
      "        convergence of golden section method.\n",
      "    \n",
      "    brenth(f, a, b, args=(), xtol=2e-12, rtol=8.8817841970012523e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find root of f in [a,b].\n",
      "        \n",
      "        A variation on the classic Brent routine to find a zero of the function f\n",
      "        between the arguments a and b that uses hyperbolic extrapolation instead of\n",
      "        inverse quadratic extrapolation. There was a paper back in the 1980's ...\n",
      "        f(a) and f(b) cannot have the same signs. Generally on a par with the\n",
      "        brent routine, but not as heavily tested.  It is a safe version of the\n",
      "        secant method that uses hyperbolic extrapolation. The version here is by\n",
      "        Chuck Harris.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number.  f must be continuous, and f(a) and\n",
      "            f(b) must have opposite signs.\n",
      "        a : number\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : number\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative. As with `brentq`, for nice\n",
      "            functions the method will often satisfy the above condition\n",
      "            will ``xtol/2`` and ``rtol/2``.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``. As with `brentq`, for nice functions\n",
      "            the method will often satisfy the above condition will\n",
      "            ``xtol/2`` and ``rtol/2``.\n",
      "        maxiter : number, optional\n",
      "            if convergence is not achieved in maxiter iterations, an error is\n",
      "            raised.  Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned.  If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a RootResults object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : RootResults (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.  In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fmin, fmin_powell, fmin_cg,\n",
      "               fmin_bfgs, fmin_ncg : multivariate local optimizers\n",
      "        \n",
      "        leastsq : nonlinear least squares minimizer\n",
      "        \n",
      "        fmin_l_bfgs_b, fmin_tnc, fmin_cobyla : constrained multivariate optimizers\n",
      "        \n",
      "        basinhopping, differential_evolution, brute : global optimizers\n",
      "        \n",
      "        fminbound, brent, golden, bracket : local scalar minimizers\n",
      "        \n",
      "        fsolve : n-dimensional root-finding\n",
      "        \n",
      "        brentq, brenth, ridder, bisect, newton : one-dimensional root-finding\n",
      "        \n",
      "        fixed_point : scalar fixed-point finder\n",
      "    \n",
      "    brentq(f, a, b, args=(), xtol=2e-12, rtol=8.8817841970012523e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in a bracketing interval using Brent's method.\n",
      "        \n",
      "        Uses the classic Brent's method to find a zero of the function `f` on\n",
      "        the sign changing interval [a , b].  Generally considered the best of the\n",
      "        rootfinding routines here.  It is a safe version of the secant method that\n",
      "        uses inverse quadratic extrapolation.  Brent's method combines root\n",
      "        bracketing, interval bisection, and inverse quadratic interpolation.  It is\n",
      "        sometimes known as the van Wijngaarden-Dekker-Brent method.  Brent (1973)\n",
      "        claims convergence is guaranteed for functions computable within [a,b].\n",
      "        \n",
      "        [Brent1973]_ provides the classic description of the algorithm.  Another\n",
      "        description can be found in a recent edition of Numerical Recipes, including\n",
      "        [PressEtal1992]_.  Another description is at\n",
      "        http://mathworld.wolfram.com/BrentsMethod.html.  It should be easy to\n",
      "        understand the algorithm just by reading our code.  Our code diverges a bit\n",
      "        from standard presentations: we choose a different formula for the\n",
      "        extrapolation step.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number.  The function :math:`f`\n",
      "            must be continuous, and :math:`f(a)` and :math:`f(b)` must\n",
      "            have opposite signs.\n",
      "        a : number\n",
      "            One end of the bracketing interval :math:`[a, b]`.\n",
      "        b : number\n",
      "            The other end of the bracketing interval :math:`[a, b]`.\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative. For nice functions, Brent's\n",
      "            method will often satisfy the above condition will ``xtol/2``\n",
      "            and ``rtol/2``. [Brent1973]_\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``. For nice functions, Brent's\n",
      "            method will often satisfy the above condition will ``xtol/2``\n",
      "            and ``rtol/2``. [Brent1973]_\n",
      "        maxiter : number, optional\n",
      "            if convergence is not achieved in maxiter iterations, an error is\n",
      "            raised.  Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned.  If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a RootResults object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : RootResults (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.  In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        multivariate local optimizers\n",
      "          `fmin`, `fmin_powell`, `fmin_cg`, `fmin_bfgs`, `fmin_ncg`\n",
      "        nonlinear least squares minimizer\n",
      "          `leastsq`\n",
      "        constrained multivariate optimizers\n",
      "          `fmin_l_bfgs_b`, `fmin_tnc`, `fmin_cobyla`\n",
      "        global optimizers\n",
      "          `basinhopping`, `brute`, `differential_evolution`\n",
      "        local scalar minimizers\n",
      "          `fminbound`, `brent`, `golden`, `bracket`\n",
      "        n-dimensional root-finding\n",
      "          `fsolve`\n",
      "        one-dimensional root-finding\n",
      "          `brenth`, `ridder`, `bisect`, `newton`\n",
      "        scalar fixed-point finder\n",
      "          `fixed_point`\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `f` must be continuous.  f(a) and f(b) must have opposite signs.\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Brent1973]\n",
      "           Brent, R. P.,\n",
      "           *Algorithms for Minimization Without Derivatives*.\n",
      "           Englewood Cliffs, NJ: Prentice-Hall, 1973. Ch. 3-4.\n",
      "        \n",
      "        .. [PressEtal1992]\n",
      "           Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T.\n",
      "           *Numerical Recipes in FORTRAN: The Art of Scientific Computing*, 2nd ed.\n",
      "           Cambridge, England: Cambridge University Press, pp. 352-355, 1992.\n",
      "           Section 9.3:  \"Van Wijngaarden-Dekker-Brent Method.\"\n",
      "    \n",
      "    broyden1(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Broyden's first Jacobian approximation.\n",
      "        \n",
      "        This method is also known as \\\"Broyden's good method\\\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        x0 : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden matrix\n",
      "            stays low. Can either be a string giving the name of the method,\n",
      "            or a tuple of the form ``(method, param1, param2, ...)``\n",
      "            that gives the name of the method and values for additional parameters.\n",
      "        \n",
      "            Methods available:\n",
      "        \n",
      "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
      "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
      "                - ``svd``: keep only the most significant SVD components.\n",
      "                  Takes an extra parameter, ``to_retain``, which determines the\n",
      "                  number of SVD components to retain when rank reduction is done.\n",
      "                  Default is ``max_rank - 2``.\n",
      "        \n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (ie., no rank reduction).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "        \n",
      "        .. math:: H_+ = H + (dx - H df) dx^\\dagger H / ( dx^\\dagger H df)\n",
      "        \n",
      "        which corresponds to Broyden's first Jacobian update\n",
      "        \n",
      "        .. math:: J_+ = J + (df - J dx) dx^\\dagger / dx^\\dagger dx\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B.A. van der Rotten, PhD thesis,\n",
      "           \\\"A limited memory Broyden method to solve high-dimensional\n",
      "           systems of nonlinear equations\\\". Mathematisch Instituut,\n",
      "           Universiteit Leiden, The Netherlands (2003).\n",
      "        \n",
      "           http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "    \n",
      "    broyden2(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Broyden's second Jacobian approximation.\n",
      "        \n",
      "        This method is also known as \"Broyden's bad method\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        x0 : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden matrix\n",
      "            stays low. Can either be a string giving the name of the method,\n",
      "            or a tuple of the form ``(method, param1, param2, ...)``\n",
      "            that gives the name of the method and values for additional parameters.\n",
      "        \n",
      "            Methods available:\n",
      "        \n",
      "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
      "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
      "                - ``svd``: keep only the most significant SVD components.\n",
      "                  Takes an extra parameter, ``to_retain``, which determines the\n",
      "                  number of SVD components to retain when rank reduction is done.\n",
      "                  Default is ``max_rank - 2``.\n",
      "        \n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (ie., no rank reduction).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "        \n",
      "        .. math:: H_+ = H + (dx - H df) df^\\dagger / ( df^\\dagger df)\n",
      "        \n",
      "        corresponding to Broyden's second method.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B.A. van der Rotten, PhD thesis,\n",
      "           \"A limited memory Broyden method to solve high-dimensional\n",
      "           systems of nonlinear equations\". Mathematisch Instituut,\n",
      "           Universiteit Leiden, The Netherlands (2003).\n",
      "        \n",
      "           http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "    \n",
      "    brute(func, ranges, args=(), Ns=20, full_output=0, finish=<function fmin at 0x0000000008511EA0>, disp=False)\n",
      "        Minimize a function over a given range by brute force.\n",
      "        \n",
      "        Uses the \"brute force\" method, i.e. computes the function's value\n",
      "        at each point of a multidimensional grid of points, to find the global\n",
      "        minimum of the function.\n",
      "        \n",
      "        The function is evaluated everywhere in the range with the datatype of the\n",
      "        first call to the function, as enforced by the ``vectorize`` NumPy\n",
      "        function.  The value and type of the function evaluation returned when\n",
      "        ``full_output=True`` are affected in addition by the ``finish`` argument\n",
      "        (see Notes).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the\n",
      "            form ``f(x, *args)``, where ``x`` is the argument in\n",
      "            the form of a 1-D array and ``args`` is a tuple of any\n",
      "            additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        ranges : tuple\n",
      "            Each component of the `ranges` tuple must be either a\n",
      "            \"slice object\" or a range tuple of the form ``(low, high)``.\n",
      "            The program uses these to create the grid of points on which\n",
      "            the objective function will be computed. See `Note 2` for\n",
      "            more detail.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        Ns : int, optional\n",
      "            Number of grid points along the axes, if not otherwise\n",
      "            specified. See `Note2`.\n",
      "        full_output : bool, optional\n",
      "            If True, return the evaluation grid and the objective function's\n",
      "            values on it.\n",
      "        finish : callable, optional\n",
      "            An optimization function that is called with the result of brute force\n",
      "            minimization as initial guess.  `finish` should take `func` and\n",
      "            the initial guess as positional arguments, and take `args` as\n",
      "            keyword arguments.  It may additionally take `full_output`\n",
      "            and/or `disp` as keyword arguments.  Use None if no \"polishing\"\n",
      "            function is to be used. See Notes for more details.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : ndarray\n",
      "            A 1-D array containing the coordinates of a point at which the\n",
      "            objective function had its minimum value. (See `Note 1` for\n",
      "            which point is returned.)\n",
      "        fval : float\n",
      "            Function value at the point `x0`. (Returned when `full_output` is\n",
      "            True.)\n",
      "        grid : tuple\n",
      "            Representation of the evaluation grid.  It has the same\n",
      "            length as `x0`. (Returned when `full_output` is True.)\n",
      "        Jout : ndarray\n",
      "            Function values at each point of the evaluation\n",
      "            grid, `i.e.`, ``Jout = func(*grid)``. (Returned\n",
      "            when `full_output` is True.)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        basinhopping, differential_evolution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        *Note 1*: The program finds the gridpoint at which the lowest value\n",
      "        of the objective function occurs.  If `finish` is None, that is the\n",
      "        point returned.  When the global minimum occurs within (or not very far\n",
      "        outside) the grid's boundaries, and the grid is fine enough, that\n",
      "        point will be in the neighborhood of the global minimum.\n",
      "        \n",
      "        However, users often employ some other optimization program to\n",
      "        \"polish\" the gridpoint values, `i.e.`, to seek a more precise\n",
      "        (local) minimum near `brute's` best gridpoint.\n",
      "        The `brute` function's `finish` option provides a convenient way to do\n",
      "        that.  Any polishing program used must take `brute's` output as its\n",
      "        initial guess as a positional argument, and take `brute's` input values\n",
      "        for `args` as keyword arguments, otherwise an error will be raised.\n",
      "        It may additionally take `full_output` and/or `disp` as keyword arguments.\n",
      "        \n",
      "        `brute` assumes that the `finish` function returns either an\n",
      "        `OptimizeResult` object or a tuple in the form:\n",
      "        ``(xmin, Jmin, ... , statuscode)``, where ``xmin`` is the minimizing\n",
      "        value of the argument, ``Jmin`` is the minimum value of the objective\n",
      "        function, \"...\" may be some other returned values (which are not used\n",
      "        by `brute`), and ``statuscode`` is the status code of the `finish` program.\n",
      "        \n",
      "        Note that when `finish` is not None, the values returned are those\n",
      "        of the `finish` program, *not* the gridpoint ones.  Consequently,\n",
      "        while `brute` confines its search to the input grid points,\n",
      "        the `finish` program's results usually will not coincide with any\n",
      "        gridpoint, and may fall outside the grid's boundary. Thus, if a\n",
      "        minimum only needs to be found over the provided grid points, make\n",
      "        sure to pass in `finish=None`.\n",
      "        \n",
      "        *Note 2*: The grid of points is a `numpy.mgrid` object.\n",
      "        For `brute` the `ranges` and `Ns` inputs have the following effect.\n",
      "        Each component of the `ranges` tuple can be either a slice object or a\n",
      "        two-tuple giving a range of values, such as (0, 5).  If the component is a\n",
      "        slice object, `brute` uses it directly.  If the component is a two-tuple\n",
      "        range, `brute` internally converts it to a slice object that interpolates\n",
      "        `Ns` points from its low-value to its high-value, inclusive.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the use of `brute` to seek the global minimum of a function\n",
      "        of two variables that is given as the sum of a positive-definite\n",
      "        quadratic and two deep \"Gaussian-shaped\" craters.  Specifically, define\n",
      "        the objective function `f` as the sum of three other functions,\n",
      "        ``f = f1 + f2 + f3``.  We suppose each of these has a signature\n",
      "        ``(z, *params)``, where ``z = (x, y)``,  and ``params`` and the functions\n",
      "        are as defined below.\n",
      "        \n",
      "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
      "        >>> def f1(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
      "        \n",
      "        >>> def f2(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
      "        \n",
      "        >>> def f3(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
      "        \n",
      "        >>> def f(z, *params):\n",
      "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
      "        \n",
      "        Thus, the objective function may have local minima near the minimum\n",
      "        of each of the three functions of which it is composed.  To\n",
      "        use `fmin` to polish its gridpoint result, we may then continue as\n",
      "        follows:\n",
      "        \n",
      "        >>> rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))\n",
      "        >>> from scipy import optimize\n",
      "        >>> resbrute = optimize.brute(f, rranges, args=params, full_output=True,\n",
      "        ...                           finish=optimize.fmin)\n",
      "        >>> resbrute[0]  # global minimum\n",
      "        array([-1.05665192,  1.80834843])\n",
      "        >>> resbrute[1]  # function value at global minimum\n",
      "        -3.4085818767\n",
      "        \n",
      "        Note that if `finish` had been set to None, we would have gotten the\n",
      "        gridpoint [-1.0 1.75] where the rounded function value is -2.892.\n",
      "    \n",
      "    check_grad(func, grad, x0, *args, **kwargs)\n",
      "        Check the correctness of a gradient function by comparing it against a\n",
      "        (forward) finite-difference approximation of the gradient.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x0, *args)``\n",
      "            Function whose derivative is to be checked.\n",
      "        grad : callable ``grad(x0, *args)``\n",
      "            Gradient of `func`.\n",
      "        x0 : ndarray\n",
      "            Points to check `grad` against forward difference approximation of grad\n",
      "            using `func`.\n",
      "        args : \\*args, optional\n",
      "            Extra arguments passed to `func` and `grad`.\n",
      "        epsilon : float, optional\n",
      "            Step size used for the finite difference approximation. It defaults to\n",
      "            ``sqrt(numpy.finfo(float).eps)``, which is approximately 1.49e-08.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        err : float\n",
      "            The square root of the sum of squares (i.e. the 2-norm) of the\n",
      "            difference between ``grad(x0, *args)`` and the finite difference\n",
      "            approximation of `grad` using func at the points `x0`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        approx_fprime\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def func(x):\n",
      "        ...     return x[0]**2 - 0.5 * x[1]**3\n",
      "        >>> def grad(x):\n",
      "        ...     return [2 * x[0], -1.5 * x[1]**2]\n",
      "        >>> from scipy.optimize import check_grad\n",
      "        >>> check_grad(func, grad, [1.5, -1.5])\n",
      "        2.9802322387695312e-08\n",
      "    \n",
      "    curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False, check_finite=True, bounds=(-inf, inf), method=None, jac=None, **kwargs)\n",
      "        Use non-linear least squares to fit a function, f, to data.\n",
      "        \n",
      "        Assumes ``ydata = f(xdata, *params) + eps``\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable\n",
      "            The model function, f(x, ...).  It must take the independent\n",
      "            variable as the first argument and the parameters to fit as\n",
      "            separate remaining arguments.\n",
      "        xdata : An M-length sequence or an (k,M)-shaped array for functions with k predictors\n",
      "            The independent variable where the data is measured.\n",
      "        ydata : M-length sequence\n",
      "            The dependent data --- nominally f(xdata, ...)\n",
      "        p0 : None, scalar, or N-length sequence, optional\n",
      "            Initial guess for the parameters.  If None, then the initial\n",
      "            values will all be 1 (if the number of parameters for the function\n",
      "            can be determined using introspection, otherwise a ValueError\n",
      "            is raised).\n",
      "        sigma : None or M-length sequence or MxM array, optional\n",
      "            Determines the uncertainty in `ydata`. If we define residuals as\n",
      "            ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\n",
      "            depends on its number of dimensions:\n",
      "        \n",
      "                - A 1-d `sigma` should contain values of standard deviations of\n",
      "                  errors in `ydata`. In this case, the optimized function is\n",
      "                  ``chisq = sum((r / sigma) ** 2)``.\n",
      "        \n",
      "                - A 2-d `sigma` should contain the covariance matrix of\n",
      "                  errors in `ydata`. In this case, the optimized function is\n",
      "                  ``chisq = r.T @ inv(sigma) @ r``.\n",
      "        \n",
      "                  .. versionadded:: 0.19\n",
      "        \n",
      "            None (default) is equivalent of 1-d `sigma` filled with ones.\n",
      "        absolute_sigma : bool, optional\n",
      "            If True, `sigma` is used in an absolute sense and the estimated parameter\n",
      "            covariance `pcov` reflects these absolute values.\n",
      "        \n",
      "            If False, only the relative magnitudes of the `sigma` values matter.\n",
      "            The returned parameter covariance matrix `pcov` is based on scaling\n",
      "            `sigma` by a constant factor. This constant is set by demanding that the\n",
      "            reduced `chisq` for the optimal parameters `popt` when using the\n",
      "            *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\n",
      "            match the sample variance of the residuals after the fit.\n",
      "            Mathematically,\n",
      "            ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\n",
      "        check_finite : bool, optional\n",
      "            If True, check that the input arrays do not contain nans of infs,\n",
      "            and raise a ValueError if they do. Setting this parameter to\n",
      "            False may silently produce nonsensical results if the input arrays\n",
      "            do contain nans. Default is True.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on independent variables. Defaults to no bounds.\n",
      "            Each element of the tuple must be either an array with the length equal\n",
      "            to the number of parameters, or a scalar (in which case the bound is\n",
      "            taken to be the same for all parameters.) Use ``np.inf`` with an\n",
      "            appropriate sign to disable bounds on all or some parameters.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        method : {'lm', 'trf', 'dogbox'}, optional\n",
      "            Method to use for optimization.  See `least_squares` for more details.\n",
      "            Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\n",
      "            provided. The method 'lm' won't work when the number of observations\n",
      "            is less than the number of variables, use 'trf' or 'dogbox' in this\n",
      "            case.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        jac : callable, string or None, optional\n",
      "            Function with signature ``jac(x, ...)`` which computes the Jacobian\n",
      "            matrix of the model function with respect to parameters as a dense\n",
      "            array_like structure. It will be scaled according to provided `sigma`.\n",
      "            If None (default), the Jacobian will be estimated numerically.\n",
      "            String keywords for 'trf' and 'dogbox' methods can be used to select\n",
      "            a finite difference scheme, see `least_squares`.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        kwargs\n",
      "            Keyword arguments passed to `leastsq` for ``method='lm'`` or\n",
      "            `least_squares` otherwise.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        popt : array\n",
      "            Optimal values for the parameters so that the sum of the squared\n",
      "            residuals of ``f(xdata, *popt) - ydata`` is minimized\n",
      "        pcov : 2d array\n",
      "            The estimated covariance of popt. The diagonals provide the variance\n",
      "            of the parameter estimate. To compute one standard deviation errors\n",
      "            on the parameters use ``perr = np.sqrt(np.diag(pcov))``.\n",
      "        \n",
      "            How the `sigma` parameter affects the estimated covariance\n",
      "            depends on `absolute_sigma` argument, as described above.\n",
      "        \n",
      "            If the Jacobian matrix at the solution doesn't have a full rank, then\n",
      "            'lm' method returns a matrix filled with ``np.inf``, on the other hand\n",
      "            'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\n",
      "            the covariance matrix.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            if either `ydata` or `xdata` contain NaNs, or if incompatible options\n",
      "            are used.\n",
      "        \n",
      "        RuntimeError\n",
      "            if the least-squares minimization fails.\n",
      "        \n",
      "        OptimizeWarning\n",
      "            if covariance of the parameters can not be estimated.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        least_squares : Minimize the sum of squares of nonlinear functions.\n",
      "        scipy.stats.linregress : Calculate a linear least squares regression for\n",
      "                                 two sets of measurements.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        With ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\n",
      "        through `leastsq`. Note that this algorithm can only deal with\n",
      "        unconstrained problems.\n",
      "        \n",
      "        Box constraints can be handled by methods 'trf' and 'dogbox'. Refer to\n",
      "        the docstring of `least_squares` for more information.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.optimize import curve_fit\n",
      "        \n",
      "        >>> def func(x, a, b, c):\n",
      "        ...     return a * np.exp(-b * x) + c\n",
      "        \n",
      "        define the data to be fit with some noise\n",
      "        \n",
      "        >>> xdata = np.linspace(0, 4, 50)\n",
      "        >>> y = func(xdata, 2.5, 1.3, 0.5)\n",
      "        >>> y_noise = 0.2 * np.random.normal(size=xdata.size)\n",
      "        >>> ydata = y + y_noise\n",
      "        >>> plt.plot(xdata, ydata, 'b-', label='data')\n",
      "        \n",
      "        Fit for the parameters a, b, c of the function `func`\n",
      "        \n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata)\n",
      "        >>> plt.plot(xdata, func(xdata, *popt), 'r-', label='fit')\n",
      "        \n",
      "        Constrain the optimization to the region of ``0 < a < 3``, ``0 < b < 2``\n",
      "        and ``0 < c < 1``:\n",
      "        \n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 2., 1.]))\n",
      "        >>> plt.plot(xdata, func(xdata, *popt), 'g--', label='fit-with-bounds')\n",
      "        \n",
      "        >>> plt.xlabel('x')\n",
      "        >>> plt.ylabel('y')\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "    \n",
      "    diagbroyden(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using diagonal Broyden Jacobian approximation.\n",
      "        \n",
      "        The Jacobian approximation is derived from previous iterations, by\n",
      "        retaining only the diagonal of Broyden matrices.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        x0 : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "    \n",
      "    differential_evolution(func, bounds, args=(), strategy='best1bin', maxiter=1000, popsize=15, tol=0.01, mutation=(0.5, 1), recombination=0.7, seed=None, callback=None, disp=False, polish=True, init='latinhypercube', atol=0)\n",
      "        Finds the global minimum of a multivariate function.\n",
      "        Differential Evolution is stochastic in nature (does not use gradient\n",
      "        methods) to find the minimium, and can search large areas of candidate\n",
      "        space, but often requires larger numbers of function evaluations than\n",
      "        conventional gradient based techniques.\n",
      "        \n",
      "        The algorithm is due to Storn and Price [1]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized.  Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a  tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence\n",
      "            Bounds for variables.  ``(min, max)`` pairs for each element in ``x``,\n",
      "            defining the lower and upper bounds for the optimizing argument of\n",
      "            `func`. It is required to have ``len(bounds) == len(x)``.\n",
      "            ``len(bounds)`` is used to determine the number of parameters in ``x``.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to\n",
      "            completely specify the objective function.\n",
      "        strategy : str, optional\n",
      "            The differential evolution strategy to use. Should be one of:\n",
      "        \n",
      "                - 'best1bin'\n",
      "                - 'best1exp'\n",
      "                - 'rand1exp'\n",
      "                - 'randtobest1exp'\n",
      "                - 'best2exp'\n",
      "                - 'rand2exp'\n",
      "                - 'randtobest1bin'\n",
      "                - 'best2bin'\n",
      "                - 'rand2bin'\n",
      "                - 'rand1bin'\n",
      "        \n",
      "            The default is 'best1bin'.\n",
      "        maxiter : int, optional\n",
      "            The maximum number of generations over which the entire population is\n",
      "            evolved. The maximum number of function evaluations (with no polishing)\n",
      "            is: ``(maxiter + 1) * popsize * len(x)``\n",
      "        popsize : int, optional\n",
      "            A multiplier for setting the total population size.  The population has\n",
      "            ``popsize * len(x)`` individuals.\n",
      "        tol : float, optional\n",
      "            Relative tolerance for convergence, the solving stops when\n",
      "            ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "            where and `atol` and `tol` are the absolute and relative tolerance\n",
      "            respectively.\n",
      "        mutation : float or tuple(float, float), optional\n",
      "            The mutation constant. In the literature this is also known as\n",
      "            differential weight, being denoted by F.\n",
      "            If specified as a float it should be in the range [0, 2].\n",
      "            If specified as a tuple ``(min, max)`` dithering is employed. Dithering\n",
      "            randomly changes the mutation constant on a generation by generation\n",
      "            basis. The mutation constant for that generation is taken from\n",
      "            ``U[min, max)``. Dithering can help speed convergence significantly.\n",
      "            Increasing the mutation constant increases the search radius, but will\n",
      "            slow down convergence.\n",
      "        recombination : float, optional\n",
      "            The recombination constant, should be in the range [0, 1]. In the\n",
      "            literature this is also known as the crossover probability, being\n",
      "            denoted by CR. Increasing this value allows a larger number of mutants\n",
      "            to progress into the next generation, but at the risk of population\n",
      "            stability.\n",
      "        seed : int or `np.random.RandomState`, optional\n",
      "            If `seed` is not specified the `np.RandomState` singleton is used.\n",
      "            If `seed` is an int, a new `np.random.RandomState` instance is used,\n",
      "            seeded with seed.\n",
      "            If `seed` is already a `np.random.RandomState instance`, then that\n",
      "            `np.random.RandomState` instance is used.\n",
      "            Specify `seed` for repeatable minimizations.\n",
      "        disp : bool, optional\n",
      "            Display status messages\n",
      "        callback : callable, `callback(xk, convergence=val)`, optional\n",
      "            A function to follow the progress of the minimization. ``xk`` is\n",
      "            the current value of ``x0``. ``val`` represents the fractional\n",
      "            value of the population convergence.  When ``val`` is greater than one\n",
      "            the function halts. If callback returns `True`, then the minimization\n",
      "            is halted (any polishing is still carried out).\n",
      "        polish : bool, optional\n",
      "            If True (default), then `scipy.optimize.minimize` with the `L-BFGS-B`\n",
      "            method is used to polish the best population member at the end, which\n",
      "            can improve the minimization slightly.\n",
      "        init : string, optional\n",
      "            Specify how the population initialization is performed. Should be\n",
      "            one of:\n",
      "        \n",
      "                - 'latinhypercube'\n",
      "                - 'random'\n",
      "        \n",
      "            The default is 'latinhypercube'. Latin Hypercube sampling tries to\n",
      "            maximize coverage of the available parameter space. 'random' initializes\n",
      "            the population randomly - this has the drawback that clustering can\n",
      "            occur, preventing the whole of parameter space being covered.\n",
      "        atol : float, optional\n",
      "            Absolute tolerance for convergence, the solving stops when\n",
      "            ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "            where and `atol` and `tol` are the absolute and relative tolerance\n",
      "            respectively.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.  If `polish`\n",
      "            was employed, and a lower minimum was obtained by the polishing, then\n",
      "            OptimizeResult also contains the ``jac`` attribute.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Differential evolution is a stochastic population based method that is\n",
      "        useful for global optimization problems. At each pass through the population\n",
      "        the algorithm mutates each candidate solution by mixing with other candidate\n",
      "        solutions to create a trial candidate. There are several strategies [2]_ for\n",
      "        creating trial candidates, which suit some problems more than others. The\n",
      "        'best1bin' strategy is a good starting point for many systems. In this\n",
      "        strategy two members of the population are randomly chosen. Their difference\n",
      "        is used to mutate the best member (the `best` in `best1bin`), :math:`b_0`,\n",
      "        so far:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            b' = b_0 + mutation * (population[rand0] - population[rand1])\n",
      "        \n",
      "        A trial vector is then constructed. Starting with a randomly chosen 'i'th\n",
      "        parameter the trial is sequentially filled (in modulo) with parameters from\n",
      "        `b'` or the original candidate. The choice of whether to use `b'` or the\n",
      "        original candidate is made with a binomial distribution (the 'bin' in\n",
      "        'best1bin') - a random number in [0, 1) is generated.  If this number is\n",
      "        less than the `recombination` constant then the parameter is loaded from\n",
      "        `b'`, otherwise it is loaded from the original candidate.  The final\n",
      "        parameter is always loaded from `b'`.  Once the trial candidate is built\n",
      "        its fitness is assessed. If the trial is better than the original candidate\n",
      "        then it takes its place. If it is also better than the best overall\n",
      "        candidate it also replaces that.\n",
      "        To improve your chances of finding a global minimum use higher `popsize`\n",
      "        values, with higher `mutation` and (dithering), but lower `recombination`\n",
      "        values. This has the effect of widening the search radius, but slowing\n",
      "        convergence.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "        function is implemented in `rosen` in `scipy.optimize`.\n",
      "        \n",
      "        >>> from scipy.optimize import rosen, differential_evolution\n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = differential_evolution(rosen, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "        \n",
      "        Next find the minimum of the Ackley function\n",
      "        (http://en.wikipedia.org/wiki/Test_functions_for_optimization).\n",
      "        \n",
      "        >>> from scipy.optimize import differential_evolution\n",
      "        >>> import numpy as np\n",
      "        >>> def ackley(x):\n",
      "        ...     arg1 = -0.2 * np.sqrt(0.5 * (x[0] ** 2 + x[1] ** 2))\n",
      "        ...     arg2 = 0.5 * (np.cos(2. * np.pi * x[0]) + np.cos(2. * np.pi * x[1]))\n",
      "        ...     return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e\n",
      "        >>> bounds = [(-5, 5), (-5, 5)]\n",
      "        >>> result = differential_evolution(ackley, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([ 0.,  0.]), 4.4408920985006262e-16)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Storn, R and Price, K, Differential Evolution - a Simple and\n",
      "               Efficient Heuristic for Global Optimization over Continuous Spaces,\n",
      "               Journal of Global Optimization, 1997, 11, 341 - 359.\n",
      "        .. [2] http://www1.icsi.berkeley.edu/~storn/code.html\n",
      "        .. [3] http://en.wikipedia.org/wiki/Differential_evolution\n",
      "    \n",
      "    excitingmixing(F, xin, iter=None, alpha=None, alphamax=1.0, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using a tuned diagonal Jacobian approximation.\n",
      "        \n",
      "        The Jacobian matrix is diagonal and is tuned on each iteration.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        x0 : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial Jacobian approximation is (-1/alpha).\n",
      "        alphamax : float, optional\n",
      "            The entries of the diagonal Jacobian are kept in the range\n",
      "            ``[alpha, alphamax]``.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "    \n",
      "    fixed_point(func, x0, args=(), xtol=1e-08, maxiter=500, method='del2')\n",
      "        Find a fixed point of the function.\n",
      "        \n",
      "        Given a function of one or more variables and a starting point, find a\n",
      "        fixed-point of the function: i.e. where ``func(x0) == x0``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            Function to evaluate.\n",
      "        x0 : array_like\n",
      "            Fixed point of function.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to `func`.\n",
      "        xtol : float, optional\n",
      "            Convergence tolerance, defaults to 1e-08.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations, defaults to 500.\n",
      "        method : {\"del2\", \"iteration\"}, optional\n",
      "            Method of finding the fixed-point, defaults to \"del2\"\n",
      "            which uses Steffensen's Method with Aitken's ``Del^2``\n",
      "            convergence acceleration [1]_. The \"iteration\" method simply iterates\n",
      "            the function until convergence is detected, without attempting to\n",
      "            accelerate the convergence.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Burden, Faires, \"Numerical Analysis\", 5th edition, pg. 80\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c1, c2):\n",
      "        ...    return np.sqrt(c1/(x+c2))\n",
      "        >>> c1 = np.array([10,12.])\n",
      "        >>> c2 = np.array([3, 5.])\n",
      "        >>> optimize.fixed_point(func, [1.2, 1.3], args=(c1,c2))\n",
      "        array([ 1.4920333 ,  1.37228132])\n",
      "    \n",
      "    fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, initial_simplex=None)\n",
      "        Minimize a function using the downhill simplex algorithm.\n",
      "        \n",
      "        This algorithm only uses function values, not derivatives or second\n",
      "        derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            The objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func, i.e. ``f(x,*args)``.\n",
      "        xtol : float, optional\n",
      "            Absolute error in xopt between iterations that is acceptable for\n",
      "            convergence.\n",
      "        ftol : number, optional\n",
      "            Absolute error in func(xopt) between iterations that is acceptable for\n",
      "            convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : number, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            Set to True if fopt and warnflag outputs are desired.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages.\n",
      "        retall : bool, optional\n",
      "            Set to True to return list of solutions at each iteration.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        initial_simplex : array_like of shape (N + 1, N), optional\n",
      "            Initial simplex. If given, overrides `x0`.\n",
      "            ``initial_simplex[j,:]`` should contain the coordinates of\n",
      "            the j-th vertex of the ``N+1`` vertices in the simplex, where\n",
      "            ``N`` is the dimension.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter that minimizes function.\n",
      "        fopt : float\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        iter : int\n",
      "            Number of iterations performed.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            1 : Maximum number of function evaluations made.\n",
      "            2 : Maximum number of iterations reached.\n",
      "        allvecs : list\n",
      "            Solution at each iteration.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Nelder-Mead' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
      "        one or more variables.\n",
      "        \n",
      "        This algorithm has a long history of successful use in applications.\n",
      "        But it will usually be slower than an algorithm that uses first or\n",
      "        second derivative information. In practice it can have poor\n",
      "        performance in high-dimensional problems and is not robust to\n",
      "        minimizing complicated functions. Additionally, there currently is no\n",
      "        complete theory describing when the algorithm will successfully\n",
      "        converge to the minimum, or how fast it will if it does. Both the ftol and\n",
      "        xtol criteria must be met for convergence.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
      "               minimization\", The Computer Journal, 7, pp. 308-313\n",
      "        \n",
      "        .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
      "               Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
      "               1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
      "               Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
      "               Harlow, UK, pp. 191-208.\n",
      "    \n",
      "    fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Minimize a function using the BFGS algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable f(x,*args)\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable f'(x,*args), optional\n",
      "            Gradient of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f and fprime.\n",
      "        gtol : float, optional\n",
      "            Gradient norm must be less than gtol before successful termination.\n",
      "        norm : float, optional\n",
      "            Order of norm (Inf is max, -Inf is min)\n",
      "        epsilon : int or ndarray, optional\n",
      "            If fprime is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function to call after each\n",
      "            iteration.  Called as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True,return fopt, func_calls, grad_calls, and warnflag\n",
      "            in addition to xopt.\n",
      "        disp : bool, optional\n",
      "            Print convergence message if True.\n",
      "        retall : bool, optional\n",
      "            Return a list of results at each iteration if True.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e. f(xopt) == fopt.\n",
      "        fopt : float\n",
      "            Minimum value.\n",
      "        gopt : ndarray\n",
      "            Value of gradient at minimum, f'(xopt), which should be near 0.\n",
      "        Bopt : ndarray\n",
      "            Value of 1/f''(xopt), i.e. the inverse hessian matrix.\n",
      "        func_calls : int\n",
      "            Number of function_calls made.\n",
      "        grad_calls : int\n",
      "            Number of gradient calls made.\n",
      "        warnflag : integer\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Gradient and/or function calls not changing.\n",
      "        allvecs  :  list\n",
      "            `OptimizeResult` at each iteration.  Only returned if retall is True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'BFGS' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Optimize the function, f, whose gradient is given by fprime\n",
      "        using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
      "        and Shanno (BFGS)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright, and Nocedal 'Numerical Optimization', 1999, pg. 198.\n",
      "    \n",
      "    fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Minimize a function using a nonlinear conjugate gradient algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable, ``f(x, *args)``\n",
      "            Objective function to be minimized.  Here `x` must be a 1-D array of\n",
      "            the variables that are to be changed in the search for a minimum, and\n",
      "            `args` are the other (fixed) parameters of `f`.\n",
      "        x0 : ndarray\n",
      "            A user-supplied initial estimate of `xopt`, the optimal value of `x`.\n",
      "            It must be a 1-D array of values.\n",
      "        fprime : callable, ``fprime(x, *args)``, optional\n",
      "            A function that returns the gradient of `f` at `x`. Here `x` and `args`\n",
      "            are as described above for `f`. The returned value must be a 1-D array.\n",
      "            Defaults to None, in which case the gradient is approximated\n",
      "            numerically (see `epsilon`, below).\n",
      "        args : tuple, optional\n",
      "            Parameter values passed to `f` and `fprime`. Must be supplied whenever\n",
      "            additional fixed parameters are needed to completely specify the\n",
      "            functions `f` and `fprime`.\n",
      "        gtol : float, optional\n",
      "            Stop when the norm of the gradient is less than `gtol`.\n",
      "        norm : float, optional\n",
      "            Order to use for the norm of the gradient\n",
      "            (``-np.Inf`` is min, ``np.Inf`` is max).\n",
      "        epsilon : float or ndarray, optional\n",
      "            Step size(s) to use when `fprime` is approximated numerically. Can be a\n",
      "            scalar or a 1-D array.  Defaults to ``sqrt(eps)``, with eps the\n",
      "            floating point machine precision.  Usually ``sqrt(eps)`` is about\n",
      "            1.5e-8.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Default is ``200 * len(x0)``.\n",
      "        full_output : bool, optional\n",
      "            If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\n",
      "            addition to `xopt`.  See the Returns section below for additional\n",
      "            information on optional return values.\n",
      "        disp : bool, optional\n",
      "            If True, return a convergence message, followed by `xopt`.\n",
      "        retall : bool, optional\n",
      "            If True, add to the returned values the results of each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each iteration.\n",
      "            Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
      "        fopt : float, optional\n",
      "            Minimum value found, f(xopt).  Only returned if `full_output` is True.\n",
      "        func_calls : int, optional\n",
      "            The number of function_calls made.  Only returned if `full_output`\n",
      "            is True.\n",
      "        grad_calls : int, optional\n",
      "            The number of gradient calls made. Only returned if `full_output` is\n",
      "            True.\n",
      "        warnflag : int, optional\n",
      "            Integer value with warning status, only returned if `full_output` is\n",
      "            True.\n",
      "        \n",
      "            0 : Success.\n",
      "        \n",
      "            1 : The maximum number of iterations was exceeded.\n",
      "        \n",
      "            2 : Gradient and/or function calls were not changing.  May indicate\n",
      "                that precision was lost, i.e., the routine did not converge.\n",
      "        \n",
      "        allvecs : list of ndarray, optional\n",
      "            List of arrays, containing the results at each iteration.\n",
      "            Only returned if `retall` is True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        minimize : common interface to all `scipy.optimize` algorithms for\n",
      "                   unconstrained and constrained minimization of multivariate\n",
      "                   functions.  It provides an alternative way to call\n",
      "                   ``fmin_cg``, by specifying ``method='CG'``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This conjugate gradient algorithm is based on that of Polak and Ribiere\n",
      "        [1]_.\n",
      "        \n",
      "        Conjugate gradient methods tend to work better when:\n",
      "        \n",
      "        1. `f` has a unique global minimizing point, and no local minima or\n",
      "           other stationary points,\n",
      "        2. `f` is, at least locally, reasonably well approximated by a\n",
      "           quadratic function of the variables,\n",
      "        3. `f` is continuous and has a continuous gradient,\n",
      "        4. `fprime` is not too large, e.g., has a norm less than 1000,\n",
      "        5. The initial guess, `x0`, is reasonably close to `f` 's global\n",
      "           minimizing point, `xopt`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Example 1: seek the minimum value of the expression\n",
      "        ``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\n",
      "        of the parameters and an initial guess ``(u, v) = (0, 0)``.\n",
      "        \n",
      "        >>> args = (2, 3, 7, 8, 9, 10)  # parameter values\n",
      "        >>> def f(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\n",
      "        >>> def gradf(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     gu = 2*a*u + b*v + d     # u-component of the gradient\n",
      "        ...     gv = b*u + 2*c*v + e     # v-component of the gradient\n",
      "        ...     return np.asarray((gu, gv))\n",
      "        >>> x0 = np.asarray((0, 0))  # Initial guess.\n",
      "        >>> from scipy import optimize\n",
      "        >>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 1.617021\n",
      "                 Iterations: 4\n",
      "                 Function evaluations: 8\n",
      "                 Gradient evaluations: 8\n",
      "        >>> res1\n",
      "        array([-1.80851064, -0.25531915])\n",
      "        \n",
      "        Example 2: solve the same problem using the `minimize` function.\n",
      "        (This `myopts` dictionary shows all of the available options,\n",
      "        although in practice only non-default values would be needed.\n",
      "        The returned value will be a dictionary.)\n",
      "        \n",
      "        >>> opts = {'maxiter' : None,    # default value.\n",
      "        ...         'disp' : True,    # non-default value.\n",
      "        ...         'gtol' : 1e-5,    # default value.\n",
      "        ...         'norm' : np.inf,  # default value.\n",
      "        ...         'eps' : 1.4901161193847656e-08}  # default value.\n",
      "        >>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\n",
      "        ...                          method='CG', options=opts)\n",
      "        Optimization terminated successfully.\n",
      "                Current function value: 1.617021\n",
      "                Iterations: 4\n",
      "                Function evaluations: 8\n",
      "                Gradient evaluations: 8\n",
      "        >>> res2.x  # minimum found\n",
      "        array([-1.80851064, -0.25531915])\n",
      "    \n",
      "    fmin_cobyla(func, x0, cons, args=(), consargs=None, rhobeg=1.0, rhoend=0.0001, iprint=1, maxfun=1000, disp=None, catol=0.0002)\n",
      "        Minimize a function using the Constrained Optimization BY Linear\n",
      "        Approximation (COBYLA) method. This method wraps a FORTRAN\n",
      "        implementation of the algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            Function to minimize. In the form func(x, \\*args).\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        cons : sequence\n",
      "            Constraint functions; must all be ``>=0`` (a single function\n",
      "            if only 1 constraint). Each function takes the parameters `x`\n",
      "            as its first argument, and it can return either a single number or\n",
      "            an array or list of numbers.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to pass to function.\n",
      "        consargs : tuple, optional\n",
      "            Extra arguments to pass to constraint functions (default of None means\n",
      "            use same extra arguments as those passed to func).\n",
      "            Use ``()`` for no extra arguments.\n",
      "        rhobeg : float, optional\n",
      "            Reasonable initial changes to the variables.\n",
      "        rhoend : float, optional\n",
      "            Final accuracy in the optimization (not precisely guaranteed). This\n",
      "            is a lower bound on the size of the trust region.\n",
      "        iprint : {0, 1, 2, 3}, optional\n",
      "            Controls the frequency of output; 0 implies no output.  Deprecated.\n",
      "        disp : {0, 1, 2, 3}, optional\n",
      "            Over-rides the iprint interface.  Preferred.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations.\n",
      "        catol : float, optional\n",
      "            Absolute tolerance for constraint violations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The argument that minimises `f`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'COBYLA' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm is based on linear approximations to the objective\n",
      "        function and each constraint. We briefly describe the algorithm.\n",
      "        \n",
      "        Suppose the function is being minimized over k variables. At the\n",
      "        jth iteration the algorithm has k+1 points v_1, ..., v_(k+1),\n",
      "        an approximate solution x_j, and a radius RHO_j.\n",
      "        (i.e. linear plus a constant) approximations to the objective\n",
      "        function and constraint functions such that their function values\n",
      "        agree with the linear approximation on the k+1 points v_1,.., v_(k+1).\n",
      "        This gives a linear program to solve (where the linear approximations\n",
      "        of the constraint functions are constrained to be non-negative).\n",
      "        \n",
      "        However the linear approximations are likely only good\n",
      "        approximations near the current simplex, so the linear program is\n",
      "        given the further requirement that the solution, which\n",
      "        will become x_(j+1), must be within RHO_j from x_j. RHO_j only\n",
      "        decreases, never increases. The initial RHO_j is rhobeg and the\n",
      "        final RHO_j is rhoend. In this way COBYLA's iterations behave\n",
      "        like a trust region algorithm.\n",
      "        \n",
      "        Additionally, the linear program may be inconsistent, or the\n",
      "        approximation may give poor improvement. For details about\n",
      "        how these issues are resolved, as well as how the points v_i are\n",
      "        updated, refer to the source code or the references below.\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1994), \"A direct search optimization method that models\n",
      "        the objective and constraint functions by linear interpolation.\", in\n",
      "        Advances in Optimization and Numerical Analysis, eds. S. Gomez and\n",
      "        J-P Hennart, Kluwer Academic (Dordrecht), pp. 51-67\n",
      "        \n",
      "        Powell M.J.D. (1998), \"Direct search algorithms for optimization\n",
      "        calculations\", Acta Numerica 7, 287-336\n",
      "        \n",
      "        Powell M.J.D. (2007), \"A view of algorithms for optimization without\n",
      "        derivatives\", Cambridge University Technical Report DAMTP 2007/NA03\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Minimize the objective function f(x,y) = x*y subject\n",
      "        to the constraints x**2 + y**2 < 1 and y > 0::\n",
      "        \n",
      "            >>> def objective(x):\n",
      "            ...     return x[0]*x[1]\n",
      "            ...\n",
      "            >>> def constr1(x):\n",
      "            ...     return 1 - (x[0]**2 + x[1]**2)\n",
      "            ...\n",
      "            >>> def constr2(x):\n",
      "            ...     return x[1]\n",
      "            ...\n",
      "            >>> from scipy.optimize import fmin_cobyla\n",
      "            >>> fmin_cobyla(objective, [0.0, 0.1], [constr1, constr2], rhoend=1e-7)\n",
      "            array([-0.70710685,  0.70710671])\n",
      "        \n",
      "        The exact solution is (-sqrt(2)/2, sqrt(2)/2).\n",
      "    \n",
      "    fmin_l_bfgs_b(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, m=10, factr=10000000.0, pgtol=1e-05, epsilon=1e-08, iprint=-1, maxfun=15000, maxiter=15000, disp=None, callback=None, maxls=20)\n",
      "        Minimize a function func using the L-BFGS-B algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Function to minimise.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable fprime(x,*args), optional\n",
      "            The gradient of `func`.  If None, then `func` returns the function\n",
      "            value and the gradient (``f, g = func(x, *args)``), unless\n",
      "            `approx_grad` is True in which case `func` returns only ``f``.\n",
      "        args : sequence, optional\n",
      "            Arguments to pass to `func` and `fprime`.\n",
      "        approx_grad : bool, optional\n",
      "            Whether to approximate the gradient numerically (in which case\n",
      "            `func` returns only the function value).\n",
      "        bounds : list, optional\n",
      "            ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the bounds on that parameter. Use None or +-inf for one of ``min`` or\n",
      "            ``max`` when there is no bound in that direction.\n",
      "        m : int, optional\n",
      "            The maximum number of variable metric corrections\n",
      "            used to define the limited memory matrix. (The limited memory BFGS\n",
      "            method does not store the full hessian but uses this many terms in an\n",
      "            approximation to it.)\n",
      "        factr : float, optional\n",
      "            The iteration stops when\n",
      "            ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,\n",
      "            where ``eps`` is the machine precision, which is automatically\n",
      "            generated by the code. Typical values for `factr` are: 1e12 for\n",
      "            low accuracy; 1e7 for moderate accuracy; 10.0 for extremely\n",
      "            high accuracy.\n",
      "        pgtol : float, optional\n",
      "            The iteration will stop when\n",
      "            ``max{|proj g_i | i = 1, ..., n} <= pgtol``\n",
      "            where ``pg_i`` is the i-th component of the projected gradient.\n",
      "        epsilon : float, optional\n",
      "            Step size used when `approx_grad` is True, for numerically\n",
      "            calculating the gradient\n",
      "        iprint : int, optional\n",
      "            Controls the frequency of output. ``iprint < 0`` means no output;\n",
      "            ``iprint = 0``    print only one line at the last iteration;\n",
      "            ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;\n",
      "            ``iprint = 99``   print details of every iteration except n-vectors;\n",
      "            ``iprint = 100``  print also the changes of active set and final x;\n",
      "            ``iprint > 100``  print details of every iteration including x and g.\n",
      "        disp : int, optional\n",
      "            If zero, then no output.  If a positive number, then this over-rides\n",
      "            `iprint` (i.e., `iprint` gets the value of `disp`).\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        maxls : int, optional\n",
      "            Maximum number of line search steps (per iteration). Default is 20.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : array_like\n",
      "            Estimated position of the minimum.\n",
      "        f : float\n",
      "            Value of `func` at the minimum.\n",
      "        d : dict\n",
      "            Information dictionary.\n",
      "        \n",
      "            * d['warnflag'] is\n",
      "        \n",
      "              - 0 if converged,\n",
      "              - 1 if too many function evaluations or too many iterations,\n",
      "              - 2 if stopped for another reason, given in d['task']\n",
      "        \n",
      "            * d['grad'] is the gradient at the minimum (should be 0 ish)\n",
      "            * d['funcalls'] is the number of function calls made.\n",
      "            * d['nit'] is the number of iterations.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'L-BFGS-B' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        License of L-BFGS-B (FORTRAN code):\n",
      "        \n",
      "        The version included here (in fortran code) is 3.0\n",
      "        (released April 25, 2011).  It was written by Ciyou Zhu, Richard Byrd,\n",
      "        and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following\n",
      "        condition for use:\n",
      "        \n",
      "        This software is freely available, but we expect that all publications\n",
      "        describing work using this software, or all commercial products using it,\n",
      "        quote at least one of the references given below. This software is released\n",
      "        under the BSD License.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound\n",
      "          Constrained Optimization, (1995), SIAM Journal on Scientific and\n",
      "          Statistical Computing, 16, 5, pp. 1190-1208.\n",
      "        * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,\n",
      "          FORTRAN routines for large scale bound constrained optimization (1997),\n",
      "          ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.\n",
      "        * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,\n",
      "          FORTRAN routines for large scale bound constrained optimization (2011),\n",
      "          ACM Transactions on Mathematical Software, 38, 1.\n",
      "    \n",
      "    fmin_ncg(f, x0, fprime, fhess_p=None, fhess=None, args=(), avextol=1e-05, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Unconstrained minimization of a function using the Newton-CG method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable ``f(x, *args)``\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable ``f'(x, *args)``\n",
      "            Gradient of f.\n",
      "        fhess_p : callable ``fhess_p(x, p, *args)``, optional\n",
      "            Function which computes the Hessian of f times an\n",
      "            arbitrary vector, p.\n",
      "        fhess : callable ``fhess(x, *args)``, optional\n",
      "            Function to compute the Hessian matrix of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f, fprime, fhess_p, and fhess\n",
      "            (the same set of extra arguments is supplied to all of\n",
      "            these functions).\n",
      "        epsilon : float or ndarray, optional\n",
      "            If fhess is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function which is called after\n",
      "            each iteration.  Called as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        avextol : float, optional\n",
      "            Convergence is assumed when the average relative error in\n",
      "            the minimizer falls below this amount.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True, return the optional outputs.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence message.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of results at each iteration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
      "        fopt : float\n",
      "            Value of the function at xopt, i.e. ``fopt = f(xopt)``.\n",
      "        fcalls : int\n",
      "            Number of function calls made.\n",
      "        gcalls : int\n",
      "            Number of gradient calls made.\n",
      "        hcalls : int\n",
      "            Number of hessian calls made.\n",
      "        warnflag : int\n",
      "            Warnings generated by the algorithm.\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "        allvecs : list\n",
      "            The result at each iteration, if retall is True (see below).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Newton-CG' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Only one of `fhess_p` or `fhess` need to be given.  If `fhess`\n",
      "        is provided, then `fhess_p` will be ignored.  If neither `fhess`\n",
      "        nor `fhess_p` is provided, then the hessian product will be\n",
      "        approximated using finite differences on `fprime`. `fhess_p`\n",
      "        must compute the hessian times an arbitrary vector. If it is not\n",
      "        given, finite-differences on `fprime` are used to compute\n",
      "        it.\n",
      "        \n",
      "        Newton-CG methods are also called truncated Newton methods. This\n",
      "        function differs from scipy.optimize.fmin_tnc because\n",
      "        \n",
      "        1. scipy.optimize.fmin_ncg is written purely in python using numpy\n",
      "            and scipy while scipy.optimize.fmin_tnc calls a C function.\n",
      "        2. scipy.optimize.fmin_ncg is only for unconstrained minimization\n",
      "            while scipy.optimize.fmin_tnc is for unconstrained minimization\n",
      "            or box constrained minimization. (Box constraints give\n",
      "            lower and upper bounds for each variable separately.)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright & Nocedal, 'Numerical Optimization', 1999, pg. 140.\n",
      "    \n",
      "    fmin_powell(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, direc=None)\n",
      "        Minimize a function using modified Powell's method. This method\n",
      "        only uses function values, not derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each\n",
      "            iteration.  Called as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        direc : ndarray, optional\n",
      "            Initial direction set.\n",
      "        xtol : float, optional\n",
      "            Line-search error tolerance.\n",
      "        ftol : float, optional\n",
      "            Relative error in ``func(xopt)`` acceptable for convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            If True, fopt, xi, direc, iter, funcalls, and\n",
      "            warnflag are returned.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence messages.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of the solution at each iteration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter which minimizes `func`.\n",
      "        fopt : number\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        direc : ndarray\n",
      "            Current direction set.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            Integer warning flag:\n",
      "                1 : Maximum number of function evaluations.\n",
      "                2 : Maximum number of iterations.\n",
      "        allvecs : list\n",
      "            List of solutions at each iteration.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to unconstrained minimization algorithms for\n",
      "            multivariate functions. See the 'Powell' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses a modification of Powell's method to find the minimum of\n",
      "        a function of N variables. Powell's method is a conjugate\n",
      "        direction method.\n",
      "        \n",
      "        The algorithm has two loops. The outer loop\n",
      "        merely iterates over the inner loop. The inner loop minimizes\n",
      "        over each current direction in the direction set. At the end\n",
      "        of the inner loop, if certain conditions are met, the direction\n",
      "        that gave the largest decrease is dropped and replaced with\n",
      "        the difference between the current estimated x and the estimated\n",
      "        x from the beginning of the inner-loop.\n",
      "        \n",
      "        The technical conditions for replacing the direction of greatest\n",
      "        increase amount to checking that\n",
      "        \n",
      "        1. No further gain can be made along the direction of greatest increase\n",
      "           from that iteration.\n",
      "        2. The direction of greatest increase accounted for a large sufficient\n",
      "           fraction of the decrease in the function value from that iteration of\n",
      "           the inner loop.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1964) An efficient method for finding the minimum of a\n",
      "        function of several variables without calculating derivatives,\n",
      "        Computer Journal, 7 (2):155-162.\n",
      "        \n",
      "        Press W., Teukolsky S.A., Vetterling W.T., and Flannery B.P.:\n",
      "        Numerical Recipes (any edition), Cambridge University Press\n",
      "    \n",
      "    fmin_slsqp(func, x0, eqcons=(), f_eqcons=None, ieqcons=(), f_ieqcons=None, bounds=(), fprime=None, fprime_eqcons=None, fprime_ieqcons=None, args=(), iter=100, acc=1e-06, iprint=1, disp=None, full_output=0, epsilon=1.4901161193847656e-08, callback=None)\n",
      "        Minimize a function using Sequential Least SQuares Programming\n",
      "        \n",
      "        Python interface function for the SLSQP Optimization subroutine\n",
      "        originally implemented by Dieter Kraft.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.  Must return a scalar.\n",
      "        x0 : 1-D ndarray of float\n",
      "            Initial guess for the independent variable(s).\n",
      "        eqcons : list, optional\n",
      "            A list of functions of length n such that\n",
      "            eqcons[j](x,*args) == 0.0 in a successfully optimized\n",
      "            problem.\n",
      "        f_eqcons : callable f(x,*args), optional\n",
      "            Returns a 1-D array in which each element must equal 0.0 in a\n",
      "            successfully optimized problem.  If f_eqcons is specified,\n",
      "            eqcons is ignored.\n",
      "        ieqcons : list, optional\n",
      "            A list of functions of length n such that\n",
      "            ieqcons[j](x,*args) >= 0.0 in a successfully optimized\n",
      "            problem.\n",
      "        f_ieqcons : callable f(x,*args), optional\n",
      "            Returns a 1-D ndarray in which each element must be greater or\n",
      "            equal to 0.0 in a successfully optimized problem.  If\n",
      "            f_ieqcons is specified, ieqcons is ignored.\n",
      "        bounds : list, optional\n",
      "            A list of tuples specifying the lower and upper bound\n",
      "            for each independent variable [(xl0, xu0),(xl1, xu1),...]\n",
      "            Infinite values will be interpreted as large floating values.\n",
      "        fprime : callable `f(x,*args)`, optional\n",
      "            A function that evaluates the partial derivatives of func.\n",
      "        fprime_eqcons : callable `f(x,*args)`, optional\n",
      "            A function of the form `f(x, *args)` that returns the m by n\n",
      "            array of equality constraint normals.  If not provided,\n",
      "            the normals will be approximated. The array returned by\n",
      "            fprime_eqcons should be sized as ( len(eqcons), len(x0) ).\n",
      "        fprime_ieqcons : callable `f(x,*args)`, optional\n",
      "            A function of the form `f(x, *args)` that returns the m by n\n",
      "            array of inequality constraint normals.  If not provided,\n",
      "            the normals will be approximated. The array returned by\n",
      "            fprime_ieqcons should be sized as ( len(ieqcons), len(x0) ).\n",
      "        args : sequence, optional\n",
      "            Additional arguments passed to func and fprime.\n",
      "        iter : int, optional\n",
      "            The maximum number of iterations.\n",
      "        acc : float, optional\n",
      "            Requested accuracy.\n",
      "        iprint : int, optional\n",
      "            The verbosity of fmin_slsqp :\n",
      "        \n",
      "            * iprint <= 0 : Silent operation\n",
      "            * iprint == 1 : Print summary upon completion (default)\n",
      "            * iprint >= 2 : Print status of each iterate and summary\n",
      "        disp : int, optional\n",
      "            Over-rides the iprint interface (preferred).\n",
      "        full_output : bool, optional\n",
      "            If False, return only the minimizer of func (default).\n",
      "            Otherwise, output final objective function and summary\n",
      "            information.\n",
      "        epsilon : float, optional\n",
      "            The step size for finite-difference derivative estimates.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(x)``, where ``x`` is the\n",
      "            current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray of float\n",
      "            The final minimizer of func.\n",
      "        fx : ndarray of float, if full_output is true\n",
      "            The final value of the objective function.\n",
      "        its : int, if full_output is true\n",
      "            The number of iterations.\n",
      "        imode : int, if full_output is true\n",
      "            The exit mode from the optimizer (see below).\n",
      "        smode : string, if full_output is true\n",
      "            Message describing the exit mode from the optimizer.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'SLSQP' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Exit modes are defined as follows ::\n",
      "        \n",
      "            -1 : Gradient evaluation required (g & a)\n",
      "             0 : Optimization terminated successfully.\n",
      "             1 : Function evaluation required (f & c)\n",
      "             2 : More equality constraints than independent variables\n",
      "             3 : More than 3*n iterations in LSQ subproblem\n",
      "             4 : Inequality constraints incompatible\n",
      "             5 : Singular matrix E in LSQ subproblem\n",
      "             6 : Singular matrix C in LSQ subproblem\n",
      "             7 : Rank-deficient equality constraint subproblem HFTI\n",
      "             8 : Positive directional derivative for linesearch\n",
      "             9 : Iteration limit exceeded\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Examples are given :ref:`in the tutorial <tutorial-sqlsp>`.\n",
      "    \n",
      "    fmin_tnc(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, epsilon=1e-08, scale=None, offset=None, messages=15, maxCGit=-1, maxfun=None, eta=-1, stepmx=0, accuracy=0, fmin=0, ftol=-1, xtol=-1, pgtol=-1, rescale=-1, disp=None, callback=None)\n",
      "        Minimize a function with variables subject to bounds, using\n",
      "        gradient information in a truncated Newton algorithm. This\n",
      "        method wraps a C implementation of the algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x, *args)``\n",
      "            Function to minimize.  Must do one of:\n",
      "        \n",
      "            1. Return f and g, where f is the value of the function and g its\n",
      "               gradient (a list of floats).\n",
      "        \n",
      "            2. Return the function value but supply gradient function\n",
      "               separately as `fprime`.\n",
      "        \n",
      "            3. Return the function value and set ``approx_grad=True``.\n",
      "        \n",
      "            If the function returns None, the minimization\n",
      "            is aborted.\n",
      "        x0 : array_like\n",
      "            Initial estimate of minimum.\n",
      "        fprime : callable ``fprime(x, *args)``, optional\n",
      "            Gradient of `func`. If None, then either `func` must return the\n",
      "            function value and the gradient (``f,g = func(x, *args)``)\n",
      "            or `approx_grad` must be True.\n",
      "        args : tuple, optional\n",
      "            Arguments to pass to function.\n",
      "        approx_grad : bool, optional\n",
      "            If true, approximate the gradient numerically.\n",
      "        bounds : list, optional\n",
      "            (min, max) pairs for each element in x0, defining the\n",
      "            bounds on that parameter. Use None or +/-inf for one of\n",
      "            min or max when there is no bound in that direction.\n",
      "        epsilon : float, optional\n",
      "            Used if approx_grad is True. The stepsize in a finite\n",
      "            difference approximation for fprime.\n",
      "        scale : array_like, optional\n",
      "            Scaling factors to apply to each variable.  If None, the\n",
      "            factors are up-low for interval bounded variables and\n",
      "            1+|x| for the others.  Defaults to None.\n",
      "        offset : array_like, optional\n",
      "            Value to subtract from each variable.  If None, the\n",
      "            offsets are (up+low)/2 for interval bounded variables\n",
      "            and x for the others.\n",
      "        messages : int, optional\n",
      "            Bit mask used to select messages display during\n",
      "            minimization values defined in the MSGS dict.  Defaults to\n",
      "            MGS_ALL.\n",
      "        disp : int, optional\n",
      "            Integer interface to messages.  0 = no message, 5 = all messages\n",
      "        maxCGit : int, optional\n",
      "            Maximum number of hessian*vector evaluations per main\n",
      "            iteration.  If maxCGit == 0, the direction chosen is\n",
      "            -gradient if maxCGit < 0, maxCGit is set to\n",
      "            max(1,min(50,n/2)).  Defaults to -1.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluation.  if None, maxfun is\n",
      "            set to max(100, 10*len(x0)).  Defaults to None.\n",
      "        eta : float, optional\n",
      "            Severity of the line search. if < 0 or > 1, set to 0.25.\n",
      "            Defaults to -1.\n",
      "        stepmx : float, optional\n",
      "            Maximum step for the line search.  May be increased during\n",
      "            call.  If too small, it will be set to 10.0.  Defaults to 0.\n",
      "        accuracy : float, optional\n",
      "            Relative precision for finite difference calculations.  If\n",
      "            <= machine_precision, set to sqrt(machine_precision).\n",
      "            Defaults to 0.\n",
      "        fmin : float, optional\n",
      "            Minimum function value estimate.  Defaults to 0.\n",
      "        ftol : float, optional\n",
      "            Precision goal for the value of f in the stoping criterion.\n",
      "            If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
      "        xtol : float, optional\n",
      "            Precision goal for the value of x in the stopping\n",
      "            criterion (after applying x scaling factors).  If xtol <\n",
      "            0.0, xtol is set to sqrt(machine_precision).  Defaults to\n",
      "            -1.\n",
      "        pgtol : float, optional\n",
      "            Precision goal for the value of the projected gradient in\n",
      "            the stopping criterion (after applying x scaling factors).\n",
      "            If pgtol < 0.0, pgtol is set to 1e-2 * sqrt(accuracy).\n",
      "            Setting it to 0.0 is not recommended.  Defaults to -1.\n",
      "        rescale : float, optional\n",
      "            Scaling factor (in log10) used to trigger f value\n",
      "            rescaling.  If 0, rescale at each iteration.  If a large\n",
      "            value, never rescale.  If < 0, rescale is set to 1.3.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution.\n",
      "        nfeval : int\n",
      "            The number of function evaluations.\n",
      "        rc : int\n",
      "            Return code, see below\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'TNC' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The underlying algorithm is truncated Newton, also called\n",
      "        Newton Conjugate-Gradient. This method differs from\n",
      "        scipy.optimize.fmin_ncg in that\n",
      "        \n",
      "        1. It wraps a C implementation of the algorithm\n",
      "        2. It allows each variable to be given an upper and lower bound.\n",
      "        \n",
      "        The algorithm incoporates the bound constraints by determining\n",
      "        the descent direction as in an unconstrained truncated Newton,\n",
      "        but never taking a step-size large enough to leave the space\n",
      "        of feasible x's. The algorithm keeps track of a set of\n",
      "        currently active constraints, and ignores them when computing\n",
      "        the minimum allowable step size. (The x's associated with the\n",
      "        active constraint are kept fixed.) If the maximum allowable\n",
      "        step size is zero then a new constraint is added. At the end\n",
      "        of each iteration one of the constraints may be deemed no\n",
      "        longer active and removed. A constraint is considered\n",
      "        no longer active is if it is currently active\n",
      "        but the gradient for that variable points inward from the\n",
      "        constraint. The specific constraint removed is the one\n",
      "        associated with the variable of largest index whose\n",
      "        constraint is no longer active.\n",
      "        \n",
      "        Return codes are defined as follows::\n",
      "        \n",
      "            -1 : Infeasible (lower bound > upper bound)\n",
      "             0 : Local minimum reached (|pg| ~= 0)\n",
      "             1 : Converged (|f_n-f_(n-1)| ~= 0)\n",
      "             2 : Converged (|x_n-x_(n-1)| ~= 0)\n",
      "             3 : Max. number of function evaluations reached\n",
      "             4 : Linear search failed\n",
      "             5 : All lower bounds are equal to the upper bounds\n",
      "             6 : Unable to progress\n",
      "             7 : User requested end of minimization\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright S., Nocedal J. (2006), 'Numerical Optimization'\n",
      "        \n",
      "        Nash S.G. (1984), \"Newton-Type Minimization Via the Lanczos Method\",\n",
      "        SIAM Journal of Numerical Analysis 21, pp. 770-778\n",
      "    \n",
      "    fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1)\n",
      "        Bounded minimization for scalar functions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized (must accept and return scalars).\n",
      "        x1, x2 : float or array scalar\n",
      "            The optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to function.\n",
      "        xtol : float, optional\n",
      "            The convergence tolerance.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations allowed.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        disp : int, optional\n",
      "            If non-zero, print messages.\n",
      "                0 : no message printing.\n",
      "                1 : non-convergence notification messages only.\n",
      "                2 : print a message on convergence too.\n",
      "                3 : print iteration results.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters (over given interval) which minimize the\n",
      "            objective function.\n",
      "        fval : number\n",
      "            The function value at the minimum point.\n",
      "        ierr : int\n",
      "            An error flag (0 if converged, 1 if maximum number of\n",
      "            function calls reached).\n",
      "        numfunc : int\n",
      "          The number of function calls made.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Bounded' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Finds a local minimizer of the scalar function `func` in the\n",
      "        interval x1 < xopt < x2 using Brent's method.  (See `brent`\n",
      "        for auto-bracketing).\n",
      "    \n",
      "    fsolve(func, x0, args=(), fprime=None, full_output=0, col_deriv=0, xtol=1.49012e-08, maxfev=0, band=None, epsfcn=None, factor=100, diag=None)\n",
      "        Find the roots of a function.\n",
      "        \n",
      "        Return the roots of the (non-linear) equations defined by\n",
      "        ``func(x) = 0`` given a starting estimate.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``f(x, *args)``\n",
      "            A function that takes at least one (possibly vector) argument.\n",
      "        x0 : ndarray\n",
      "            The starting estimate for the roots of ``func(x) = 0``.\n",
      "        args : tuple, optional\n",
      "            Any extra arguments to `func`.\n",
      "        fprime : callable(x), optional\n",
      "            A function to compute the Jacobian of `func` with derivatives\n",
      "            across the rows. By default, the Jacobian will be estimated.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        col_deriv : bool, optional\n",
      "            Specify whether the Jacobian function computes derivatives down\n",
      "            the columns (faster, because there is no transpose operation).\n",
      "        xtol : float, optional\n",
      "            The calculation will terminate if the relative error between two\n",
      "            consecutive iterates is at most `xtol`.\n",
      "        maxfev : int, optional\n",
      "            The maximum number of calls to the function. If zero, then\n",
      "            ``100*(N+1)`` is the maximum where N is the number of elements\n",
      "            in `x0`.\n",
      "        band : tuple, optional\n",
      "            If set to a two-sequence containing the number of sub- and\n",
      "            super-diagonals within the band of the Jacobi matrix, the\n",
      "            Jacobi matrix is considered banded (only for ``fprime=None``).\n",
      "        epsfcn : float, optional\n",
      "            A suitable step length for the forward-difference\n",
      "            approximation of the Jacobian (for ``fprime=None``). If\n",
      "            `epsfcn` is less than the machine precision, it is assumed\n",
      "            that the relative errors in the functions are of the order of\n",
      "            the machine precision.\n",
      "        factor : float, optional\n",
      "            A parameter determining the initial step bound\n",
      "            (``factor * || diag * x||``).  Should be in the interval\n",
      "            ``(0.1, 100)``.\n",
      "        diag : sequence, optional\n",
      "            N positive entries that serve as a scale factors for the\n",
      "            variables.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution (or the result of the last iteration for\n",
      "            an unsuccessful call).\n",
      "        infodict : dict\n",
      "            A dictionary of optional outputs with the keys:\n",
      "        \n",
      "            ``nfev``\n",
      "                number of function calls\n",
      "            ``njev``\n",
      "                number of Jacobian calls\n",
      "            ``fvec``\n",
      "                function evaluated at the output\n",
      "            ``fjac``\n",
      "                the orthogonal matrix, q, produced by the QR\n",
      "                factorization of the final approximate Jacobian\n",
      "                matrix, stored column wise\n",
      "            ``r``\n",
      "                upper triangular matrix produced by QR factorization\n",
      "                of the same matrix\n",
      "            ``qtf``\n",
      "                the vector ``(transpose(q) * fvec)``\n",
      "        \n",
      "        ier : int\n",
      "            An integer flag.  Set to 1 if a solution was found, otherwise refer\n",
      "            to `mesg` for more information.\n",
      "        mesg : str\n",
      "            If no solution is found, `mesg` details the cause of failure.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "        functions. See the 'hybr' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        ``fsolve`` is a wrapper around MINPACK's hybrd and hybrj algorithms.\n",
      "    \n",
      "    golden(func, args=(), brack=None, tol=1.4901161193847656e-08, full_output=0, maxiter=5000)\n",
      "        Return the minimum of a function of one variable.\n",
      "        \n",
      "        Given a function of one variable and a possible bracketing interval,\n",
      "        return the minimum of the function isolated to a fractional precision of\n",
      "        tol.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            Objective function to minimize.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to func.\n",
      "        brack : tuple, optional\n",
      "            Triple (a,b,c), where (a<b<c) and func(b) <\n",
      "            func(a),func(c).  If bracket consists of two numbers (a,\n",
      "            c), then they are assumed to be a starting interval for a\n",
      "            downhill bracket search (see `bracket`); it doesn't always\n",
      "            mean that obtained solution will satisfy a<=x<=c.\n",
      "        tol : float, optional\n",
      "            x tolerance stop criterion\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        maxiter : int\n",
      "            Maximum number of iterations to perform.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Golden' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses analog of bisection method to decrease the bracketed\n",
      "        interval.\n",
      "    \n",
      "    least_squares(fun, x0, jac='2-point', bounds=(-inf, inf), method='trf', ftol=1e-08, xtol=1e-08, gtol=1e-08, x_scale=1.0, loss='linear', f_scale=1.0, diff_step=None, tr_solver=None, tr_options={}, jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={})\n",
      "        Solve a nonlinear least-squares problem with bounds on the variables.\n",
      "        \n",
      "        Given the residuals f(x) (an m-dimensional real function of n real\n",
      "        variables) and the loss function rho(s) (a scalar function), `least_squares`\n",
      "        finds a local minimum of the cost function F(x)::\n",
      "        \n",
      "            minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\n",
      "            subject to lb <= x <= ub\n",
      "        \n",
      "        The purpose of the loss function rho(s) is to reduce the influence of\n",
      "        outliers on the solution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Function which computes the vector of residuals, with the signature\n",
      "            ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\n",
      "            respect to its first argument. The argument ``x`` passed to this\n",
      "            function is an ndarray of shape (n,) (never a scalar, even for n=1).\n",
      "            It must return a 1-d array_like of shape (m,) or a scalar. If the\n",
      "            argument ``x`` is complex or the function ``fun`` returns complex\n",
      "            residuals, it must be wrapped in a real function of real arguments,\n",
      "            as shown at the end of the Examples section.\n",
      "        x0 : array_like with shape (n,) or float\n",
      "            Initial guess on independent variables. If float, it will be treated\n",
      "            as a 1-d array with one element.\n",
      "        jac : {'2-point', '3-point', 'cs', callable}, optional\n",
      "            Method of computing the Jacobian matrix (an m-by-n matrix, where\n",
      "            element (i, j) is the partial derivative of f[i] with respect to\n",
      "            x[j]). The keywords select a finite difference scheme for numerical\n",
      "            estimation. The scheme '3-point' is more accurate, but requires\n",
      "            twice as much operations compared to '2-point' (default). The\n",
      "            scheme 'cs' uses complex steps, and while potentially the most\n",
      "            accurate, it is applicable only when `fun` correctly handles\n",
      "            complex inputs and can be analytically continued to the complex\n",
      "            plane. Method 'lm' always uses the '2-point' scheme. If callable,\n",
      "            it is used as ``jac(x, *args, **kwargs)`` and should return a\n",
      "            good approximation (or the exact value) for the Jacobian as an\n",
      "            array_like (np.atleast_2d is applied), a sparse matrix or a\n",
      "            `scipy.sparse.linalg.LinearOperator`.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on independent variables. Defaults to no bounds.\n",
      "            Each array must match the size of `x0` or be a scalar, in the latter\n",
      "            case a bound will be the same for all variables. Use ``np.inf`` with\n",
      "            an appropriate sign to disable bounds on all or some variables.\n",
      "        method : {'trf', 'dogbox', 'lm'}, optional\n",
      "            Algorithm to perform minimization.\n",
      "        \n",
      "                * 'trf' : Trust Region Reflective algorithm, particularly suitable\n",
      "                  for large sparse problems with bounds. Generally robust method.\n",
      "                * 'dogbox' : dogleg algorithm with rectangular trust regions,\n",
      "                  typical use case is small problems with bounds. Not recommended\n",
      "                  for problems with rank-deficient Jacobian.\n",
      "                * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\n",
      "                  Doesn't handle bounds and sparse Jacobians. Usually the most\n",
      "                  efficient method for small unconstrained problems.\n",
      "        \n",
      "            Default is 'trf'. See Notes for more information.\n",
      "        ftol : float, optional\n",
      "            Tolerance for termination by the change of the cost function. Default\n",
      "            is 1e-8. The optimization process is stopped when  ``dF < ftol * F``,\n",
      "            and there was an adequate agreement between a local quadratic model and\n",
      "            the true model in the last step.\n",
      "        xtol : float, optional\n",
      "            Tolerance for termination by the change of the independent variables.\n",
      "            Default is 1e-8. The exact condition depends on the `method` used:\n",
      "        \n",
      "                * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``\n",
      "                * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\n",
      "                  a trust-region radius and ``xs`` is the value of ``x``\n",
      "                  scaled according to `x_scale` parameter (see below).\n",
      "        \n",
      "        gtol : float, optional\n",
      "            Tolerance for termination by the norm of the gradient. Default is 1e-8.\n",
      "            The exact condition depends on a `method` used:\n",
      "        \n",
      "                * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\n",
      "                  ``g_scaled`` is the value of the gradient scaled to account for\n",
      "                  the presence of the bounds [STIR]_.\n",
      "                * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\n",
      "                  ``g_free`` is the gradient with respect to the variables which\n",
      "                  are not in the optimal state on the boundary.\n",
      "                * For 'lm' : the maximum absolute value of the cosine of angles\n",
      "                  between columns of the Jacobian and the residual vector is less\n",
      "                  than `gtol`, or the residual vector is zero.\n",
      "        \n",
      "        x_scale : array_like or 'jac', optional\n",
      "            Characteristic scale of each variable. Setting `x_scale` is equivalent\n",
      "            to reformulating the problem in scaled variables ``xs = x / x_scale``.\n",
      "            An alternative view is that the size of a trust region along j-th\n",
      "            dimension is proportional to ``x_scale[j]``. Improved convergence may\n",
      "            be achieved by setting `x_scale` such that a step of a given size\n",
      "            along any of the scaled variables has a similar effect on the cost\n",
      "            function. If set to 'jac', the scale is iteratively updated using the\n",
      "            inverse norms of the columns of the Jacobian matrix (as described in\n",
      "            [JJMore]_).\n",
      "        loss : str or callable, optional\n",
      "            Determines the loss function. The following keyword values are allowed:\n",
      "        \n",
      "                * 'linear' (default) : ``rho(z) = z``. Gives a standard\n",
      "                  least-squares problem.\n",
      "                * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\n",
      "                  approximation of l1 (absolute value) loss. Usually a good\n",
      "                  choice for robust least squares.\n",
      "                * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5 - 1``. Works\n",
      "                  similarly to 'soft_l1'.\n",
      "                * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\n",
      "                  influence, but may cause difficulties in optimization process.\n",
      "                * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\n",
      "                  a single residual, has properties similar to 'cauchy'.\n",
      "        \n",
      "            If callable, it must take a 1-d ndarray ``z=f**2`` and return an\n",
      "            array_like with shape (3, m) where row 0 contains function values,\n",
      "            row 1 contains first derivatives and row 2 contains second\n",
      "            derivatives. Method 'lm' supports only 'linear' loss.\n",
      "        f_scale : float, optional\n",
      "            Value of soft margin between inlier and outlier residuals, default\n",
      "            is 1.0. The loss function is evaluated as follows\n",
      "            ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\n",
      "            and ``rho`` is determined by `loss` parameter. This parameter has\n",
      "            no effect with ``loss='linear'``, but for other `loss` values it is\n",
      "            of crucial importance.\n",
      "        max_nfev : None or int, optional\n",
      "            Maximum number of function evaluations before the termination.\n",
      "            If None (default), the value is chosen automatically:\n",
      "        \n",
      "                * For 'trf' and 'dogbox' : 100 * n.\n",
      "                * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)\n",
      "                  otherwise (because 'lm' counts function calls in Jacobian\n",
      "                  estimation).\n",
      "        \n",
      "        diff_step : None or array_like, optional\n",
      "            Determines the relative step size for the finite difference\n",
      "            approximation of the Jacobian. The actual step is computed as\n",
      "            ``x * diff_step``. If None (default), then `diff_step` is taken to be\n",
      "            a conventional \"optimal\" power of machine epsilon for the finite\n",
      "            difference scheme used [NR]_.\n",
      "        tr_solver : {None, 'exact', 'lsmr'}, optional\n",
      "            Method for solving trust-region subproblems, relevant only for 'trf'\n",
      "            and 'dogbox' methods.\n",
      "        \n",
      "                * 'exact' is suitable for not very large problems with dense\n",
      "                  Jacobian matrices. The computational complexity per iteration is\n",
      "                  comparable to a singular value decomposition of the Jacobian\n",
      "                  matrix.\n",
      "                * 'lsmr' is suitable for problems with sparse and large Jacobian\n",
      "                  matrices. It uses the iterative procedure\n",
      "                  `scipy.sparse.linalg.lsmr` for finding a solution of a linear\n",
      "                  least-squares problem and only requires matrix-vector product\n",
      "                  evaluations.\n",
      "        \n",
      "            If None (default) the solver is chosen based on the type of Jacobian\n",
      "            returned on the first iteration.\n",
      "        tr_options : dict, optional\n",
      "            Keyword options passed to trust-region solver.\n",
      "        \n",
      "                * ``tr_solver='exact'``: `tr_options` are ignored.\n",
      "                * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\n",
      "                  Additionally  ``method='trf'`` supports  'regularize' option\n",
      "                  (bool, default is True) which adds a regularization term to the\n",
      "                  normal equation, which improves convergence if the Jacobian is\n",
      "                  rank-deficient [Byrd]_ (eq. 3.4).\n",
      "        \n",
      "        jac_sparsity : {None, array_like, sparse matrix}, optional\n",
      "            Defines the sparsity structure of the Jacobian matrix for finite\n",
      "            difference estimation, its shape must be (m, n). If the Jacobian has\n",
      "            only few non-zero elements in *each* row, providing the sparsity\n",
      "            structure will greatly speed up the computations [Curtis]_. A zero\n",
      "            entry means that a corresponding element in the Jacobian is identically\n",
      "            zero. If provided, forces the use of 'lsmr' trust-region solver.\n",
      "            If None (default) then dense differencing will be used. Has no effect\n",
      "            for 'lm' method.\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "        \n",
      "                * 0 (default) : work silently.\n",
      "                * 1 : display a termination report.\n",
      "                * 2 : display progress during iterations (not supported by 'lm'\n",
      "                  method).\n",
      "        \n",
      "        args, kwargs : tuple and dict, optional\n",
      "            Additional arguments passed to `fun` and `jac`. Both empty by default.\n",
      "            The calling signature is ``fun(x, *args, **kwargs)`` and the same for\n",
      "            `jac`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `OptimizeResult` with the following fields defined:\n",
      "        x : ndarray, shape (n,)\n",
      "            Solution found.\n",
      "        cost : float\n",
      "            Value of the cost function at the solution.\n",
      "        fun : ndarray, shape (m,)\n",
      "            Vector of residuals at the solution.\n",
      "        jac : ndarray, sparse matrix or LinearOperator, shape (m, n)\n",
      "            Modified Jacobian matrix at the solution, in the sense that J^T J\n",
      "            is a Gauss-Newton approximation of the Hessian of the cost function.\n",
      "            The type is the same as the one used by the algorithm.\n",
      "        grad : ndarray, shape (m,)\n",
      "            Gradient of the cost function at the solution.\n",
      "        optimality : float\n",
      "            First-order optimality measure. In unconstrained problems, it is always\n",
      "            the uniform norm of the gradient. In constrained problems, it is the\n",
      "            quantity which was compared with `gtol` during iterations.\n",
      "        active_mask : ndarray of int, shape (n,)\n",
      "            Each component shows whether a corresponding constraint is active\n",
      "            (that is, whether a variable is at the bound):\n",
      "        \n",
      "                *  0 : a constraint is not active.\n",
      "                * -1 : a lower bound is active.\n",
      "                *  1 : an upper bound is active.\n",
      "        \n",
      "            Might be somewhat arbitrary for 'trf' method as it generates a sequence\n",
      "            of strictly feasible iterates and `active_mask` is determined within a\n",
      "            tolerance threshold.\n",
      "        nfev : int\n",
      "            Number of function evaluations done. Methods 'trf' and 'dogbox' do not\n",
      "            count function calls for numerical Jacobian approximation, as opposed\n",
      "            to 'lm' method.\n",
      "        njev : int or None\n",
      "            Number of Jacobian evaluations done. If numerical Jacobian\n",
      "            approximation is used in 'lm' method, it is set to None.\n",
      "        status : int\n",
      "            The reason for algorithm termination:\n",
      "        \n",
      "                * -1 : improper input parameters status returned from MINPACK.\n",
      "                *  0 : the maximum number of function evaluations is exceeded.\n",
      "                *  1 : `gtol` termination condition is satisfied.\n",
      "                *  2 : `ftol` termination condition is satisfied.\n",
      "                *  3 : `xtol` termination condition is satisfied.\n",
      "                *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\n",
      "        \n",
      "        message : str\n",
      "            Verbal description of the termination reason.\n",
      "        success : bool\n",
      "            True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        leastsq : A legacy wrapper for the MINPACK implementation of the\n",
      "                  Levenberg-Marquadt algorithm.\n",
      "        curve_fit : Least-squares minimization applied to a curve fitting problem.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares\n",
      "        algorithms implemented in MINPACK (lmder, lmdif). It runs the\n",
      "        Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\n",
      "        The implementation is based on paper [JJMore]_, it is very robust and\n",
      "        efficient with a lot of smart tricks. It should be your first choice\n",
      "        for unconstrained problems. Note that it doesn't support bounds. Also\n",
      "        it doesn't work when m < n.\n",
      "        \n",
      "        Method 'trf' (Trust Region Reflective) is motivated by the process of\n",
      "        solving a system of equations, which constitute the first-order optimality\n",
      "        condition for a bound-constrained minimization problem as formulated in\n",
      "        [STIR]_. The algorithm iteratively solves trust-region subproblems\n",
      "        augmented by a special diagonal quadratic term and with trust-region shape\n",
      "        determined by the distance from the bounds and the direction of the\n",
      "        gradient. This enhancements help to avoid making steps directly into bounds\n",
      "        and efficiently explore the whole space of variables. To further improve\n",
      "        convergence, the algorithm considers search directions reflected from the\n",
      "        bounds. To obey theoretical requirements, the algorithm keeps iterates\n",
      "        strictly feasible. With dense Jacobians trust-region subproblems are\n",
      "        solved by an exact method very similar to the one described in [JJMore]_\n",
      "        (and implemented in MINPACK). The difference from the MINPACK\n",
      "        implementation is that a singular value decomposition of a Jacobian\n",
      "        matrix is done once per iteration, instead of a QR decomposition and series\n",
      "        of Givens rotation eliminations. For large sparse Jacobians a 2-d subspace\n",
      "        approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\n",
      "        The subspace is spanned by a scaled gradient and an approximate\n",
      "        Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\n",
      "        constraints are imposed the algorithm is very similar to MINPACK and has\n",
      "        generally comparable performance. The algorithm works quite robust in\n",
      "        unbounded and bounded problems, thus it is chosen as a default algorithm.\n",
      "        \n",
      "        Method 'dogbox' operates in a trust-region framework, but considers\n",
      "        rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\n",
      "        The intersection of a current trust region and initial bounds is again\n",
      "        rectangular, so on each iteration a quadratic minimization problem subject\n",
      "        to bound constraints is solved approximately by Powell's dogleg method\n",
      "        [NumOpt]_. The required Gauss-Newton step can be computed exactly for\n",
      "        dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\n",
      "        sparse Jacobians. The algorithm is likely to exhibit slow convergence when\n",
      "        the rank of Jacobian is less than the number of variables. The algorithm\n",
      "        often outperforms 'trf' in bounded problems with a small number of\n",
      "        variables.\n",
      "        \n",
      "        Robust loss functions are implemented as described in [BA]_. The idea\n",
      "        is to modify a residual vector and a Jacobian matrix on each iteration\n",
      "        such that computed gradient and Gauss-Newton Hessian approximation match\n",
      "        the true gradient and Hessian approximation of the cost function. Then\n",
      "        the algorithm proceeds in a normal way, i.e. robust loss functions are\n",
      "        implemented as a simple wrapper over standard least-squares algorithms.\n",
      "        \n",
      "        .. versionadded:: 0.17.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "                  and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "                  Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "                  Vol. 21, Number 1, pp 1-23, 1999.\n",
      "        .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\n",
      "                Computing. 3rd edition\", Sec. 5.7.\n",
      "        .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\n",
      "                  solution of the trust region problem by minimization over\n",
      "                  two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\n",
      "                  1988.\n",
      "        .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\n",
      "                    sparse Jacobian matrices\", Journal of the Institute of\n",
      "                    Mathematics and its Applications, 13, pp. 117-120, 1974.\n",
      "        .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\n",
      "                    and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\n",
      "                    Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\n",
      "        .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\n",
      "                    Dogleg Approach for Unconstrained and Bound Constrained\n",
      "                    Nonlinear Optimization\", WSEAS International Conference on\n",
      "                    Applied Mathematics, Corfu, Greece, 2004.\n",
      "        .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\n",
      "                    2nd edition\", Chapter 4.\n",
      "        .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\n",
      "                Proceedings of the International Workshop on Vision Algorithms:\n",
      "                Theory and Practice, pp. 298-372, 1999.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In this example we find a minimum of the Rosenbrock function without bounds\n",
      "        on independed variables.\n",
      "        \n",
      "        >>> def fun_rosenbrock(x):\n",
      "        ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\n",
      "        \n",
      "        Notice that we only provide the vector of the residuals. The algorithm\n",
      "        constructs the cost function as a sum of squares of the residuals, which\n",
      "        gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\n",
      "        \n",
      "        >>> from scipy.optimize import least_squares\n",
      "        >>> x0_rosenbrock = np.array([2, 2])\n",
      "        >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\n",
      "        >>> res_1.x\n",
      "        array([ 1.,  1.])\n",
      "        >>> res_1.cost\n",
      "        9.8669242910846867e-30\n",
      "        >>> res_1.optimality\n",
      "        8.8928864934219529e-14\n",
      "        \n",
      "        We now constrain the variables, in such a way that the previous solution\n",
      "        becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\n",
      "        ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\n",
      "        to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\n",
      "        \n",
      "        We also provide the analytic Jacobian:\n",
      "        \n",
      "        >>> def jac_rosenbrock(x):\n",
      "        ...     return np.array([\n",
      "        ...         [-20 * x[0], 10],\n",
      "        ...         [-1, 0]])\n",
      "        \n",
      "        Putting this all together, we see that the new solution lies on the bound:\n",
      "        \n",
      "        >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\n",
      "        ...                       bounds=([-np.inf, 1.5], np.inf))\n",
      "        >>> res_2.x\n",
      "        array([ 1.22437075,  1.5       ])\n",
      "        >>> res_2.cost\n",
      "        0.025213093946805685\n",
      "        >>> res_2.optimality\n",
      "        1.5885401433157753e-07\n",
      "        \n",
      "        Now we solve a system of equations (i.e., the cost function should be zero\n",
      "        at a minimum) for a Broyden tridiagonal vector-valued function of 100000\n",
      "        variables:\n",
      "        \n",
      "        >>> def fun_broyden(x):\n",
      "        ...     f = (3 - x) * x + 1\n",
      "        ...     f[1:] -= x[:-1]\n",
      "        ...     f[:-1] -= 2 * x[1:]\n",
      "        ...     return f\n",
      "        \n",
      "        The corresponding Jacobian matrix is sparse. We tell the algorithm to\n",
      "        estimate it by finite differences and provide the sparsity structure of\n",
      "        Jacobian to significantly speed up this process.\n",
      "        \n",
      "        >>> from scipy.sparse import lil_matrix\n",
      "        >>> def sparsity_broyden(n):\n",
      "        ...     sparsity = lil_matrix((n, n), dtype=int)\n",
      "        ...     i = np.arange(n)\n",
      "        ...     sparsity[i, i] = 1\n",
      "        ...     i = np.arange(1, n)\n",
      "        ...     sparsity[i, i - 1] = 1\n",
      "        ...     i = np.arange(n - 1)\n",
      "        ...     sparsity[i, i + 1] = 1\n",
      "        ...     return sparsity\n",
      "        ...\n",
      "        >>> n = 100000\n",
      "        >>> x0_broyden = -np.ones(n)\n",
      "        ...\n",
      "        >>> res_3 = least_squares(fun_broyden, x0_broyden,\n",
      "        ...                       jac_sparsity=sparsity_broyden(n))\n",
      "        >>> res_3.cost\n",
      "        4.5687069299604613e-23\n",
      "        >>> res_3.optimality\n",
      "        1.1650454296851518e-11\n",
      "        \n",
      "        Let's also solve a curve fitting problem using robust loss function to\n",
      "        take care of outliers in the data. Define the model function as\n",
      "        ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\n",
      "        observation and a, b, c are parameters to estimate.\n",
      "        \n",
      "        First, define the function which generates the data with noise and\n",
      "        outliers, define the model parameters, and generate data:\n",
      "        \n",
      "        >>> def gen_data(t, a, b, c, noise=0, n_outliers=0, random_state=0):\n",
      "        ...     y = a + b * np.exp(t * c)\n",
      "        ...\n",
      "        ...     rnd = np.random.RandomState(random_state)\n",
      "        ...     error = noise * rnd.randn(t.size)\n",
      "        ...     outliers = rnd.randint(0, t.size, n_outliers)\n",
      "        ...     error[outliers] *= 10\n",
      "        ...\n",
      "        ...     return y + error\n",
      "        ...\n",
      "        >>> a = 0.5\n",
      "        >>> b = 2.0\n",
      "        >>> c = -1\n",
      "        >>> t_min = 0\n",
      "        >>> t_max = 10\n",
      "        >>> n_points = 15\n",
      "        ...\n",
      "        >>> t_train = np.linspace(t_min, t_max, n_points)\n",
      "        >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\n",
      "        \n",
      "        Define function for computing residuals and initial estimate of\n",
      "        parameters.\n",
      "        \n",
      "        >>> def fun(x, t, y):\n",
      "        ...     return x[0] + x[1] * np.exp(x[2] * t) - y\n",
      "        ...\n",
      "        >>> x0 = np.array([1.0, 1.0, 0.0])\n",
      "        \n",
      "        Compute a standard least-squares solution:\n",
      "        \n",
      "        >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\n",
      "        \n",
      "        Now compute two solutions with two different robust loss functions. The\n",
      "        parameter `f_scale` is set to 0.1, meaning that inlier residuals should\n",
      "        not significantly exceed 0.1 (the noise level used).\n",
      "        \n",
      "        >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\n",
      "        ...                             args=(t_train, y_train))\n",
      "        >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\n",
      "        ...                         args=(t_train, y_train))\n",
      "        \n",
      "        And finally plot all the curves. We see that by selecting an appropriate\n",
      "        `loss`  we can get estimates close to optimal even in the presence of\n",
      "        strong outliers. But keep in mind that generally it is recommended to try\n",
      "        'soft_l1' or 'huber' losses first (if at all necessary) as the other two\n",
      "        options may cause difficulties in optimization process.\n",
      "        \n",
      "        >>> t_test = np.linspace(t_min, t_max, n_points * 10)\n",
      "        >>> y_true = gen_data(t_test, a, b, c)\n",
      "        >>> y_lsq = gen_data(t_test, *res_lsq.x)\n",
      "        >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
      "        >>> y_log = gen_data(t_test, *res_log.x)\n",
      "        ...\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> plt.plot(t_train, y_train, 'o')\n",
      "        >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\n",
      "        >>> plt.plot(t_test, y_lsq, label='linear loss')\n",
      "        >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\n",
      "        >>> plt.plot(t_test, y_log, label='cauchy loss')\n",
      "        >>> plt.xlabel(\"t\")\n",
      "        >>> plt.ylabel(\"y\")\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "        \n",
      "        In the next example, we show how complex-valued residual functions of\n",
      "        complex variables can be optimized with ``least_squares()``. Consider the\n",
      "        following function:\n",
      "        \n",
      "        >>> def f(z):\n",
      "        ...     return z - (0.5 + 0.5j)\n",
      "        \n",
      "        We wrap it into a function of real variables that returns real residuals\n",
      "        by simply handling the real and imaginary parts as independent variables:\n",
      "        \n",
      "        >>> def f_wrap(x):\n",
      "        ...     fx = f(x[0] + 1j*x[1])\n",
      "        ...     return np.array([fx.real, fx.imag])\n",
      "        \n",
      "        Thus, instead of the original m-dimensional complex function of n complex\n",
      "        variables we optimize a 2m-dimensional real function of 2n real variables:\n",
      "        \n",
      "        >>> from scipy.optimize import least_squares\n",
      "        >>> res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))\n",
      "        >>> z = res_wrapped.x[0] + res_wrapped.x[1]*1j\n",
      "        >>> z\n",
      "        (0.49999999999925893+0.49999999999925893j)\n",
      "    \n",
      "    leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n",
      "        Minimize the sum of squares of a set of equations.\n",
      "        \n",
      "        ::\n",
      "        \n",
      "            x = arg min(sum(func(y)**2,axis=0))\n",
      "                     y\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            should take at least one (possibly length N vector) argument and\n",
      "            returns M floating point numbers. It must not return NaNs or\n",
      "            fitting might fail.\n",
      "        x0 : ndarray\n",
      "            The starting estimate for the minimization.\n",
      "        args : tuple, optional\n",
      "            Any extra arguments to func are placed in this tuple.\n",
      "        Dfun : callable, optional\n",
      "            A function or method to compute the Jacobian of func with derivatives\n",
      "            across the rows. If this is None, the Jacobian will be estimated.\n",
      "        full_output : bool, optional\n",
      "            non-zero to return all optional outputs.\n",
      "        col_deriv : bool, optional\n",
      "            non-zero to specify that the Jacobian function computes derivatives\n",
      "            down the columns (faster, because there is no transpose operation).\n",
      "        ftol : float, optional\n",
      "            Relative error desired in the sum of squares.\n",
      "        xtol : float, optional\n",
      "            Relative error desired in the approximate solution.\n",
      "        gtol : float, optional\n",
      "            Orthogonality desired between the function vector and the columns of\n",
      "            the Jacobian.\n",
      "        maxfev : int, optional\n",
      "            The maximum number of calls to the function. If `Dfun` is provided\n",
      "            then the default `maxfev` is 100*(N+1) where N is the number of elements\n",
      "            in x0, otherwise the default `maxfev` is 200*(N+1).\n",
      "        epsfcn : float, optional\n",
      "            A variable used in determining a suitable step length for the forward-\n",
      "            difference approximation of the Jacobian (for Dfun=None).\n",
      "            Normally the actual step length will be sqrt(epsfcn)*x\n",
      "            If epsfcn is less than the machine precision, it is assumed that the\n",
      "            relative errors are of the order of the machine precision.\n",
      "        factor : float, optional\n",
      "            A parameter determining the initial step bound\n",
      "            (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
      "        diag : sequence, optional\n",
      "            N positive entries that serve as a scale factors for the variables.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution (or the result of the last iteration for an unsuccessful\n",
      "            call).\n",
      "        cov_x : ndarray\n",
      "            Uses the fjac and ipvt optional outputs to construct an\n",
      "            estimate of the jacobian around the solution. None if a\n",
      "            singular matrix encountered (indicates very flat curvature in\n",
      "            some direction).  This matrix must be multiplied by the\n",
      "            residual variance to get the covariance of the\n",
      "            parameter estimates -- see curve_fit.\n",
      "        infodict : dict\n",
      "            a dictionary of optional outputs with the key s:\n",
      "        \n",
      "            ``nfev``\n",
      "                The number of function calls\n",
      "            ``fvec``\n",
      "                The function evaluated at the output\n",
      "            ``fjac``\n",
      "                A permutation of the R matrix of a QR\n",
      "                factorization of the final approximate\n",
      "                Jacobian matrix, stored column wise.\n",
      "                Together with ipvt, the covariance of the\n",
      "                estimate can be approximated.\n",
      "            ``ipvt``\n",
      "                An integer array of length N which defines\n",
      "                a permutation matrix, p, such that\n",
      "                fjac*p = q*r, where r is upper triangular\n",
      "                with diagonal elements of nonincreasing\n",
      "                magnitude. Column j of p is column ipvt(j)\n",
      "                of the identity matrix.\n",
      "            ``qtf``\n",
      "                The vector (transpose(q) * fvec).\n",
      "        \n",
      "        mesg : str\n",
      "            A string message giving information about the cause of failure.\n",
      "        ier : int\n",
      "            An integer flag.  If it is equal to 1, 2, 3 or 4, the solution was\n",
      "            found.  Otherwise, the solution was not found. In either case, the\n",
      "            optional output variable 'mesg' gives more information.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \"leastsq\" is a wrapper around MINPACK's lmdif and lmder algorithms.\n",
      "        \n",
      "        cov_x is a Jacobian approximation to the Hessian of the least squares\n",
      "        objective function.\n",
      "        This approximation assumes that the objective function is based on the\n",
      "        difference between some observed target data (ydata) and a (non-linear)\n",
      "        function of the parameters `f(xdata, params)` ::\n",
      "        \n",
      "               func(params) = ydata - f(xdata, params)\n",
      "        \n",
      "        so that the objective function is ::\n",
      "        \n",
      "               min   sum((ydata - f(xdata, params))**2, axis=0)\n",
      "             params\n",
      "    \n",
      "    line_search = line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=50)\n",
      "        Find alpha that satisfies strong Wolfe conditions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable f(x,*args)\n",
      "            Objective function.\n",
      "        myfprime : callable f'(x,*args)\n",
      "            Objective function gradient.\n",
      "        xk : ndarray\n",
      "            Starting point.\n",
      "        pk : ndarray\n",
      "            Search direction.\n",
      "        gfk : ndarray, optional\n",
      "            Gradient value for x=xk (xk being the current parameter\n",
      "            estimate). Will be recomputed if omitted.\n",
      "        old_fval : float, optional\n",
      "            Function value for x=xk. Will be recomputed if omitted.\n",
      "        old_old_fval : float, optional\n",
      "            Function value for the point preceding x=xk\n",
      "        args : tuple, optional\n",
      "            Additional arguments passed to objective function.\n",
      "        c1 : float, optional\n",
      "            Parameter for Armijo condition rule.\n",
      "        c2 : float, optional\n",
      "            Parameter for curvature condition rule.\n",
      "        amax : float, optional\n",
      "            Maximum step size\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alpha : float or None\n",
      "            Alpha for which ``x_new = x0 + alpha * pk``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        fc : int\n",
      "            Number of function evaluations made.\n",
      "        gc : int\n",
      "            Number of gradient evaluations made.\n",
      "        new_fval : float or None\n",
      "            New function value ``f(x_new)=f(x0+alpha*pk)``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        old_fval : float\n",
      "            Old function value ``f(x0)``.\n",
      "        new_slope : float or None\n",
      "            The local slope along the search direction at the\n",
      "            new value ``<myfprime(x_new), pk>``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses the line search algorithm to enforce strong Wolfe\n",
      "        conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
      "        1999, pg. 59-60.\n",
      "        \n",
      "        For the zoom phase it uses an algorithm by [...].\n",
      "    \n",
      "    linear_sum_assignment(cost_matrix)\n",
      "        Solve the linear sum assignment problem.\n",
      "        \n",
      "        The linear sum assignment problem is also known as minimum weight matching\n",
      "        in bipartite graphs. A problem instance is described by a matrix C, where\n",
      "        each C[i,j] is the cost of matching vertex i of the first partite set\n",
      "        (a \"worker\") and vertex j of the second set (a \"job\"). The goal is to find\n",
      "        a complete assignment of workers to jobs of minimal cost.\n",
      "        \n",
      "        Formally, let X be a boolean matrix where :math:`X[i,j] = 1` iff row i is\n",
      "        assigned to column j. Then the optimal assignment has cost\n",
      "        \n",
      "        .. math::\n",
      "            \\min \\sum_i \\sum_j C_{i,j} X_{i,j}\n",
      "        \n",
      "        s.t. each row is assignment to at most one column, and each column to at\n",
      "        most one row.\n",
      "        \n",
      "        This function can also solve a generalization of the classic assignment\n",
      "        problem where the cost matrix is rectangular. If it has more rows than\n",
      "        columns, then not every row needs to be assigned to a column, and vice\n",
      "        versa.\n",
      "        \n",
      "        The method used is the Hungarian algorithm, also known as the Munkres or\n",
      "        Kuhn-Munkres algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cost_matrix : array\n",
      "            The cost matrix of the bipartite graph.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        row_ind, col_ind : array\n",
      "            An array of row indices and one of corresponding column indices giving\n",
      "            the optimal assignment. The cost of the assignment can be computed\n",
      "            as ``cost_matrix[row_ind, col_ind].sum()``. The row indices will be\n",
      "            sorted; in the case of a square cost matrix they will be equal to\n",
      "            ``numpy.arange(cost_matrix.shape[0])``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.17.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> cost = np.array([[4, 1, 3], [2, 0, 5], [3, 2, 2]])\n",
      "        >>> from scipy.optimize import linear_sum_assignment\n",
      "        >>> row_ind, col_ind = linear_sum_assignment(cost)\n",
      "        >>> col_ind\n",
      "        array([1, 0, 2])\n",
      "        >>> cost[row_ind, col_ind].sum()\n",
      "        5\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        1. http://csclab.murraystate.edu/bob.pilgrim/445/munkres.html\n",
      "        \n",
      "        2. Harold W. Kuhn. The Hungarian Method for the assignment problem.\n",
      "           *Naval Research Logistics Quarterly*, 2:83-97, 1955.\n",
      "        \n",
      "        3. Harold W. Kuhn. Variants of the Hungarian method for assignment\n",
      "           problems. *Naval Research Logistics Quarterly*, 3: 253-258, 1956.\n",
      "        \n",
      "        4. Munkres, J. Algorithms for the Assignment and Transportation Problems.\n",
      "           *J. SIAM*, 5(1):32-38, March, 1957.\n",
      "        \n",
      "        5. https://en.wikipedia.org/wiki/Hungarian_algorithm\n",
      "    \n",
      "    linearmixing(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using a scalar Jacobian approximation.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        x0 : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            The Jacobian approximation is (-1/alpha).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "    \n",
      "    linprog(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None, method='simplex', callback=None, options=None)\n",
      "        Minimize a linear objective function subject to linear\n",
      "        equality and inequality constraints.\n",
      "        \n",
      "        Linear Programming is intended to solve the following problem form:\n",
      "        \n",
      "        Minimize:     c^T * x\n",
      "        \n",
      "        Subject to:   A_ub * x <= b_ub\n",
      "                      A_eq * x == b_eq\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        c : array_like\n",
      "            Coefficients of the linear objective function to be minimized.\n",
      "        A_ub : array_like, optional\n",
      "            2-D array which, when matrix-multiplied by x, gives the values of the\n",
      "            upper-bound inequality constraints at x.\n",
      "        b_ub : array_like, optional\n",
      "            1-D array of values representing the upper-bound of each inequality\n",
      "            constraint (row) in A_ub.\n",
      "        A_eq : array_like, optional\n",
      "            2-D array which, when matrix-multiplied by x, gives the values of the\n",
      "            equality constraints at x.\n",
      "        b_eq : array_like, optional\n",
      "            1-D array of values representing the RHS of each equality constraint\n",
      "            (row) in A_eq.\n",
      "        bounds : sequence, optional\n",
      "            ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the bounds on that parameter. Use None for one of ``min`` or\n",
      "            ``max`` when there is no bound in that direction. By default\n",
      "            bounds are ``(0, None)`` (non-negative)\n",
      "            If a sequence containing a single tuple is provided, then ``min`` and\n",
      "            ``max`` will be applied to all variables in the problem.\n",
      "        method : str, optional\n",
      "            Type of solver.  At this time only 'simplex' is supported\n",
      "            :ref:`(see here) <optimize.linprog-simplex>`.\n",
      "        callback : callable, optional\n",
      "            If a callback function is provide, it will be called within each\n",
      "            iteration of the simplex algorithm. The callback must have the signature\n",
      "            `callback(xk, **kwargs)` where xk is the current solution vector\n",
      "            and kwargs is a dictionary containing the following::\n",
      "        \n",
      "                \"tableau\" : The current Simplex algorithm tableau\n",
      "                \"nit\" : The current iteration.\n",
      "                \"pivot\" : The pivot (row, column) used for the next iteration.\n",
      "                \"phase\" : Whether the algorithm is in Phase 1 or Phase 2.\n",
      "                \"basis\" : The indices of the columns of the basic variables.\n",
      "        \n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All methods accept the following\n",
      "            generic options:\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform.\n",
      "                disp : bool\n",
      "                    Set to True to print convergence messages.\n",
      "        \n",
      "            For method-specific options, see `show_options('linprog')`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "        \n",
      "            x : ndarray\n",
      "                The independent variable vector which optimizes the linear\n",
      "                programming problem.\n",
      "            fun : float\n",
      "                Value of the objective function.\n",
      "            slack : ndarray\n",
      "                The values of the slack variables.  Each slack variable corresponds\n",
      "                to an inequality constraint.  If the slack is zero, then the\n",
      "                corresponding constraint is active.\n",
      "            success : bool\n",
      "                Returns True if the algorithm succeeded in finding an optimal\n",
      "                solution.\n",
      "            status : int\n",
      "                An integer representing the exit status of the optimization::\n",
      "        \n",
      "                     0 : Optimization terminated successfully\n",
      "                     1 : Iteration limit reached\n",
      "                     2 : Problem appears to be infeasible\n",
      "                     3 : Problem appears to be unbounded\n",
      "        \n",
      "            nit : int\n",
      "                The number of iterations performed.\n",
      "            message : str\n",
      "                A string descriptor of the exit status of the optimization.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is :ref:`Simplex <optimize.linprog-simplex>`.\n",
      "        \n",
      "        Method *Simplex* uses the Simplex algorithm (as it relates to Linear\n",
      "        Programming, NOT the Nelder-Mead Simplex) [1]_, [2]_. This algorithm\n",
      "        should be reasonably reliable and fast.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Dantzig, George B., Linear programming and extensions. Rand\n",
      "               Corporation Research Study Princeton Univ. Press, Princeton, NJ, 1963\n",
      "        .. [2] Hillier, S.H. and Lieberman, G.J. (1995), \"Introduction to\n",
      "               Mathematical Programming\", McGraw-Hill, Chapter 4.\n",
      "        .. [3] Bland, Robert G. New finite pivoting rules for the simplex method.\n",
      "               Mathematics of Operations Research (2), 1977: pp. 103-107.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Consider the following problem:\n",
      "        \n",
      "        Minimize: f = -1*x[0] + 4*x[1]\n",
      "        \n",
      "        Subject to: -3*x[0] + 1*x[1] <= 6\n",
      "                     1*x[0] + 2*x[1] <= 4\n",
      "                                x[1] >= -3\n",
      "        \n",
      "        where:  -inf <= x[0] <= inf\n",
      "        \n",
      "        This problem deviates from the standard linear programming problem.\n",
      "        In standard form, linear programming problems assume the variables x are\n",
      "        non-negative.  Since the variables don't have standard bounds where\n",
      "        0 <= x <= inf, the bounds of the variables must be explicitly set.\n",
      "        \n",
      "        There are two upper-bound constraints, which can be expressed as\n",
      "        \n",
      "        dot(A_ub, x) <= b_ub\n",
      "        \n",
      "        The input for this problem is as follows:\n",
      "        \n",
      "        >>> c = [-1, 4]\n",
      "        >>> A = [[-3, 1], [1, 2]]\n",
      "        >>> b = [6, 4]\n",
      "        >>> x0_bounds = (None, None)\n",
      "        >>> x1_bounds = (-3, None)\n",
      "        >>> from scipy.optimize import linprog\n",
      "        >>> res = linprog(c, A_ub=A, b_ub=b, bounds=(x0_bounds, x1_bounds),\n",
      "        ...               options={\"disp\": True})\n",
      "        Optimization terminated successfully.\n",
      "             Current function value: -22.000000\n",
      "             Iterations: 1\n",
      "        >>> print(res)\n",
      "             fun: -22.0\n",
      "         message: 'Optimization terminated successfully.'\n",
      "             nit: 1\n",
      "           slack: array([ 39.,   0.])\n",
      "          status: 0\n",
      "         success: True\n",
      "               x: array([ 10.,  -3.])\n",
      "        \n",
      "        Note the actual objective value is 11.428571.  In this case we minimized\n",
      "        the negative of the objective function.\n",
      "    \n",
      "    linprog_verbose_callback(xk, **kwargs)\n",
      "        A sample callback function demonstrating the linprog callback interface.\n",
      "        This callback produces detailed output to sys.stdout before each iteration\n",
      "        and after the final iteration of the simplex algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        xk : array_like\n",
      "            The current solution vector.\n",
      "        **kwargs : dict\n",
      "            A dictionary containing the following parameters:\n",
      "        \n",
      "            tableau : array_like\n",
      "                The current tableau of the simplex algorithm.\n",
      "                Its structure is defined in _solve_simplex.\n",
      "            phase : int\n",
      "                The current Phase of the simplex algorithm (1 or 2)\n",
      "            nit : int\n",
      "                The current iteration number.\n",
      "            pivot : tuple(int, int)\n",
      "                The index of the tableau selected as the next pivot,\n",
      "                or nan if no pivot exists\n",
      "            basis : array(int)\n",
      "                A list of the current basic variables.\n",
      "                Each element contains the name of a basic variable and its value.\n",
      "            complete : bool\n",
      "                True if the simplex algorithm has completed\n",
      "                (and this is the final call to callback), otherwise False.\n",
      "    \n",
      "    lsq_linear(A, b, bounds=(-inf, inf), method='trf', tol=1e-10, lsq_solver=None, lsmr_tol=None, max_iter=None, verbose=0)\n",
      "        Solve a linear least-squares problem with bounds on the variables.\n",
      "        \n",
      "        Given a m-by-n design matrix A and a target vector b with m elements,\n",
      "        `lsq_linear` solves the following optimization problem::\n",
      "        \n",
      "            minimize 0.5 * ||A x - b||**2\n",
      "            subject to lb <= x <= ub\n",
      "        \n",
      "        This optimization problem is convex, hence a found minimum (if iterations\n",
      "        have converged) is guaranteed to be global.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : array_like, sparse matrix of LinearOperator, shape (m, n)\n",
      "            Design matrix. Can be `scipy.sparse.linalg.LinearOperator`.\n",
      "        b : array_like, shape (m,)\n",
      "            Target vector.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on independent variables. Defaults to no bounds.\n",
      "            Each array must have shape (n,) or be a scalar, in the latter\n",
      "            case a bound will be the same for all variables. Use ``np.inf`` with\n",
      "            an appropriate sign to disable bounds on all or some variables.\n",
      "        method : 'trf' or 'bvls', optional\n",
      "            Method to perform minimization.\n",
      "        \n",
      "                * 'trf' : Trust Region Reflective algorithm adapted for a linear\n",
      "                  least-squares problem. This is an interior-point-like method\n",
      "                  and the required number of iterations is weakly correlated with\n",
      "                  the number of variables.\n",
      "                * 'bvls' : Bounded-Variable Least-Squares algorithm. This is\n",
      "                  an active set method, which requires the number of iterations\n",
      "                  comparable to the number of variables. Can't be used when `A` is\n",
      "                  sparse or LinearOperator.\n",
      "        \n",
      "            Default is 'trf'.\n",
      "        tol : float, optional\n",
      "            Tolerance parameter. The algorithm terminates if a relative change\n",
      "            of the cost function is less than `tol` on the last iteration.\n",
      "            Additionally the first-order optimality measure is considered:\n",
      "        \n",
      "                * ``method='trf'`` terminates if the uniform norm of the gradient,\n",
      "                  scaled to account for the presence of the bounds, is less than\n",
      "                  `tol`.\n",
      "                * ``method='bvls'`` terminates if Karush-Kuhn-Tucker conditions\n",
      "                  are satisfied within `tol` tolerance.\n",
      "        \n",
      "        lsq_solver : {None, 'exact', 'lsmr'}, optional\n",
      "            Method of solving unbounded least-squares problems throughout\n",
      "            iterations:\n",
      "        \n",
      "                * 'exact' : Use dense QR or SVD decomposition approach. Can't be\n",
      "                  used when `A` is sparse or LinearOperator.\n",
      "                * 'lsmr' : Use `scipy.sparse.linalg.lsmr` iterative procedure\n",
      "                  which requires only matrix-vector product evaluations. Can't\n",
      "                  be used with ``method='bvls'``.\n",
      "        \n",
      "            If None (default) the solver is chosen based on type of `A`.\n",
      "        lsmr_tol : None, float or 'auto', optional\n",
      "            Tolerance parameters 'atol' and 'btol' for `scipy.sparse.linalg.lsmr`\n",
      "            If None (default), it is set to ``1e-2 * tol``. If 'auto', the\n",
      "            tolerance will be adjusted based on the optimality of the current\n",
      "            iterate, which can speed up the optimization process, but is not always\n",
      "            reliable.\n",
      "        max_iter : None or int, optional\n",
      "            Maximum number of iterations before termination. If None (default), it\n",
      "            is set to 100 for ``method='trf'`` or to the number of variables for\n",
      "            ``method='bvls'`` (not counting iterations for 'bvls' initialization).\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "        \n",
      "                * 0 : work silently (default).\n",
      "                * 1 : display a termination report.\n",
      "                * 2 : display progress during iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        OptimizeResult with the following fields defined:\n",
      "        x : ndarray, shape (n,)\n",
      "            Solution found.\n",
      "        cost : float\n",
      "            Value of the cost function at the solution.\n",
      "        fun : ndarray, shape (m,)\n",
      "            Vector of residuals at the solution.\n",
      "        optimality : float\n",
      "            First-order optimality measure. The exact meaning depends on `method`,\n",
      "            refer to the description of `tol` parameter.\n",
      "        active_mask : ndarray of int, shape (n,)\n",
      "            Each component shows whether a corresponding constraint is active\n",
      "            (that is, whether a variable is at the bound):\n",
      "        \n",
      "                *  0 : a constraint is not active.\n",
      "                * -1 : a lower bound is active.\n",
      "                *  1 : an upper bound is active.\n",
      "        \n",
      "            Might be somewhat arbitrary for the `trf` method as it generates a\n",
      "            sequence of strictly feasible iterates and active_mask is determined\n",
      "            within a tolerance threshold.\n",
      "        nit : int\n",
      "            Number of iterations. Zero if the unconstrained solution is optimal.\n",
      "        status : int\n",
      "            Reason for algorithm termination:\n",
      "        \n",
      "                * -1 : the algorithm was not able to make progress on the last\n",
      "                  iteration.\n",
      "                *  0 : the maximum number of iterations is exceeded.\n",
      "                *  1 : the first-order optimality measure is less than `tol`.\n",
      "                *  2 : the relative change of the cost function is less than `tol`.\n",
      "                *  3 : the unconstrained solution is optimal.\n",
      "        \n",
      "        message : str\n",
      "            Verbal description of the termination reason.\n",
      "        success : bool\n",
      "            True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        nnls : Linear least squares with non-negativity constraint.\n",
      "        least_squares : Nonlinear least squares with bounds on the variables.                    \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm first computes the unconstrained least-squares solution by\n",
      "        `numpy.linalg.lstsq` or `scipy.sparse.linalg.lsmr` depending on\n",
      "        `lsq_solver`. This solution is returned as optimal if it lies within the\n",
      "        bounds.\n",
      "        \n",
      "        Method 'trf' runs the adaptation of the algorithm described in [STIR]_ for\n",
      "        a linear least-squares problem. The iterations are essentially the same as\n",
      "        in the nonlinear least-squares algorithm, but as the quadratic function\n",
      "        model is always accurate, we don't need to track or modify the radius of\n",
      "        a trust region. The line search (backtracking) is used as a safety net\n",
      "        when a selected step does not decrease the cost function. Read more\n",
      "        detailed description of the algorithm in `scipy.optimize.least_squares`.\n",
      "        \n",
      "        Method 'bvls' runs a Python implementation of the algorithm described in\n",
      "        [BVLS]_. The algorithm maintains active and free sets of variables, on\n",
      "        each iteration chooses a new variable to move from the active set to the\n",
      "        free set and then solves the unconstrained least-squares problem on free\n",
      "        variables. This algorithm is guaranteed to give an accurate solution\n",
      "        eventually, but may require up to n iterations for a problem with n\n",
      "        variables. Additionally, an ad-hoc initialization procedure is\n",
      "        implemented, that determines which variables to set free or active\n",
      "        initially. It takes some number of iterations before actual BVLS starts,\n",
      "        but can significantly reduce the number of further iterations.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "                  and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "                  Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "                  Vol. 21, Number 1, pp 1-23, 1999.\n",
      "        .. [BVLS] P. B. Start and R. L. Parker, \"Bounded-Variable Least-Squares:\n",
      "                  an Algorithm and Applications\", Computational Statistics, 10,\n",
      "                  129-141, 1995.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In this example a problem with a large sparse matrix and bounds on the\n",
      "        variables is solved.\n",
      "        \n",
      "        >>> from scipy.sparse import rand\n",
      "        >>> from scipy.optimize import lsq_linear\n",
      "        ...\n",
      "        >>> np.random.seed(0)\n",
      "        ...\n",
      "        >>> m = 20000\n",
      "        >>> n = 10000\n",
      "        ...\n",
      "        >>> A = rand(m, n, density=1e-4)\n",
      "        >>> b = np.random.randn(m)\n",
      "        ...\n",
      "        >>> lb = np.random.randn(n)\n",
      "        >>> ub = lb + 1\n",
      "        ...\n",
      "        >>> res = lsq_linear(A, b, bounds=(lb, ub), lsmr_tol='auto', verbose=1)\n",
      "        # may vary\n",
      "        The relative change of the cost function is less than `tol`.\n",
      "        Number of iterations 16, initial cost 1.5039e+04, final cost 1.1112e+04,\n",
      "        first-order optimality 4.66e-08.\n",
      "    \n",
      "    minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "        Minimization of scalar function of one or more variables.\n",
      "        \n",
      "        In general, the optimization problems are of the form::\n",
      "        \n",
      "            minimize f(x) subject to\n",
      "        \n",
      "            g_i(x) >= 0,  i = 1,...,m\n",
      "            h_j(x)  = 0,  j = 1,...,p\n",
      "        \n",
      "        where x is a vector of one or more variables.\n",
      "        ``g_i(x)`` are the inequality constraints.\n",
      "        ``h_j(x)`` are the equality constrains.\n",
      "        \n",
      "        Optionally, the lower and upper bounds for each element in x can also be\n",
      "        specified using the `bounds` argument.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Objective function.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its\n",
      "            derivatives (Jacobian, Hessian).\n",
      "        method : str or callable, optional\n",
      "            Type of solver.  Should be one of\n",
      "        \n",
      "                - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "                - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "                - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "                - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "                - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "                - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "                - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "                - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "                - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "                - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "                - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "                - custom - a callable object (added in version 0.14.0),\n",
      "                  see below for description.\n",
      "        \n",
      "            If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "            depending if the problem has constraints or bounds.\n",
      "        jac : bool or callable, optional\n",
      "            Jacobian (gradient) of objective function. Only for CG, BFGS,\n",
      "            Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg.\n",
      "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "            gradient along with the objective function. If False, the\n",
      "            gradient will be estimated numerically.\n",
      "            `jac` can also be a callable returning the gradient of the\n",
      "            objective. In this case, it must accept the same arguments as `fun`.\n",
      "        hess, hessp : callable, optional\n",
      "            Hessian (matrix of second-order derivatives) of objective function or\n",
      "            Hessian of objective function times an arbitrary vector p.  Only for\n",
      "            Newton-CG, dogleg, trust-ncg.\n",
      "            Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "            provided, then `hessp` will be ignored.  If neither `hess` nor\n",
      "            `hessp` is provided, then the Hessian product will be approximated\n",
      "            using finite differences on `jac`. `hessp` must compute the Hessian\n",
      "            times an arbitrary vector.\n",
      "        bounds : sequence, optional\n",
      "            Bounds for variables (only for L-BFGS-B, TNC and SLSQP).\n",
      "            ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the bounds on that parameter. Use None for one of ``min`` or\n",
      "            ``max`` when there is no bound in that direction.\n",
      "        constraints : dict or sequence of dict, optional\n",
      "            Constraints definition (only for COBYLA and SLSQP).\n",
      "            Each constraint is defined in a dictionary with fields:\n",
      "        \n",
      "                type : str\n",
      "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "                fun : callable\n",
      "                    The function defining the constraint.\n",
      "                jac : callable, optional\n",
      "                    The Jacobian of `fun` (only for SLSQP).\n",
      "                args : sequence, optional\n",
      "                    Extra arguments to be passed to the function and Jacobian.\n",
      "        \n",
      "            Equality constraint means that the constraint function result is to\n",
      "            be zero whereas inequality means that it is to be non-negative.\n",
      "            Note that COBYLA only supports inequality constraints.\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All methods accept the following\n",
      "            generic options:\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform.\n",
      "                disp : bool\n",
      "                    Set to True to print convergence messages.\n",
      "        \n",
      "            For method-specific options, see :func:`show_options()`.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar : Interface to minimization algorithms for scalar\n",
      "            univariate functions\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *BFGS*.\n",
      "        \n",
      "        **Unconstrained minimization**\n",
      "        \n",
      "        Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "        Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "        applications. However, if numerical computation of derivative can be\n",
      "        trusted, other algorithms using the first and/or second derivatives\n",
      "        information might be preferred for their better performance in\n",
      "        general.\n",
      "        \n",
      "        Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "        of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "        method. It performs sequential one-dimensional minimizations along\n",
      "        each vector of the directions set (`direc` field in `options` and\n",
      "        `info`), which is updated at each iteration of the main\n",
      "        minimization loop. The function need not be differentiable, and no\n",
      "        derivatives are taken.\n",
      "        \n",
      "        Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "        gradient algorithm by Polak and Ribiere, a variant of the\n",
      "        Fletcher-Reeves method described in [5]_ pp.  120-122. Only the\n",
      "        first derivatives are used.\n",
      "        \n",
      "        Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "        method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "        pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "        performance even for non-smooth optimizations. This method also\n",
      "        returns an approximation of the Hessian inverse, stored as\n",
      "        `hess_inv` in the OptimizeResult object.\n",
      "        \n",
      "        Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "        Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "        Newton method). It uses a CG method to the compute the search\n",
      "        direction. See also *TNC* method for a box-constrained\n",
      "        minimization with a similar algorithm.\n",
      "        \n",
      "        Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "        trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "        algorithm requires the gradient and Hessian; furthermore the\n",
      "        Hessian is required to be positive definite.\n",
      "        \n",
      "        Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "        Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "        unconstrained minimization. This algorithm requires the gradient\n",
      "        and either the Hessian or a function that computes the product of\n",
      "        the Hessian with a given vector.\n",
      "        \n",
      "        **Constrained minimization**\n",
      "        \n",
      "        Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "        algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "        \n",
      "        Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "        algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "        to bounds. This algorithm uses gradient information; it is also\n",
      "        called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "        method described above as it wraps a C implementation and allows\n",
      "        each variable to be given upper and lower bounds.\n",
      "        \n",
      "        Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "        Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "        [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "        approximations to the objective function and each constraint. The\n",
      "        method wraps a FORTRAN implementation of the algorithm. The\n",
      "        constraints functions 'fun' may return either a single number\n",
      "        or an array or list of numbers.\n",
      "        \n",
      "        Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "        Least SQuares Programming to minimize a function of several\n",
      "        variables with any combination of bounds, equality and inequality\n",
      "        constraints. The method wraps the SLSQP Optimization subroutine\n",
      "        originally implemented by Dieter Kraft [12]_. Note that the\n",
      "        wrapper handles infinite values in bounds by converting them into\n",
      "        large floating values.\n",
      "        \n",
      "        **Custom minimizers**\n",
      "        \n",
      "        It may be useful to pass a custom minimization method, for example\n",
      "        when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "        or a different library.  You can simply pass a callable as the ``method``\n",
      "        parameter.\n",
      "        \n",
      "        The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "        where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "        (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "        its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "        `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "        `fun` returns just the function values and `jac` is converted to a function\n",
      "        returning the Jacobian.  The method shall return an ``OptimizeResult``\n",
      "        object.\n",
      "        \n",
      "        The provided `method` callable must be able to accept (and possibly ignore)\n",
      "        arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "        expand in future versions and then these parameters will be passed to\n",
      "        the method.  You can find an example in the scipy.optimize tutorial.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "            Minimization. The Computer Journal 7: 308-13.\n",
      "        .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "            respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "            Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "            Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "            191-208.\n",
      "        .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "           a function of several variables without calculating derivatives. The\n",
      "           Computer Journal 7: 155-162.\n",
      "        .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "           Numerical Recipes (any edition), Cambridge University Press.\n",
      "        .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "           Springer New York.\n",
      "        .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "           Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "           Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "        .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "           778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "           optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "           550-560.\n",
      "        .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "           1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "        .. [9] Powell, M J D. A direct search optimization method that models\n",
      "           the objective and constraint functions by linear interpolation.\n",
      "           1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "           and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "        .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "           calculations. 1998. Acta Numerica 7: 287-336.\n",
      "        .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "           derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "           2007/NA03\n",
      "        .. [12] Kraft, D. A software package for sequential quadratic\n",
      "           programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "           Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "        function (and its respective derivatives) is implemented in `rosen`\n",
      "        (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "        \n",
      "        >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "        \n",
      "        A simple application of the *Nelder-Mead* method is:\n",
      "        \n",
      "        >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "        >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "        >>> res.x\n",
      "        array([ 1.,  1.,  1.,  1.,  1.])\n",
      "        \n",
      "        Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "        options:\n",
      "        \n",
      "        >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "        ...                options={'gtol': 1e-6, 'disp': True})\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 26\n",
      "                 Function evaluations: 31\n",
      "                 Gradient evaluations: 31\n",
      "        >>> res.x\n",
      "        array([ 1.,  1.,  1.,  1.,  1.])\n",
      "        >>> print(res.message)\n",
      "        Optimization terminated successfully.\n",
      "        >>> res.hess_inv\n",
      "        array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "               [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "               [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "               [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "               [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "        \n",
      "        \n",
      "        Next, consider a minimization problem with several constraints (namely\n",
      "        Example 16.4 from [5]_). The objective function is:\n",
      "        \n",
      "        >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "        \n",
      "        There are three constraints defined as:\n",
      "        \n",
      "        >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "        \n",
      "        And variables must be positive, hence the following bounds:\n",
      "        \n",
      "        >>> bnds = ((0, None), (0, None))\n",
      "        \n",
      "        The optimization problem is solved using the SLSQP method as:\n",
      "        \n",
      "        >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "        ...                constraints=cons)\n",
      "        \n",
      "        It should converge to the theoretical solution (1.4 ,1.7).\n",
      "    \n",
      "    minimize_scalar(fun, bracket=None, bounds=None, args=(), method='brent', tol=None, options=None)\n",
      "        Minimization of scalar function of one variable.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Objective function.\n",
      "            Scalar function, must return a scalar.\n",
      "        bracket : sequence, optional\n",
      "            For methods 'brent' and 'golden', `bracket` defines the bracketing\n",
      "            interval and can either have three items ``(a, b, c)`` so that\n",
      "            ``a < b < c`` and ``fun(b) < fun(a), fun(c)`` or two items ``a`` and\n",
      "            ``c`` which are assumed to be a starting interval for a downhill\n",
      "            bracket search (see `bracket`); it doesn't always mean that the\n",
      "            obtained solution will satisfy ``a <= x <= c``.\n",
      "        bounds : sequence, optional\n",
      "            For method 'bounded', `bounds` is mandatory and must have two items\n",
      "            corresponding to the optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function.\n",
      "        method : str or callable, optional\n",
      "            Type of solver.  Should be one of:\n",
      "        \n",
      "                - 'Brent'     :ref:`(see here) <optimize.minimize_scalar-brent>`\n",
      "                - 'Bounded'   :ref:`(see here) <optimize.minimize_scalar-bounded>`\n",
      "                - 'Golden'    :ref:`(see here) <optimize.minimize_scalar-golden>`\n",
      "                - custom - a callable object (added in version 0.14.0), see below\n",
      "        \n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options.\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform.\n",
      "                disp : bool\n",
      "                    Set to True to print convergence messages.\n",
      "        \n",
      "            See :func:`show_options()` for solver-specific options.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize : Interface to minimization algorithms for scalar multivariate\n",
      "            functions\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *Brent*.\n",
      "        \n",
      "        Method :ref:`Brent <optimize.minimize_scalar-brent>` uses Brent's\n",
      "        algorithm to find a local minimum.  The algorithm uses inverse\n",
      "        parabolic interpolation when possible to speed up convergence of\n",
      "        the golden section method.\n",
      "        \n",
      "        Method :ref:`Golden <optimize.minimize_scalar-golden>` uses the\n",
      "        golden section search technique. It uses analog of the bisection\n",
      "        method to decrease the bracketed interval. It is usually\n",
      "        preferable to use the *Brent* method.\n",
      "        \n",
      "        Method :ref:`Bounded <optimize.minimize_scalar-bounded>` can\n",
      "        perform bounded minimization. It uses the Brent method to find a\n",
      "        local minimum in the interval x1 < xopt < x2.\n",
      "        \n",
      "        **Custom minimizers**\n",
      "        \n",
      "        It may be useful to pass a custom minimization method, for example\n",
      "        when using some library frontend to minimize_scalar.  You can simply\n",
      "        pass a callable as the ``method`` parameter.\n",
      "        \n",
      "        The callable is called as ``method(fun, args, **kwargs, **options)``\n",
      "        where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "        (such as `bracket`, `tol`, etc.), except the `options` dict, which has\n",
      "        its contents also passed as `method` parameters pair by pair.  The method\n",
      "        shall return an ``OptimizeResult`` object.\n",
      "        \n",
      "        The provided `method` callable must be able to accept (and possibly ignore)\n",
      "        arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "        expand in future versions and then these parameters will be passed to\n",
      "        the method.  You can find an example in the scipy.optimize tutorial.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Consider the problem of minimizing the following function.\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x - 2) * x * (x + 2)**2\n",
      "        \n",
      "        Using the *Brent* method, we find the local minimum as:\n",
      "        \n",
      "        >>> from scipy.optimize import minimize_scalar\n",
      "        >>> res = minimize_scalar(f)\n",
      "        >>> res.x\n",
      "        1.28077640403\n",
      "        \n",
      "        Using the *Bounded* method, we find a local minimum with specified\n",
      "        bounds as:\n",
      "        \n",
      "        >>> res = minimize_scalar(f, bounds=(-3, -1), method='bounded')\n",
      "        >>> res.x\n",
      "        -2.0000002026\n",
      "    \n",
      "    newton(func, x0, fprime=None, args=(), tol=1.48e-08, maxiter=50, fprime2=None)\n",
      "        Find a zero using the Newton-Raphson or secant method.\n",
      "        \n",
      "        Find a zero of the function `func` given a nearby starting point `x0`.\n",
      "        The Newton-Raphson method is used if the derivative `fprime` of `func`\n",
      "        is provided, otherwise the secant method is used.  If the second order\n",
      "        derivate `fprime2` of `func` is provided, parabolic Halley's method\n",
      "        is used.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            The function whose zero is wanted. It must be a function of a\n",
      "            single variable of the form f(x,a,b,c...), where a,b,c... are extra\n",
      "            arguments that can be passed in the `args` parameter.\n",
      "        x0 : float\n",
      "            An initial estimate of the zero that should be somewhere near the\n",
      "            actual zero.\n",
      "        fprime : function, optional\n",
      "            The derivative of the function when available and convenient. If it\n",
      "            is None (default), then the secant method is used.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to be used in the function call.\n",
      "        tol : float, optional\n",
      "            The allowable error of the zero value.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        fprime2 : function, optional\n",
      "            The second order derivative of the function when available and\n",
      "            convenient. If it is None (default), then the normal Newton-Raphson\n",
      "            or the secant method is used. If it is given, parabolic Halley's\n",
      "            method is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        zero : float\n",
      "            Estimated location where function is zero.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, ridder, bisect\n",
      "        fsolve : find zeroes in n dimensions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The convergence rate of the Newton-Raphson method is quadratic,\n",
      "        the Halley method is cubic, and the secant method is\n",
      "        sub-quadratic.  This means that if the function is well behaved\n",
      "        the actual error in the estimated zero is approximately the square\n",
      "        (cube for Halley) of the requested tolerance up to roundoff\n",
      "        error. However, the stopping criterion used here is the step size\n",
      "        and there is no guarantee that a zero has been found. Consequently\n",
      "        the result should be verified. Safer algorithms are brentq,\n",
      "        brenth, ridder, and bisect, but they all require that the root\n",
      "        first be bracketed in an interval where the function changes\n",
      "        sign. The brentq algorithm is recommended for general use in one\n",
      "        dimensional problems when such an interval has been found.\n",
      "    \n",
      "    newton_krylov(F, xin, iter=None, rdiff=None, method='lgmres', inner_maxiter=20, inner_M=None, outer_k=10, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Krylov approximation for inverse Jacobian.\n",
      "        \n",
      "        This method is suitable for solving large-scale problems.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        x0 : array_like\n",
      "            Initial guess for the solution\n",
      "        rdiff : float, optional\n",
      "            Relative step size to use in numerical differentiation.\n",
      "        method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or function\n",
      "            Krylov method to use to approximate the Jacobian.\n",
      "            Can be a string, or a function implementing the same interface as\n",
      "            the iterative solvers in `scipy.sparse.linalg`.\n",
      "        \n",
      "            The default is `scipy.sparse.linalg.lgmres`.\n",
      "        inner_M : LinearOperator or InverseJacobian\n",
      "            Preconditioner for the inner Krylov iteration.\n",
      "            Note that you can use also inverse Jacobians as (adaptive)\n",
      "            preconditioners. For example,\n",
      "        \n",
      "            >>> from scipy.optimize.nonlin import BroydenFirst, KrylovJacobian\n",
      "            >>> from scipy.optimize.nonlin import InverseJacobian\n",
      "            >>> jac = BroydenFirst()\n",
      "            >>> kjac = KrylovJacobian(inner_M=InverseJacobian(jac))\n",
      "        \n",
      "            If the preconditioner has a method named 'update', it will be called\n",
      "            as ``update(x, f)`` after each nonlinear step, with ``x`` giving\n",
      "            the current point, and ``f`` the current function value.\n",
      "        inner_tol, inner_maxiter, ...\n",
      "            Parameters to pass on to the \\\"inner\\\" Krylov solver.\n",
      "            See `scipy.sparse.linalg.gmres` for details.\n",
      "        outer_k : int, optional\n",
      "            Size of the subspace kept across LGMRES nonlinear iterations.\n",
      "            See `scipy.sparse.linalg.lgmres` for details.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.sparse.linalg.gmres\n",
      "        scipy.sparse.linalg.lgmres\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements a Newton-Krylov solver. The basic idea is\n",
      "        to compute the inverse of the Jacobian with an iterative Krylov\n",
      "        method. These methods require only evaluating the Jacobian-vector\n",
      "        products, which are conveniently approximated by a finite difference:\n",
      "        \n",
      "        .. math:: J v \\approx (f(x + \\omega*v/|v|) - f(x)) / \\omega\n",
      "        \n",
      "        Due to the use of iterative matrix inverses, these methods can\n",
      "        deal with large nonlinear problems.\n",
      "        \n",
      "        Scipy's `scipy.sparse.linalg` module offers a selection of Krylov\n",
      "        solvers to choose from. The default here is `lgmres`, which is a\n",
      "        variant of restarted GMRES iteration that reuses some of the\n",
      "        information obtained in the previous Newton steps to invert\n",
      "        Jacobians in subsequent steps.\n",
      "        \n",
      "        For a review on Newton-Krylov methods, see for example [1]_,\n",
      "        and for the LGMRES sparse inverse method, see [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] D.A. Knoll and D.E. Keyes, J. Comp. Phys. 193, 357 (2004).\n",
      "               :doi:`10.1016/j.jcp.2003.08.010`\n",
      "        .. [2] A.H. Baker and E.R. Jessup and T. Manteuffel,\n",
      "               SIAM J. Matrix Anal. Appl. 26, 962 (2005).\n",
      "               :doi:`10.1137/S0895479803422014`\n",
      "    \n",
      "    nnls(A, b)\n",
      "        Solve ``argmin_x || Ax - b ||_2`` for ``x>=0``. This is a wrapper\n",
      "        for a FORTRAN non-negative least squares solver.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : ndarray\n",
      "            Matrix ``A`` as shown above.\n",
      "        b : ndarray\n",
      "            Right-hand side vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Solution vector.\n",
      "        rnorm : float\n",
      "            The residual, ``|| Ax-b ||_2``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The FORTRAN code was published in the book below. The algorithm\n",
      "        is an active set method. It solves the KKT (Karush-Kuhn-Tucker)\n",
      "        conditions for the non-negative least squares problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Lawson C., Hanson R.J., (1987) Solving Least Squares Problems, SIAM\n",
      "    \n",
      "    ridder(f, a, b, args=(), xtol=2e-12, rtol=8.8817841970012523e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in an interval.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number.  f must be continuous, and f(a) and\n",
      "            f(b) must have opposite signs.\n",
      "        a : number\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : number\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``.\n",
      "        maxiter : number, optional\n",
      "            if convergence is not achieved in maxiter iterations, an error is\n",
      "            raised.  Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned.  If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a RootResults object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : RootResults (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.\n",
      "            In particular, ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, bisect, newton : one-dimensional root-finding\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses [Ridders1979]_ method to find a zero of the function `f` between the\n",
      "        arguments `a` and `b`. Ridders' method is faster than bisection, but not\n",
      "        generally as fast as the Brent rountines. [Ridders1979]_ provides the\n",
      "        classic description and source of the algorithm. A description can also be\n",
      "        found in any recent edition of Numerical Recipes.\n",
      "        \n",
      "        The routine used here diverges slightly from standard presentations in\n",
      "        order to be a bit more careful of tolerance.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Ridders1979]\n",
      "           Ridders, C. F. J. \"A New Algorithm for Computing a\n",
      "           Single Root of a Real Continuous Function.\"\n",
      "           IEEE Trans. Circuits Systems 26, 979-980, 1979.\n",
      "    \n",
      "    root(fun, x0, args=(), method='hybr', jac=None, tol=None, callback=None, options=None)\n",
      "        Find a root of a vector function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            A vector function to find a root of.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its Jacobian.\n",
      "        method : str, optional\n",
      "            Type of solver.  Should be one of\n",
      "        \n",
      "                - 'hybr'             :ref:`(see here) <optimize.root-hybr>`\n",
      "                - 'lm'               :ref:`(see here) <optimize.root-lm>`\n",
      "                - 'broyden1'         :ref:`(see here) <optimize.root-broyden1>`\n",
      "                - 'broyden2'         :ref:`(see here) <optimize.root-broyden2>`\n",
      "                - 'anderson'         :ref:`(see here) <optimize.root-anderson>`\n",
      "                - 'linearmixing'     :ref:`(see here) <optimize.root-linearmixing>`\n",
      "                - 'diagbroyden'      :ref:`(see here) <optimize.root-diagbroyden>`\n",
      "                - 'excitingmixing'   :ref:`(see here) <optimize.root-excitingmixing>`\n",
      "                - 'krylov'           :ref:`(see here) <optimize.root-krylov>`\n",
      "                - 'df-sane'          :ref:`(see here) <optimize.root-dfsane>`\n",
      "        \n",
      "        jac : bool or callable, optional\n",
      "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "            value of Jacobian along with the objective function. If False, the\n",
      "            Jacobian will be estimated numerically.\n",
      "            `jac` can also be a callable returning the Jacobian of `fun`. In\n",
      "            this case, it must accept the same arguments as `fun`.\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual. For all methods but 'hybr' and 'lm'.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. E.g. `xtol` or `maxiter`, see\n",
      "            :obj:`show_options()` for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : OptimizeResult\n",
      "            The solution represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the algorithm exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *hybr*.\n",
      "        \n",
      "        Method *hybr* uses a modification of the Powell hybrid method as\n",
      "        implemented in MINPACK [1]_.\n",
      "        \n",
      "        Method *lm* solves the system of nonlinear equations in a least squares\n",
      "        sense using a modification of the Levenberg-Marquardt algorithm as\n",
      "        implemented in MINPACK [1]_.\n",
      "        \n",
      "        Method *df-sane* is a derivative-free spectral method. [3]_\n",
      "        \n",
      "        Methods *broyden1*, *broyden2*, *anderson*, *linearmixing*,\n",
      "        *diagbroyden*, *excitingmixing*, *krylov* are inexact Newton methods,\n",
      "        with backtracking or full line searches [2]_. Each method corresponds\n",
      "        to a particular Jacobian approximations. See `nonlin` for details.\n",
      "        \n",
      "        - Method *broyden1* uses Broyden's first Jacobian approximation, it is\n",
      "          known as Broyden's good method.\n",
      "        - Method *broyden2* uses Broyden's second Jacobian approximation, it\n",
      "          is known as Broyden's bad method.\n",
      "        - Method *anderson* uses (extended) Anderson mixing.\n",
      "        - Method *Krylov* uses Krylov approximation for inverse Jacobian. It\n",
      "          is suitable for large-scale problem.\n",
      "        - Method *diagbroyden* uses diagonal Broyden Jacobian approximation.\n",
      "        - Method *linearmixing* uses a scalar Jacobian approximation.\n",
      "        - Method *excitingmixing* uses a tuned diagonal Jacobian\n",
      "          approximation.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "            The algorithms implemented for methods *diagbroyden*,\n",
      "            *linearmixing* and *excitingmixing* may be useful for specific\n",
      "            problems, but whether they will work may depend strongly on the\n",
      "            problem.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] More, Jorge J., Burton S. Garbow, and Kenneth E. Hillstrom.\n",
      "           1980. User Guide for MINPACK-1.\n",
      "        .. [2] C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear\n",
      "            Equations. Society for Industrial and Applied Mathematics.\n",
      "            <http://www.siam.org/books/kelley/>\n",
      "        .. [3] W. La Cruz, J.M. Martinez, M. Raydan. Math. Comp. 75, 1429 (2006).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations and its\n",
      "        jacobian.\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        >>> def jac(x):\n",
      "        ...     return np.array([[1 + 1.5 * (x[0] - x[1])**2,\n",
      "        ...                       -1.5 * (x[0] - x[1])**2],\n",
      "        ...                      [-1.5 * (x[1] - x[0])**2,\n",
      "        ...                       1 + 1.5 * (x[1] - x[0])**2]])\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')\n",
      "        >>> sol.x\n",
      "        array([ 0.8411639,  0.1588361])\n",
      "    \n",
      "    rosen(x)\n",
      "        The Rosenbrock function.\n",
      "        \n",
      "        The function computed is::\n",
      "        \n",
      "            sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Rosenbrock function is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f : float\n",
      "            The value of the Rosenbrock function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen_der, rosen_hess, rosen_hess_prod\n",
      "    \n",
      "    rosen_der(x)\n",
      "        The derivative (i.e. gradient) of the Rosenbrock function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the derivative is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_der : (N,) ndarray\n",
      "            The gradient of the Rosenbrock function at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_hess, rosen_hess_prod\n",
      "    \n",
      "    rosen_hess(x)\n",
      "        The Hessian matrix of the Rosenbrock function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_der, rosen_hess_prod\n",
      "    \n",
      "    rosen_hess_prod(x, p)\n",
      "        Product of the Hessian matrix of the Rosenbrock function with a vector.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        p : array_like\n",
      "            1-D array, the vector to be multiplied by the Hessian matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess_prod : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x` multiplied\n",
      "            by the vector `p`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_der, rosen_hess\n",
      "    \n",
      "    show_options(solver=None, method=None, disp=True)\n",
      "        Show documentation for additional options of optimization solvers.\n",
      "        \n",
      "        These are method-specific options that can be supplied through the\n",
      "        ``options`` dict.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        solver : str\n",
      "            Type of optimization solver. One of 'minimize', 'minimize_scalar',\n",
      "            'root', or 'linprog'.\n",
      "        method : str, optional\n",
      "            If not given, shows all methods of the specified solver. Otherwise,\n",
      "            show only the options for the specified method. Valid values\n",
      "            corresponds to methods' names of respective solver (e.g. 'BFGS' for\n",
      "            'minimize').\n",
      "        disp : bool, optional\n",
      "            Whether to print the result rather than returning it.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        text\n",
      "            Either None (for disp=False) or the text string (disp=True)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The solver-specific methods are:\n",
      "        \n",
      "        `scipy.optimize.minimize`\n",
      "        \n",
      "        - :ref:`Nelder-Mead <optimize.minimize-neldermead>`\n",
      "        - :ref:`Powell      <optimize.minimize-powell>`\n",
      "        - :ref:`CG          <optimize.minimize-cg>`\n",
      "        - :ref:`BFGS        <optimize.minimize-bfgs>`\n",
      "        - :ref:`Newton-CG   <optimize.minimize-newtoncg>`\n",
      "        - :ref:`L-BFGS-B    <optimize.minimize-lbfgsb>`\n",
      "        - :ref:`TNC         <optimize.minimize-tnc>`\n",
      "        - :ref:`COBYLA      <optimize.minimize-cobyla>`\n",
      "        - :ref:`SLSQP       <optimize.minimize-slsqp>`\n",
      "        - :ref:`dogleg      <optimize.minimize-dogleg>`\n",
      "        - :ref:`trust-ncg   <optimize.minimize-trustncg>`\n",
      "        \n",
      "        `scipy.optimize.root`\n",
      "        \n",
      "        - :ref:`hybr              <optimize.root-hybr>`\n",
      "        - :ref:`lm                <optimize.root-lm>`\n",
      "        - :ref:`broyden1          <optimize.root-broyden1>`\n",
      "        - :ref:`broyden2          <optimize.root-broyden2>`\n",
      "        - :ref:`anderson          <optimize.root-anderson>`\n",
      "        - :ref:`linearmixing      <optimize.root-linearmixing>`\n",
      "        - :ref:`diagbroyden       <optimize.root-diagbroyden>`\n",
      "        - :ref:`excitingmixing    <optimize.root-excitingmixing>`\n",
      "        - :ref:`krylov            <optimize.root-krylov>`\n",
      "        - :ref:`df-sane           <optimize.root-dfsane>`\n",
      "        \n",
      "        `scipy.optimize.minimize_scalar`\n",
      "        \n",
      "        - :ref:`brent       <optimize.minimize_scalar-brent>`\n",
      "        - :ref:`golden      <optimize.minimize_scalar-golden>`\n",
      "        - :ref:`bounded     <optimize.minimize_scalar-bounded>`\n",
      "        \n",
      "        `scipy.optimize.linprog`\n",
      "        \n",
      "        - :ref:`simplex     <optimize.linprog-simplex>`\n",
      "\n",
      "DATA\n",
      "    __all__ = ['LbfgsInvHessProduct', 'OptimizeResult', 'OptimizeWarning',...\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "\n",
      "FILE\n",
      "    c:\\programdata\\anaconda3\\lib\\site-packages\\scipy\\optimize\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0\n"
     ]
    }
   ],
   "source": [
    "#---------------scipy.linalg.det()-------------\n",
    "arr = np.array([[1,2],[3,4]])\n",
    "\n",
    "print(linalg.det(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.661338147750939e-16\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[3,2],[6,4]])\n",
    "\n",
    "print(linalg.det(arr))      # singular matrix det. is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected square matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-653fd966e4c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# det. is allowed only for square matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py\u001b[0m in \u001b[0;36mdet\u001b[1;34m(a, overwrite_a, check_finite)\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected square matrix'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m     \u001b[0moverwrite_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moverwrite_a\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_datacopied\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m     \u001b[0mfdet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_flinalg_funcs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'det'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: expected square matrix"
     ]
    }
   ],
   "source": [
    "print(linalg.det(np.ones((3,4)))) # det. is allowed only for square matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "#----------------scipy.linalg.inv()---------\n",
    "arr = np.array([[1,2],[3,4]])\n",
    "\n",
    "iarr = linalg.inv(arr)\n",
    "\n",
    "print(iarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#dot product of matrix with its inverse is identity matrix\n",
    "print(np.allclose(np.dot(arr, iarr),np.eye(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-7f2e383f9157>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\basic.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a, overwrite_a, check_finite)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0minv_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlwork\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite_lu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m         raise ValueError('illegal value in %d-th argument of internal '\n",
      "\u001b[1;31mLinAlgError\u001b[0m: singular matrix"
     ]
    }
   ],
   "source": [
    "arr = np.array([[3,2],[6,4]])\n",
    "print(linalg.inv(arr))     #throws error due to identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.random.normal(size=1000)\n",
    "bins = np.arange(-4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.61367946e+00   8.59133380e-01   1.12912191e+00   2.20428339e-01\n",
      "  -1.26402010e+00   1.87401857e-01  -9.63602358e-01  -4.19554553e-01\n",
      "   1.15751079e+00  -1.34140430e-01  -1.40450610e+00  -4.90042566e-01\n",
      "   2.21579285e+00  -3.28023323e-01   1.54926012e+00  -9.85780769e-01\n",
      "   2.83036564e-01   8.15279321e-01  -6.52608343e-01   2.19809325e-01\n",
      "  -1.26485822e-01   4.86704854e-01   1.02597126e+00  -6.57018380e-01\n",
      "   2.08854859e+00  -8.69693137e-01   2.23662594e+00  -4.68880494e-01\n",
      "   1.45394003e+00  -4.53985310e-01  -5.73097458e-01  -5.29501391e-01\n",
      "   8.36280238e-01   1.59719219e+00  -8.74286872e-01   1.24856678e+00\n",
      "  -7.66231827e-01   2.12451620e-01   3.43479624e-01  -1.40700416e+00\n",
      "  -1.39664170e+00   1.03440202e+00  -5.70022232e-01   2.98168260e-01\n",
      "   6.55557097e-01  -6.61393417e-02  -2.02666045e-01   6.64528885e-01\n",
      "  -6.37714091e-01   1.76959825e-01   1.41534637e+00  -9.70986394e-01\n",
      "  -8.05723468e-02   1.02628418e+00   1.47522805e+00   8.70726217e-01\n",
      "  -2.29584876e+00   6.41103226e-01  -1.26085049e+00  -1.79534454e+00\n",
      "  -1.07776974e+00  -8.19058068e-01  -5.19821023e-01   5.29081569e-01\n",
      "  -6.38286791e-02   3.58285450e-01  -4.90402572e-01   1.19627560e+00\n",
      "  -8.83869417e-01  -1.09938103e+00   8.52923676e-01   3.93944984e-01\n",
      "  -4.74826957e-01   8.68186962e-01   9.69567832e-01   1.32132865e+00\n",
      "  -1.39272770e+00  -1.08393851e-01  -6.24751287e-01  -1.65625538e-01\n",
      "  -1.45694937e+00  -1.27300448e-01  -1.57620921e+00   1.03869133e-01\n",
      "   8.22381790e-01  -3.83628429e-01   1.22901711e+00   6.77121935e-01\n",
      "  -8.39766011e-01   9.47616476e-01  -1.86333015e+00  -8.70287293e-01\n",
      "  -4.33658695e-01  -9.58759923e-01  -3.98499791e-01   1.20540478e+00\n",
      "  -9.67723218e-01  -4.25917626e-01   7.33099468e-01  -1.22496197e-01\n",
      "  -1.08882385e+00  -8.72231904e-01  -3.51801585e-03  -1.17857058e-01\n",
      "  -3.14822742e-01   7.62256131e-01   3.68390693e-01  -1.17331450e+00\n",
      "  -1.52414563e+00  -3.45703486e-01   5.39902274e-01   1.25111194e+00\n",
      "  -7.81519943e-01   8.26885736e-01   3.07898684e-01   4.09738719e-02\n",
      "   1.46852664e-01  -2.25326733e-01   1.50256593e-01  -4.55153584e-01\n",
      "  -6.29052340e-01  -8.80053435e-01  -1.08287672e+00  -5.69811372e-01\n",
      "   5.81448562e-01   9.53218004e-01  -1.02229728e+00   3.71608666e-01\n",
      "   5.24926508e-02   6.67088353e-01   6.69607504e-01   1.67274896e+00\n",
      "   2.06905522e-01  -6.92304197e-01   6.99132625e-01  -2.45319457e-01\n",
      "  -1.89685731e+00  -1.61140645e-02  -1.71653273e+00   4.56442026e-01\n",
      "   7.37027951e-01   1.24048698e+00   2.99607349e-01  -5.77262801e-01\n",
      "   1.79954317e-01  -5.65219904e-01  -1.72511460e+00  -1.39903668e+00\n",
      "   6.68564970e-02  -1.00631961e+00   8.70814817e-01  -3.48489898e+00\n",
      "  -2.41705821e+00   9.81741190e-01   9.15982106e-01   8.14353657e-01\n",
      "  -2.34005522e+00  -3.02107610e-01  -2.92147591e-01  -3.23175631e-01\n",
      "  -2.96841437e-01   8.95555157e-01   2.79913320e-01   3.67365323e-02\n",
      "  -1.36606815e+00  -2.10768779e-01   1.56798835e-01  -3.29183347e-02\n",
      "   8.15366933e-01   1.28052729e+00   6.62389284e-01   6.84929505e-01\n",
      "  -3.71590705e-01   1.47970083e+00   9.78716132e-01   5.93949699e-01\n",
      "   2.13928463e-01   9.07050230e-01   2.17572967e-01   3.01961956e-01\n",
      "   6.47249933e-01  -4.45593671e-01   8.55492618e-01  -6.03063249e-01\n",
      "   1.04944547e+00  -7.48885013e-02  -2.20517370e+00  -1.21297395e+00\n",
      "   3.28231614e-01   5.29508239e-01  -1.82467791e+00   1.21798041e+00\n",
      "   1.66911180e-01  -1.76838625e+00  -1.22390598e+00   1.20945643e-01\n",
      "  -1.04174070e-01  -6.19493299e-01  -5.76084387e-01  -6.01311409e-01\n",
      "   7.18061463e-01   7.01232269e-01   2.90828570e-01  -6.89600569e-01\n",
      "   1.04463264e-01   1.27434029e+00  -1.24928193e+00   2.02690430e-01\n",
      "   9.73185037e-01  -1.26424781e-01  -9.02772728e-01  -4.42861070e-01\n",
      "  -4.82712997e-01  -1.43346583e-01  -5.24092564e-01  -9.87366949e-01\n",
      "   8.80998730e-01   1.05997445e+00  -8.09815800e-01  -8.22243673e-01\n",
      "  -1.93071590e-01  -1.22060755e-02  -1.65251874e+00   3.17078355e-01\n",
      "   4.27500182e-02   8.28579192e-01  -7.60216959e-02  -1.01534170e+00\n",
      "   3.98199223e-01   9.55029044e-01   4.40281325e-01  -8.16891707e-01\n",
      "   1.16811975e+00  -1.45189496e+00  -4.75423272e-01  -8.40987901e-01\n",
      "  -1.85628980e+00  -1.34930790e-01  -1.49531787e-01  -5.52644858e-02\n",
      "  -6.15095894e-01  -1.16536120e+00   9.78741465e-01   1.02299597e+00\n",
      "   2.23660808e+00   1.18447445e-01   6.04596772e-01  -4.11548348e-01\n",
      "   1.05607218e+00   2.89413203e-01  -1.65503658e+00   7.56419650e-01\n",
      "   4.28281402e-01   4.47532255e-01   1.62053679e-01  -8.99400895e-03\n",
      "   1.05099679e-01   8.08612213e-01   3.15266461e-01  -6.77252448e-01\n",
      "  -3.59555563e-01   1.02210427e-01  -4.66060577e-01  -1.48416640e+00\n",
      "  -9.40905272e-01   1.66472830e-01  -2.25005764e+00  -2.48205375e-01\n",
      "   7.29562952e-01  -1.96477384e-01   2.58407532e-01   8.07662376e-01\n",
      "  -4.16232354e-01  -8.84707590e-01   1.15123387e+00  -8.82856268e-01\n",
      "  -1.51837916e-01   1.51582697e+00  -1.40460570e+00  -1.79299371e-01\n",
      "   1.38591254e+00   9.29027516e-01   1.18946422e+00   2.83852644e+00\n",
      "   5.19355285e-01   8.74033605e-01   5.17507107e-02  -9.30425304e-01\n",
      "  -1.90733360e+00  -7.59544352e-02  -1.15610693e+00   9.07055174e-01\n",
      "   9.49062390e-01   1.13098940e+00   1.40877657e+00   1.71216690e-02\n",
      "  -1.91527587e+00   8.49688102e-01  -1.96579580e+00  -1.35512562e+00\n",
      "   3.71602815e-01   9.37514708e-02   6.57973002e-01  -4.68512658e-05\n",
      "  -1.68283022e+00  -3.10550363e-01  -9.57900232e-01  -1.14995877e+00\n",
      "  -1.09654877e+00   1.50602173e-01   6.36218127e-02  -5.71911027e-02\n",
      "   3.79631303e-01   5.31035517e-01  -1.61806837e+00   5.50511167e-01\n",
      "  -4.87725896e-01  -5.21132196e-01  -4.90493789e-01  -3.49449737e-01\n",
      "   1.28608467e+00  -1.28028986e+00  -2.82740834e-01   4.08786769e-01\n",
      "   7.50352133e-02  -5.10412448e-02   1.70621902e+00   4.15084129e-01\n",
      "  -9.25548814e-01  -9.90957769e-02   2.01340417e+00   6.02571206e-01\n",
      "   3.76774282e-01   6.70614686e-01   1.16866580e+00  -7.08686993e-01\n",
      "  -1.56381681e-01   9.47207818e-01  -1.02303488e+00  -1.38293190e+00\n",
      "  -3.87956020e-02   1.53707389e+00  -3.60166783e-01  -1.05808503e+00\n",
      "  -2.45011142e-02   4.69231323e-02   2.68156346e-01   4.17503186e-01\n",
      "  -7.10260735e-01   9.70035245e-01  -4.88423407e-01  -5.46476817e-01\n",
      "   1.42870167e+00   4.14039427e-03  -1.56472889e+00  -8.13989288e-02\n",
      "  -2.13810811e+00  -2.42430354e-01  -6.24061191e-01   6.77154410e-01\n",
      "   3.52118658e-01   5.21779695e-01  -1.04308994e+00  -1.58921070e+00\n",
      "  -1.55137960e-03   8.17145192e-01   1.42336150e+00  -3.93968109e-01\n",
      "  -5.28788176e-02   2.01914673e+00  -8.91588426e-01   2.00927609e+00\n",
      "   9.20192948e-01  -7.97186024e-02   1.09076815e-01  -5.48035066e-01\n",
      "  -9.62556415e-02  -5.57216169e-01   2.40319032e+00  -7.49611259e-01\n",
      "  -3.78955653e-01   6.22964427e-01   1.33687764e+00  -1.29139455e+00\n",
      "  -2.23615423e+00   7.98174135e-02   1.26979496e+00   3.80402152e-01\n",
      "  -1.23419663e+00  -2.51864945e-01  -7.02728793e-01   7.15141803e-01\n",
      "  -1.71311083e-01   4.81123637e-01   7.53833378e-02  -4.31571635e-01\n",
      "  -7.34720450e-01  -3.61217528e-01   1.19157945e+00  -2.39545425e-01\n",
      "  -5.06309335e-01   3.70416661e-01  -4.76841851e-01  -2.32690970e+00\n",
      "  -1.25667487e+00  -1.86535509e-01   1.60920262e-01   3.80399097e-01\n",
      "  -3.39194064e-01   2.64198061e+00   1.12704496e+00   1.32590527e+00\n",
      "   6.02999210e-01   5.19206040e-02  -1.23752894e+00  -1.78923299e+00\n",
      "   2.22447536e+00  -4.00633685e-01  -4.29695073e-01  -9.67478445e-01\n",
      "  -9.40466056e-01   8.21088412e-01   1.69402853e+00  -6.06249059e-01\n",
      "  -1.03555951e+00  -5.73101340e-01  -1.29061165e+00   2.30172831e+00\n",
      "   8.26293073e-01   6.35153339e-01  -1.03619901e+00   1.25141182e+00\n",
      "  -1.46390047e+00   6.51412487e-01   2.21750830e+00  -1.06261333e+00\n",
      "   1.85675729e+00   3.44546896e-01  -2.18883390e-01  -9.77189611e-01\n",
      "  -4.01914716e-01   7.25331872e-01  -8.27264217e-01  -9.48759137e-01\n",
      "  -8.63145223e-01   3.87037103e-01   1.53940152e-01   2.45240508e-01\n",
      "   7.47111660e-01   9.51280650e-01  -1.60291274e+00  -7.52327740e-01\n",
      "  -4.60143041e-01  -9.74769689e-01  -5.37154485e-01  -1.79541116e-01\n",
      "   2.43136798e-01   4.09696828e-01   7.56353773e-01   2.47614073e-01\n",
      "  -1.13778122e+00   5.07944681e-01   7.79484643e-01   1.12422399e+00\n",
      "   1.88651735e+00  -9.72653716e-01   9.82643649e-01  -8.82256690e-01\n",
      "  -1.90267260e-01  -1.25859217e-01  -1.72296585e+00  -6.18658365e-02\n",
      "   2.79183637e-01  -6.45726039e-01  -2.01118026e-01  -5.63757702e-01\n",
      "  -1.10276069e+00  -1.03849895e+00  -5.24736170e-01   2.23881835e-01\n",
      "   7.10890018e-01  -5.15921870e-01  -7.13615212e-01  -1.24930292e+00\n",
      "  -5.67191061e-01   1.71176627e-01   2.46830210e+00  -1.91760093e+00\n",
      "  -1.88964356e+00   4.45714083e-01   3.54831849e-01   1.00151457e+00\n",
      "   2.89573804e-01  -1.39352249e-01  -5.36325358e-01   1.14422583e-02\n",
      "   1.72259493e+00   6.88870069e-01  -1.19459204e+00   6.77851751e-01\n",
      "   3.03981150e-01  -6.81781512e-01  -4.55514679e-01  -8.34844460e-01\n",
      "   4.63839492e-01  -1.30165814e+00   1.67481422e-01   9.14912868e-01\n",
      "  -7.93306469e-02   5.80102487e-01  -2.02688547e+00   4.42265912e-01\n",
      "   8.07638916e-01  -2.53314864e-01   8.15490278e-02   1.11692480e-01\n",
      "   2.03642892e+00  -3.30984262e-01  -1.49936337e+00  -1.56031236e+00\n",
      "  -1.14691645e+00   5.11376913e-01  -6.63869369e-01  -1.71853728e+00\n",
      "   8.23435557e-01   1.80284472e-01  -2.01854020e+00   1.33160513e+00\n",
      "   9.85144615e-01  -1.08439769e+00   3.25241048e-02   5.34447899e-01\n",
      "  -2.01995156e-01   3.85991165e-01  -6.36553050e-01   4.39287408e-01\n",
      "   1.07546567e+00  -2.25091782e+00  -5.84503474e-01   2.84852678e-01\n",
      "   2.20466316e+00  -7.82991049e-01  -6.57345475e-01  -4.40114196e-01\n",
      "   1.29334028e+00  -1.13319740e+00  -8.55023482e-01  -6.63029863e-01\n",
      "  -2.35716318e-01  -1.31545512e-01   4.79750547e-01   9.63679902e-01\n",
      "  -3.49278378e-02  -7.91700395e-01  -4.13657386e-01   6.14431058e-01\n",
      "   1.94560561e-01   1.93241036e+00  -2.45265761e-02   1.23048621e+00\n",
      "  -1.67393444e+00  -8.61689209e-02   8.30344351e-01  -1.87844216e-02\n",
      "  -5.41278007e-02   2.86455110e-01   6.80735759e-01   2.31364026e-01\n",
      "   5.23992689e-01   1.40137079e+00   2.11194646e-01  -1.61242888e-01\n",
      "   7.56927452e-01  -7.96626131e-01   7.47789839e-01   4.95674579e-01\n",
      "   1.79671462e-01   7.47323270e-01  -3.60046116e-01  -4.54145861e-01\n",
      "  -1.44765856e+00   4.68742087e-01  -4.89519227e-01   1.70012237e+00\n",
      "  -2.24364385e+00  -8.74424855e-01  -2.97346469e-01   6.74741163e-01\n",
      "  -9.64089991e-03  -1.14140838e+00   5.80905735e-01  -1.14432931e+00\n",
      "   9.15820335e-01  -9.62349759e-01  -4.51023344e-01  -1.47322590e+00\n",
      "  -1.01065121e+00  -1.10884226e+00   1.18498812e+00   5.05491835e-01\n",
      "  -5.26038045e-01   6.38529304e-01  -5.95144262e-01  -2.55957726e-01\n",
      "  -2.29970255e-02   7.63805364e-01   9.70104849e-01  -1.39081862e-01\n",
      "   2.56982762e-01   1.62454343e-01  -1.07648840e-01   1.33461840e+00\n",
      "   1.10039756e+00   7.25381827e-01   1.11197438e+00  -1.55003457e+00\n",
      "  -6.89035161e-01   2.96086742e-01  -4.45865190e-01  -8.15211808e-01\n",
      "  -9.63556469e-01  -1.21602164e+00  -2.25813519e-01   8.56148926e-01\n",
      "  -2.88162588e-01   7.78645520e-02  -2.48986210e-01   9.15607956e-01\n",
      "  -3.39401129e-01   9.00062102e-01   9.21557794e-01   1.24685574e-01\n",
      "  -5.24840300e-01   3.13026158e-01  -8.14611894e-01   6.42331990e-01\n",
      "  -1.77231729e+00   2.06910222e+00  -1.43804636e+00   8.31383041e-01\n",
      "   1.71671910e+00  -3.85590751e-02  -1.01411604e+00   7.97491118e-01\n",
      "   5.88966909e-01   1.29789754e+00   1.17617853e+00   1.62051297e-01\n",
      "   4.73372798e-01   1.24998737e+00   1.96922481e+00  -1.31140957e+00\n",
      "  -6.02294092e-01  -1.54298075e-01  -1.40356093e+00   1.25020430e-01\n",
      "   1.34462210e+00   2.19644934e+00  -1.03386925e+00   1.80796590e+00\n",
      "   4.93960408e-01  -6.15260562e-01   7.13690989e-01  -1.41300666e+00\n",
      "   6.87096495e-02   1.84796962e-01  -3.02629429e-01  -2.84365853e-01\n",
      "  -1.31817865e+00  -1.14857950e-01   8.06128834e-01   1.82548577e-01\n",
      "   7.11938066e-01   1.24241702e+00   1.06250376e-01   1.30011222e+00\n",
      "   1.26693986e+00   8.11064642e-02  -4.25945091e-02   8.97912194e-01\n",
      "  -1.12234279e+00  -1.07499956e+00  -4.70441068e-01   3.34349930e-01\n",
      "   2.37042711e+00   1.35742284e-01  -4.01408437e-01  -2.57427850e-01\n",
      "   2.83328602e-01   4.18928092e-01   1.32666180e+00  -7.25322337e-01\n",
      "   3.14239507e-01  -9.31172132e-01  -6.44414805e-02  -7.26900785e-01\n",
      "   2.37837215e-02   1.25207005e+00  -1.52686160e+00  -6.87894959e-02\n",
      "  -1.28906504e+00  -2.01233300e-01   1.63446153e+00   9.20718330e-03\n",
      "   1.32980890e+00   1.05117595e-01  -5.27367870e-02   2.77565428e-01\n",
      "   5.02071452e-01  -1.90286177e-02  -2.09158785e-01   1.77000831e+00\n",
      "   8.49798950e-01  -1.13567099e-03   1.22593920e+00  -2.29250575e-01\n",
      "   1.26151079e+00  -2.35703882e-01  -8.30171847e-01   1.04921171e+00\n",
      "   1.60253746e+00  -6.14502233e-01  -1.87066567e+00   1.15324517e-01\n",
      "   9.18298235e-01  -8.45629020e-01   2.09162679e+00  -7.33995008e-01\n",
      "   1.78910277e+00   1.25241995e+00  -1.44735647e+00  -1.11857639e-02\n",
      "  -4.24654892e-01  -7.69937606e-01  -1.40281240e-01   4.69090206e-01\n",
      "  -2.34564333e+00  -8.65901667e-01   4.13492709e-01   1.60084817e-01\n",
      "   6.72673097e-01   6.45004426e-02  -7.12905644e-01  -4.48075901e-01\n",
      "  -6.83542260e-01  -2.43632105e-01  -9.37528388e-01   4.22553058e-01\n",
      "  -1.42433826e+00  -2.69892768e+00   2.63584100e-01  -1.42706575e+00\n",
      "  -3.46203939e-01   1.30565888e+00   1.45812691e+00   4.68347492e-01\n",
      "   8.26092176e-01  -2.28259237e+00   9.34078114e-01   9.37642336e-01\n",
      "  -7.28478720e-01   1.96373014e-01   1.46761152e-01   4.78877035e-02\n",
      "   9.30632160e-01   2.59701637e-01   5.16618096e-01   5.69286778e-01\n",
      "   2.21049912e-01   5.96769297e-02  -2.97111818e-01  -9.36167302e-01\n",
      "  -5.56013197e-02  -5.00417056e-01   4.10404088e-01  -1.21156909e+00\n",
      "   2.10114961e-01   3.79872493e-01   1.71018375e+00   1.08943853e+00\n",
      "  -6.98072881e-01   4.80514393e-01  -9.66163479e-02  -1.00286061e+00\n",
      "   2.02827117e+00   1.31299076e+00   2.85186930e-01  -3.52433305e-01\n",
      "  -9.95508681e-01   8.12125806e-01   1.58475716e+00  -3.21206386e-01\n",
      "  -7.63779641e-01  -3.03208563e-01  -5.51720662e-01   1.26212458e+00\n",
      "   3.46104724e-01  -1.45858508e+00   9.37058701e-02   1.63656644e+00\n",
      "   7.74406484e-01   5.15067899e-01  -3.52661929e-01   2.25651217e-02\n",
      "   5.95409310e-01  -5.22961754e-01   1.32229359e+00  -7.30203839e-01\n",
      "  -3.94429091e-01   9.27723930e-01  -4.49078407e-01   6.21434092e-01\n",
      "   1.47857873e-01   1.75427890e+00   1.91087738e+00   1.19294931e-01\n",
      "  -1.30671910e+00  -8.49669643e-01  -2.15753586e-01  -7.33891553e-01\n",
      "  -1.07679080e+00  -6.43718137e-01   1.17588412e+00   6.77752700e-01\n",
      "  -2.81150226e-01  -1.71037356e+00   1.18618216e+00   8.52215943e-02\n",
      "   1.37680893e+00   1.28972216e+00  -2.06010734e-01  -1.05093114e+00\n",
      "   2.01562859e+00   1.04167839e+00   1.12977192e+00   2.99405454e-01\n",
      "   3.33188608e-01  -5.14381820e-01   5.83488088e-02   5.74602532e-03\n",
      "  -6.38408457e-02   1.22095859e+00   3.89086371e-01   1.73464387e+00\n",
      "   1.48686847e+00   8.64496253e-01   1.93585059e+00   1.18802245e+00\n",
      "   1.72691815e+00  -5.42436014e-01  -4.55529590e-01  -5.27503937e-01\n",
      "  -5.62307455e-01   1.54783556e-01   1.28719261e+00  -1.62875617e+00\n",
      "   9.88053478e-01   7.54474200e-01   1.74469560e-01   3.20238354e-02\n",
      "   1.25860592e+00  -1.66883233e+00  -7.84450574e-01   2.02560928e+00\n",
      "   1.63931082e+00  -3.11440956e-01  -8.94578725e-02   1.32295788e+00\n",
      "   3.45116572e-01  -1.14334352e-01  -1.08889575e+00  -2.05867689e-01\n",
      "  -1.88444065e+00   6.18437661e-01  -7.16358853e-01  -5.81313748e-01\n",
      "  -1.36866877e-01  -2.67186144e-01   1.18618103e+00   6.74056298e-01\n",
      "   1.51368958e+00   6.47086292e-01  -5.06581533e-01   5.05875645e-01\n",
      "  -1.01586032e+00  -1.11871434e-01  -5.92647181e-02   7.74539746e-01\n",
      "   1.16693929e+00  -5.79413889e-01  -1.42712548e+00   4.14308602e-01\n",
      "  -3.04807856e-01  -1.58875095e-01  -1.77714690e+00  -6.00141712e-01\n",
      "   5.04530142e-01   1.16668888e+00   3.13426787e-02   8.02545601e-01\n",
      "  -1.46436554e+00  -5.11253333e-01   1.37104279e+00  -9.75949288e-01\n",
      "   8.05784989e-02  -2.35921185e-01  -1.47291902e+00   1.19218459e+00\n",
      "   2.16198492e-01  -2.90471920e-01  -1.95396827e-01   7.84475970e-02\n",
      "   4.87159418e-01   1.11715494e+00   5.72836295e-01   1.92007930e+00\n",
      "  -6.77983290e-01  -3.23782442e-01   3.64805876e-01   7.86601061e-01\n",
      "  -9.62839749e-01   2.95440344e-01   1.28371951e+00   2.60102441e-01\n",
      "   1.09467475e-01   2.11471262e+00  -1.32821453e+00  -1.36630712e+00\n",
      "   1.61687888e-01  -1.93129538e+00   7.88362541e-02   1.84904372e+00\n",
      "   4.51863218e-01   4.57190009e-01   3.71420155e-01  -2.25983352e+00\n",
      "   1.26619825e-01  -5.58160839e-01  -1.54849183e-01   1.20838539e+00\n",
      "  -1.48848006e-01   1.01220125e+00  -1.40991631e+00  -7.24455135e-01\n",
      "   1.14686037e+00  -4.62065367e-01  -1.50432711e-01   6.66211763e-01\n",
      "   1.74580726e+00   7.78568180e-01  -9.64200007e-01  -6.87513948e-01\n",
      "   2.20001677e-01   1.44419405e+00  -3.11330806e-01  -8.23542512e-01\n",
      "  -1.45489348e+00  -1.03499329e+00  -2.49398384e-02  -4.92981993e-01\n",
      "  -1.17156624e+00  -2.33729962e-01  -1.66868953e+00   2.32659900e-02\n",
      "   7.50811831e-01  -1.96005153e+00  -1.11925239e+00   5.89737633e-01\n",
      "   6.06616048e-01   1.33016052e+00   5.80586062e-01   2.10743616e-01\n",
      "  -8.09937672e-01  -3.73845893e-01  -9.30406096e-01  -1.40767985e+00\n",
      "   2.17243147e-01   6.80454140e-02  -2.73066821e-01   9.84732281e-01\n",
      "  -1.19260034e+00  -8.80722720e-01  -1.80273738e-01   4.75839767e-01\n",
      "   1.42954275e+00  -2.07705275e+00   3.58685737e-01   3.16245717e-01\n",
      "   8.81542184e-01   7.93199961e-01   3.41695951e-01   6.97845755e-01\n",
      "  -1.36310410e+00  -1.08579548e-01   9.57245692e-01   1.05028169e+00\n",
      "  -1.14884298e+00  -1.11484919e+00  -1.54984450e+00  -4.64959983e-01]\n",
      "[-4 -3 -2 -1  0  1  2  3  4]\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.001,  0.017,  0.135,  0.344,  0.35 ,  0.129,  0.024,  0.   ]), array([-4, -3, -2, -1,  0,  1,  2,  3,  4]))\n"
     ]
    }
   ],
   "source": [
    "print(np.histogram(a, bins=bins, density=True))\n",
    "histogram = np.histogram(a, bins=bins, density=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  2,  20, 158, 317, 352, 132,  16,   3], dtype=int64), array([-4, -3, -2, -1,  0,  1,  2,  3,  4]))\n"
     ]
    }
   ],
   "source": [
    "print(np.histogram(a, bins=bins, density=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.5 -2.5 -1.5 -0.5  0.5  1.5  2.5  3.5]\n"
     ]
    }
   ],
   "source": [
    "bins = 0.5*(bins[1:]+bins[:-1])\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "b = stats.norm.pdf(bins)  # norms is a distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0ldKKS+ipa+UUl5ES18ppbzI/wd9cIaFgejHNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb607518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(bins,histogram,label='From NumPy')\n",
    "plt.plot(bins,b,label='From SciPy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0103247208148\n"
     ]
    }
   ],
   "source": [
    "#--------------Percentiles----------\n",
    "print(np.median(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0103247208148\n"
     ]
    }
   ],
   "source": [
    "print(stats.scoreatpercentile(a,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25210503644\n"
     ]
    }
   ],
   "source": [
    "print(stats.scoreatpercentile(a,90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
